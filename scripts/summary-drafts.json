{
  "ai-risk-portfolio-analysis": {
    "title": "AI Risk Portfolio Analysis",
    "current": "A framework for comparing the relative importance of different AI risk categories to guide resource allocation",
    "draft": "A framework for comparing the relative importance of different AI risk categories to guide resource allocation. Estimates in this model carry substantial uncertainty (often ±50% or more).",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Estimates in this model carry substantial uncertainty (often ±50% or more)."
    ]
  },
  "authentication-collapse-timeline": {
    "title": "Authentication Collapse Timeline Model",
    "current": "Projecting when digital verification systems cross critical failure thresholds",
    "draft": "Projecting when digital verification systems cross critical failure thresholds. This asymmetry, combined with observed decay rates in detection accuracy across modalities (text, image, audio, video), suggests authentication will face increasing stress absent significant defensive investments.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "This asymmetry, combined with observed decay rates in detection accuracy across modalities (text, image, audio, video), suggests authentication will face increasing stress absent significant defensive investments.",
      "Current data shows text detection already at random-chance levels, image detection declining 5-10% annually, and similar patterns emerging across audio and video as model capabilities advance.",
      "Conceptual Framework  B{Detection Accuracy}     B -->|>70%| C[Functional Authentication]     B -->|50-70%| D[Degraded Authentication]     B -->| F[Normal Operations]     D --> G{Intervention Success?",
      "Unlike continuous degradation, these thresholds create discontinuous jumps in risk as institutions that rely on authentication suddenly find their verification methods insufficient for their operational requirements.",
      "This threshold has already been crossed for text generation, where commercial detection tools and open-source alternatives alike perform at 45-55% accuracy—statistically indistinguishable from coin flips."
    ]
  },
  "authoritarian-tools-diffusion": {
    "title": "Authoritarian Tools Diffusion Model",
    "current": "Analyzes how AI surveillance and control technologies spread from developers to authoritarian regimes",
    "draft": "Analyzes how AI surveillance and control technologies spread from developers to authoritarian regimes. The model identifies semiconductor supply chains as the highest-leverage intervention point, but estimates this advantage will erode within 5-10 years as China develops domestic chip manufacturi...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The model identifies semiconductor supply chains as the highest-leverage intervention point, but estimates this advantage will erode within 5-10 years as China develops domestic chip manufacturing capabilities.",
      "Estimates suggest $50-100 billion has been invested in digital infrastructure in developing nations between 2015 and 2024, with surveillance capabilities integrated into supposedly neutral projects like telecommunications networks, public safety systems, and urban management platforms.",
      "Project Raven, revealed through investigative reporting, demonstrated how Western expertise was deployed to target dissidents, journalists, and rival nations.",
      "China operates over 500 million surveillance cameras with a target of 1 billion by 2030, creating camera density that enables continuous tracking in urban environments.",
      "Chinese semiconductor development succeeds faster than expected (by 2028-2030), eliminating the primary choke point for AI capability diffusion."
    ]
  },
  "automation-bias-cascade": {
    "title": "Automation Bias Cascade Model",
    "current": "Analysis of over-reliance dynamics and calibration failures in human-AI systems",
    "draft": "Analysis of over-reliance dynamics and calibration failures in human-AI systems. Parameter Estimates The table below provides empirically-grounded estimates for key parameters in automation bias dynamics across different domains.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Parameter Estimates The table below provides empirically-grounded estimates for key parameters in automation bias dynamics across different domains.",
      "These estimates draw from studies of AI-assisted decision making in medical, financial, and judicial contexts.",
      "| Parameter | Best Estimate | Range | Confidence | Source | |-----------|--------------|-------|------------|--------| | Initial trust level $T0$ | 0.",
      "50 | Medium | Medical error studies | | Time to critical dependence | 3 years | 1-7 years | Low | Institutional case studies | | Skill atrophy rate | 15%/year | 10-25%/year | Low | Expert performance studies | | Calibration error at 5 years | +0.",
      "AI diagnostic tools have proliferated rapidly, with over 500 FDA-approved AI devices in clinical use and adoption rates increasing 30-40% annually."
    ]
  },
  "autonomous-weapons-escalation": {
    "title": "Autonomous Weapons Escalation Model",
    "current": "Risk modeling for AI-accelerated conflict escalation at machine speed",
    "draft": "Risk modeling for AI-accelerated conflict escalation at machine speed. The model suggests annual catastrophic escalation risks between 1-5% once systems are widely deployed, implying 10-40% probability of at least one crisis over a decade.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The model suggests annual catastrophic escalation risks between 1-5% once systems are widely deployed, implying 10-40% probability of at least one crisis over a decade.",
      "195$ This suggests approximately 20% of ambiguous incidents trigger escalation spirals.",
      "999$ This unrealistically high estimate reveals the model's limitations—it assumes incidents are independent and systems do not learn from near-misses.",
      "Nevertheless, even with correlation-adjusted estimates placing annual risk at 1-5%, cumulative decade-scale risk remains substantial.",
      "Parameter Estimates and Sensitivity Analysis The model's outputs are highly sensitive to several key parameters."
    ]
  },
  "autonomous-weapons-proliferation": {
    "title": "LAWS Proliferation Model",
    "current": "Timeline and dynamics of lethal autonomous weapons spreading globally",
    "draft": "Timeline and dynamics of lethal autonomous weapons spreading globally. Five-Stage Proliferation Model | Stage | Timeline | Actor Type | Access Method | Capability Level | Current Status | |-------|----------|------------|---------------|------------------|----------------| | Stage 1: Great Power ...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Five-Stage Proliferation Model | Stage | Timeline | Actor Type | Access Method | Capability Level | Current Status | |-------|----------|------------|---------------|------------------|----------------| | Stage 1: Great Power Development | 2015-2023 | U.",
      ", China, Russia, Israel | Indigenous R&D | Advanced autonomy, military-grade | Complete (100%) | | Stage 2: Regional Power Adoption | 2020-2026 | Turkey, Iran, UK, France, India, S.",
      "LAWS are projected to reach more nations by 2032 than nuclear weapons have reached in 80 years, while simultaneously becoming accessible to non-state actors—a proliferation outcome that nuclear weapons control successfully prevented.",
      "By 2030, half of all militarily capable nations will possess autonomous weapons—a proliferation outcome that transforms LAWS from a specialized capability of great powers to a standard military technology.",
      "Autonomous weapons are projected to follow a similar but somewhat delayed timeline, with the critical transition to regular use by non-state actors occurring around 2030-2032—roughly 10-12 years behind state-level capabilities."
    ]
  },
  "bioweapons-ai-uplift": {
    "title": "AI Uplift Assessment Model",
    "current": "This model estimates AI's marginal contribution to bioweapons risk over time. It projects uplift increasing from 1.3-2.5x (2024) to 3-5x by 2030, with biosecurity evasion capabilities posing the greatest concern as they could undermine existing defenses before triggering policy response.",
    "draft": "This model estimates AI's marginal contribution to bioweapons risk over time. It projects uplift increasing from 1.3-2.5x (2024) to 3-5x by 2030, with biosecurity evasion capabilities posing the greatest concern as they could undermine existing defenses before triggering policy response.",
    "status": "ok",
    "reason": "Already has findings"
  },
  "bioweapons-attack-chain": {
    "title": "Bioweapons Attack Chain Model",
    "current": "Multiplicative probability model decomposing AI-assisted bioweapons attack requirements",
    "draft": "Multiplicative probability model decomposing AI-assisted bioweapons attack requirements. Parameter Estimates  Full Parameter Table | Step | Parameter | Low Estimate | Central | High Estimate | Confidence | Key Uncertainty | |------|-----------|--------------|---------|---------------|------------...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Parameter Estimates  Full Parameter Table | Step | Parameter | Low Estimate | Central | High Estimate | Confidence | Key Uncertainty | |------|-----------|--------------|---------|---------------|------------|-----------------| | 1 | Motivated actor exists | 0.",
      "6% | Very Low | Correlation structure |  Actor-Specific Estimates Different actor types face different bottlenecks.",
      "Current evidence suggests frontier model guardrails remain partially effective for the most dangerous queries, but this may not persist as open-source models approach frontier capabilities.",
      "This suggests current AI systems provide minimal uplift over existing information sources.",
      "02% | 1,000-10,000 | Low | | Central | Central estimates throughout | 0."
    ]
  },
  "bioweapons-timeline": {
    "title": "AI-Bioweapons Timeline Model",
    "current": "Projecting when AI capabilities might meaningfully increase bioweapons risks",
    "draft": "Projecting when AI capabilities might meaningfully increase bioweapons risks. The framework emphasizes that the current period (2024-2027) represents a critical window where governance structures can be established before the most dangerous capabilities proliferate.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Overview This model projects when AI capabilities might cross thresholds that meaningfully change bioweapons risk, providing a structured timeline for when different types of AI-enabled biological threats could emerge.",
      "High uncertainty persists throughout, but even imprecise timeline estimates are more useful than treating all risks as equally imminent or distant.",
      "The framework emphasizes that the current period (2024-2027) represents a critical window where governance structures can be established before the most dangerous capabilities proliferate.",
      "Projection: Likely fully crossed by 2025-2027 as models become more capable and open-source versions proliferate without the same safeguards.",
      "Projection: 2027-2032, contingent on progress in biological design tools, automated lab systems, and integration between AI systems."
    ]
  },
  "capabilities-to-safety-pipeline": {
    "title": "Capabilities-to-Safety Pipeline Model",
    "current": "Analyzing how researchers transition from AI capabilities work to AI safety research",
    "draft": "Analyzing how researchers transition from AI capabilities work to AI safety research. Current estimates suggest annual transitions of 50-400 researchers, against a potential pool of 50,000-100,000 ML researchers globally.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "This suggests a dual intervention strategy: upstream interventions focused on building genuine concern and consideration, combined with downstream interventions that reduce practical barriers for those already motivated.",
      "Current estimates suggest annual transitions of 50-400 researchers, against a potential pool of 50,000-100,000 ML researchers globally.",
      "Current Flow Estimates  Source Population Characteristics The global ML research community represents a substantial potential talent pool, though only a fraction meets the quality and motivation thresholds for impactful safety work.",
      "The largest absolute drop-off occurs at the awareness-to-consideration transition, where 85-90% of researchers who understand safety concerns do not seriously consider changing their career direction.",
      "| Pull Factor | Prevalence | Strength | Retention Correlation | Key Characteristic | |-------------|------------|----------|----------------------|-------------------| | Existential risk concern | 60-80% | Very High | 0."
    ]
  },
  "capability-alignment-race": {
    "title": "Capability-Alignment Race Model",
    "current": "Risk emerges from the gap between capability progress and safety/governance readiness.",
    "draft": "Risk emerges from the gap between capability progress and safety/governance readiness.",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "1. **Capability → Economic pressure → Deployment speed → Accident risk**"
  },
  "capability-threshold-model": {
    "title": "Capability Threshold Model",
    "current": "Mapping AI capability levels to specific risk activation thresholds",
    "draft": "Mapping AI capability levels to specific risk activation thresholds. The estimated threshold crossing is 2026-2029, with the current bottleneck being specialized synthesis knowledge and the ability to troubleshoot complex laboratory procedures autonomously.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The estimated threshold crossing is 2026-2029, with the current bottleneck being specialized synthesis knowledge and the ability to troubleshoot complex laboratory procedures autonomously.",
      "The estimated threshold crossing is 2025-2027, with current bottlenecks in autonomous operation over extended periods and adaptive persistence that responds to defensive measures.",
      "The estimated threshold crossing is 2025-2026, notably soon given current progress in language generation.",
      "The estimated threshold crossing is 2025-2027, with current bottlenecks in deep self-modeling and deployment context awareness that goes beyond surface-level pattern recognition.",
      "The estimated threshold crossing is 2027-2035 or beyond, with substantial uncertainty."
    ]
  },
  "compounding-risks-analysis": {
    "title": "Compounding Risks Analysis",
    "current": "How multiple AI risks compound to create outcomes worse than the sum of their parts",
    "draft": "How multiple AI risks compound to create outcomes worse than the sum of their parts. + Rn$ | Independent, non-interacting | 2-5x | | Multiplicative | $R{total} = 1 - \\prodi(1 - Ri) \\times IF$ | Overlapping vulnerabilities | 1.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "+ Rn$ | Independent, non-interacting | 2-5x | | Multiplicative | $R{total} = 1 - \\prodi(1 - Ri) \\times IF$ | Overlapping vulnerabilities | 1.",
      "Expected harm by 2040 approaches certainty if trajectory continues.",
      "Compound risk reaches \"elevated\" levels by 2030 but catastrophic combinations are avoided through reactive interventions.",
      "645 by 2040 represents significantly elevated danger, with substantial probability mass on catastrophic outcomes from Scenario A and threshold events from Scenario D.",
      "Different analysts could reasonably propose coefficients that differ by 2-3x, which would dramatically change compound risk estimates."
    ]
  },
  "concentration-of-power": {
    "title": "Concentration of Power Systems Model",
    "current": "Systems dynamics analysis of power accumulation and entrenchment mechanisms",
    "draft": "Systems dynamics analysis of power accumulation and entrenchment mechanisms. The probability estimate of 40% reflects the strength of current reinforcing loops and the demonstrated weakness of regulatory responses.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Historical precedents suggest that concentration, once achieved, persists for decades or longer absent major shocks.",
      "Current estimates suggest this condition is met for leading AI developers.",
      "The probability estimate of 40% reflects the strength of current reinforcing loops and the demonstrated weakness of regulatory responses.",
      "The EU AI Act and US export controls suggest movement toward this scenario.",
      "Most critically, AGI timeline estimates span decades and dramatically affect concentration speed."
    ]
  },
  "consensus-manufacturing-dynamics": {
    "title": "Consensus Manufacturing Dynamics Model",
    "current": "Analyzes how AI enables artificial consensus creation and opinion convergence at scale",
    "draft": "Analyzes how AI enables artificial consensus creation and opinion convergence at scale",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "This model examines how AI systems can be used to manufacture artificial consensus, creating the appearance of widespread agreement where genuine consensus does not exist. It analyzes the mechanisms, "
  },
  "corrigibility-failure-pathways": {
    "title": "Corrigibility Failure Pathways",
    "current": "Causal model mapping pathways from training to corrigibility failure with intervention points",
    "draft": "Causal model mapping pathways from training to corrigibility failure with intervention points. The probability of this pathway activating ranges from 60-90% for capable optimizers with unbounded goals, though this can be reduced through interventions.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "This model maps the causal pathways through which corrigibility failure can arise, identifies intervention points, and estimates probabilities.",
      "The probability of this pathway activating ranges from 60-90% for capable optimizers with unbounded goals, though this can be reduced through interventions.",
      "Key variables that modulate this pathway include goal boundedness, where bounded or satiable goals reduce probability by approximately 40%.",
      "These interventions show medium effectiveness at 40-70% reduction in pathway activation.",
      "This pathway activates with 50-80% probability, modulated by several key variables."
    ]
  },
  "critical-uncertainties": {
    "title": "Critical Uncertainties Model",
    "current": "Focus on high-leverage, genuinely uncertain, and empirically resolvable variables.",
    "draft": "Focus on high-leverage, genuinely uncertain, and empirically resolvable variables. Variable Categories  Hardware & Compute (6 nodes) | Variable | Current Estimate | Uncertainty Range | Resolvable Via | |----------|------------------|-------------------|----------------| | GPU production growth | ...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Variable Categories  Hardware & Compute (6 nodes) | Variable | Current Estimate | Uncertainty Range | Resolvable Via | |----------|------------------|-------------------|----------------| | GPU production growth | 2x / 2."
    ]
  },
  "cyber-psychosis-cascade": {
    "title": "Cyber Psychosis Cascade Model",
    "current": "Models how AI-generated content can trigger psychological distress cascades in vulnerable populations",
    "draft": "Models how AI-generated content can trigger psychological distress cascades in vulnerable populations. 50 | Amplification (+), Trigger detection (-) | Rough cascade probability range: 1-10%, but this range reflects uncertainty about correlation structure more than parameter uncertainty.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Current evidence suggests defense is losing this race, making proactive intervention increasingly urgent.",
      "50 | Amplification (+), Trigger detection (-) | Rough cascade probability range: 1-10%, but this range reflects uncertainty about correlation structure more than parameter uncertainty.",
      "5-2x | Medium-High | | Information overload | 50-60% | Decision paralysis | 1.",
      "5-2% | Very low | Epidemiological Projection: $$ \\text{Dependency Rate}(t) = D0 \\times e^{rt} \\times \\left(1 - \\frac{D(t)}{K}\\right) $$ Where: - $D0$ = initial dependency rate (~0.",
      "5 annually) - $K$ = carrying capacity (~5-15% of population) | Year | Projected Dependency Rate | Confidence Interval | Key Assumptions | |------|---------------------------|---------------------|-----------------| | 2025 | 0."
    ]
  },
  "cyberweapons-attack-automation": {
    "title": "Autonomous Cyber Attack Timeline",
    "current": "Multiplicative capability model projecting when AI systems will conduct end-to-end cyber attacks autonomously",
    "draft": "Multiplicative capability model projecting when AI systems will conduct end-to-end cyber attacks autonomously. Overview This model projects when AI systems will be capable of conducting complete cyber attack campaigns with minimal or no human involvement.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Overview This model projects when AI systems will be capable of conducting complete cyber attack campaigns with minimal or no human involvement."
    ]
  },
  "cyberweapons-offense-defense": {
    "title": "Cyber Offense-Defense Balance Model",
    "current": "Analyzes whether AI shifts the balance between cyber attackers and defenders",
    "draft": "Analyzes whether AI shifts the balance between cyber attackers and defenders. The key insight emerging from this analysis is that AI provides a temporary but significant offense advantage (estimated at 30-70% net improvement in attack success rates) driven primarily by automation scaling and vuln...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The analysis synthesizes empirical evidence from documented attacks, security industry data, and capability assessments to produce probability-weighted estimates of net advantage across different scenarios.",
      "The key insight emerging from this analysis is that AI provides a temporary but significant offense advantage (estimated at 30-70% net improvement in attack success rates) driven primarily by automation scaling and vulnerability discovery acceleration.",
      "1)  Parameter Estimates | Parameter | Best Estimate | Range | Confidence | Source | |-----------|---------------|-------|------------|--------| | Current $B{OD}$ ratio | 1.",
      "05× | Medium | Automated containment studies | | Offense tool diffusion rate | 25%/year | 15-40% | Low | Dark web monitoring | | Defense investment gap | -35% | -50% to -20% | Medium | Industry spending reports | | Vulnerability discovery speedup | 1.",
      "By 2027, successful ransomware incidents increase 3-5× from 2024 levels, and critical infrastructure experiences multiple coordinated attacks."
    ]
  },
  "deceptive-alignment-decomposition": {
    "title": "Deceptive Alignment Decomposition Model",
    "current": "Probability decomposition model analyzing the conditions under which deceptive alignment could emerge in AI training",
    "draft": "Probability decomposition model analyzing the conditions under which deceptive alignment could emerge in AI training. Parameter Estimates  Full Parameter Table | Component | Parameter | Low | Central | High | Confidence | Key Uncertainty Driver | |-----------|-----------|-----|---------|------|--...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "This creates multiple intervention opportunities and suggests that even partial progress on several fronts can substantially reduce aggregate risk.",
      "Parameter Estimates  Full Parameter Table | Component | Parameter | Low | Central | High | Confidence | Key Uncertainty Driver | |-----------|-----------|-----|---------|------|------------|----------------------| | Mesa-optimization | $P(M)$ | 0.",
      "2% | Very Low | Correlation structure |  Scenario-Based Estimates | Scenario | $P(M)$ | $P(G)$ | $P(S)$ | $P(D)$ | $P(V)$ | Compound | Key Assumption | |----------|--------|--------|--------|--------|--------|----------|----------------| | Base case | 0.",
      "0% | Central estimates throughout | | Pessimistic | 0.",
      "However, the threshold at which \"complex heuristics\" become \"internal optimization\" remains theoretically unclear, making this parameter difficult to estimate with precision."
    ]
  },
  "deepfakes-authentication-crisis": {
    "title": "Deepfakes Authentication Crisis Model",
    "current": "Timeline toward an authenticity crisis where synthetic media becomes indistinguishable",
    "draft": "Timeline toward an authenticity crisis where synthetic media becomes indistinguishable. ) | 50-55% | 55-65% | Text/image prompt | Crisis | Audio synthesis has progressed fastest toward the crisis threshold.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      ") | 50-55% | 55-65% | Text/image prompt | Crisis | Audio synthesis has progressed fastest toward the crisis threshold.",
      "Image synthesis underwent a step-change improvement with the introduction of diffusion models in 2022-2023.",
      "The trajectory suggests that within 5 years, audio and video evidence may require extensive technical authentication procedures that add significant cost and time to legal proceedings, or may be excluded entirely in favor of other evidence types.",
      "Current adoption stands at approximately 5-10% of media, with major technology companies supporting the standard but limited camera manufacturer integration.",
      "Projections range from 50-70% adoption by 2030 in optimistic scenarios to under 30% if coordination fails."
    ]
  },
  "defense-in-depth-model": {
    "title": "Defense in Depth Model",
    "current": "How layered safety measures combine to reduce overall risk, and analysis of failure modes when layers break",
    "draft": "How layered safety measures combine to reduce overall risk, and analysis of failure modes when layers break. This represents a dramatic improvement over relying on any single layer, where failure probabilities range from 20-40%.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "This represents a dramatic improvement over relying on any single layer, where failure probabilities range from 20-40%.",
      "5$), the effective failure probability falls between the independent and correlated extremes, typically in the 4-8% range for the examples above.",
      "45\\%$$ However, when adjusted for the observed correlations using the partial correlation model, the realistic failure probability increases to approximately 2-5%.",
      "While this is substantially worse than the independent case, it still represents a dramatic improvement over relying on any single layer (25-50% failure probability).",
      "First, we do not know the true failure correlations between real AI safety interventions—the correlation coefficients used in examples are illustrative estimates rather than empirically validated measurements."
    ]
  },
  "disinformation-detection-race": {
    "title": "Disinformation Detection Arms Race Model",
    "current": "Competitive dynamics between AI-generated disinformation and AI-powered detection",
    "draft": "Competitive dynamics between AI-generated disinformation and AI-powered detection. If content-based detection falls below practical utility (which this model projects by 2028-2030), societies cannot rely on technical systems to distinguish authentic from synthetic content.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "If content-based detection falls below practical utility (which this model projects by 2028-2030), societies cannot rely on technical systems to distinguish authentic from synthetic content.",
      "Current detection accuracy trends from ~85-95% in 2018 to ~60-70% in 2024 suggest we have perhaps 3-5 years before content-based detection becomes practically useless, making this a critical window for building alternative infrastructure.",
      "Under high adversarial pressure, this occurs by 2026-2027.",
      "Scenario B: Provenance Transition (30% probability) C2PA or similar cryptographic authentication achieves critical mass adoption by 2028-2030.",
      "Detectors remain ~75-80% accurate through 2030."
    ]
  },
  "disinformation-electoral-impact": {
    "title": "Electoral Impact Assessment Model",
    "current": "Framework for estimating AI disinformation's marginal impact on elections",
    "draft": "Framework for estimating AI disinformation's marginal impact on elections. 01 per piece Multiplier Effect: - Volume increase: 100-1000x - Quality increase: 1.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "01 per piece Multiplier Effect: - Volume increase: 100-1000x - Quality increase: 1.",
      "5-3x (more convincing) - Personalization increase: 10-100x (targeted messaging) Overall AI Impact on Content Creation: ~150-3000x increase in effective disinformation output Confidence: High.",
      "2-2x amplification vs.",
      "baseline Audience Reach: - Traditional disinformation: reaches 5-15% of target audience - AI-personalized disinformation: reaches 10-30% of target audience (better targeting) Overall Exposure Multiplier (AI vs traditional): 1.",
      "5-4x Confidence: Medium."
    ]
  },
  "economic-disruption-impact": {
    "title": "Economic Disruption Impact Model",
    "current": "System dynamics analysis of AI-driven labor displacement and economic instability",
    "draft": "System dynamics analysis of AI-driven labor displacement and economic instability. 5-3x (each job lost triggers 0.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "It moves beyond simple displacement estimates to examine systemic fragility and adaptation capacity.",
      "5-3x (each job lost triggers 0.",
      "unemployment: 3-4% - U."
    ]
  },
  "economic-disruption": {
    "title": "Economic Disruption Structural Model",
    "current": "Macroeconomic analysis of AI-driven labor market and structural transformations",
    "draft": "Macroeconomic analysis of AI-driven labor market and structural transformations. 1%: Own ~30% (AI owners) Pessimistic: Top 1%: Own ~70-80% of wealth Top 0.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "1%: Own ~30% (AI owners) Pessimistic: Top 1%: Own ~70-80% of wealth Top 0."
    ]
  },
  "epistemic-collapse-threshold": {
    "title": "Epistemic Collapse Threshold Model",
    "current": "Analyzing critical thresholds where society loses the ability to establish shared facts and make collective decisions",
    "draft": "Analyzing critical thresholds where society loses the ability to establish shared facts and make collective decisions. Central Question: At what point does epistemic degradation become irreversible, and what intervention windows remain before critical thresholds are crossed?",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Historical precedents from collapsed information environments—late Roman Empire, Weimar Germany, Soviet final years—suggest that epistemic systems can reach severely degraded states, though these cases involved major exogenous shocks beyond information dynamics alone.",
      "Central Question: At what point does epistemic degradation become irreversible, and what intervention windows remain before critical thresholds are crossed?",
      "3 \\cdot M(t) $$ Current estimates for the United States in 2024 suggest verification capacity is already approaching critical thresholds.",
      "| Subcomponent | Current Estimate (2024) | AI Impact by 2030 | Projected 2030 Value | Key Drivers | |--------------|------------------------|-------------------|---------------------|-------------| | Authentication $A$ | 0.",
      "32 | Crosses critical threshold (0."
    ]
  },
  "expertise-atrophy-cascade": {
    "title": "Expertise Atrophy Cascade Model",
    "current": "Analyzing how AI dependency causes cascading skill degradation across domains and generations",
    "draft": "Analyzing how AI dependency causes cascading skill degradation across domains and generations. Parameter Estimates | Parameter | Symbol | Low Estimate | Central | High Estimate | Confidence | Key Uncertainty | |-----------|--------|--------------|---------|---------------|------------|-----------...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Parameter Estimates | Parameter | Symbol | Low Estimate | Central | High Estimate | Confidence | Key Uncertainty | |-----------|--------|--------------|---------|---------------|------------|-----------------| | Learning rate per practice hour | $\\alpha$ | 0.",
      "60 | Low | Emerging phenomenon |  Critical Threshold Analysis Expertise exists along a continuum, but three thresholds mark qualitatively different states with distinct implications for system resilience and recovery potential.",
      "Current evidence from GitHub Copilot adoption suggests this cascade is already underway.",
      "90 | 100% | Yes | Intact | | Gen 1 (AI-Assisted) | 2020-2035 | 0.",
      "70 | 65-85% | Partially | Degrading | | Gen 2 (AI-Native) | 2035-2050 | 0."
    ]
  },
  "expertise-atrophy-progression": {
    "title": "Expertise Atrophy Progression Model",
    "current": "Five-phase model of human skill degradation from AI augmentation to irreversible dependency",
    "draft": "Five-phase model of human skill degradation from AI augmentation to irreversible dependency. Calibration Loss - Lose intuition for what's \"reasonable\" - Can't sanity-check AI outputs - Example: Accepting navigation route that's obviously wrong Critical Threshold: When human skill level drops belo...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Calibration Loss - Lose intuition for what's \"reasonable\" - Can't sanity-check AI outputs - Example: Accepting navigation route that's obviously wrong Critical Threshold: When human skill level drops below the level needed to: 1.",
      "Critical thresholds: Exact points of no return 4."
    ]
  },
  "feedback-loops": {
    "title": "Feedback Loop & Cascade Model",
    "current": "AI risk emerges from reinforcing feedback loops that can accelerate through critical thresholds.",
    "draft": "AI risk emerges from reinforcing feedback loops that can accelerate through critical thresholds. breakout {       margin-left: 0;       margin-right: 0;       width: 100%;     }   } } Core thesis: AI risk isn't static—it emerges from reinforcing feedback loops that can rapidly accelerate through ...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "breakout {       margin-left: 0;       margin-right: 0;       width: 100%;     }   } } Core thesis: AI risk isn't static—it emerges from reinforcing feedback loops that can rapidly accelerate through critical thresholds."
    ]
  },
  "flash-dynamics-threshold": {
    "title": "Flash Dynamics Threshold Model",
    "current": "Threshold analysis of when AI system speeds exceed human oversight capacity",
    "draft": "Threshold analysis of when AI system speeds exceed human oversight capacity. Overview This model analyzes critical thresholds where AI system interaction speeds exceed human capacity for oversight, intervention, or comprehension.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Overview This model analyzes critical thresholds where AI system interaction speeds exceed human capacity for oversight, intervention, or comprehension.",
      "Current evidence suggests we have exceeded Thresholds 1-2 in finance and are approaching them in cybersecurity, infrastructure, and AI development itself.",
      "The baseline trajectory without intervention shows multiple domains approaching Threshold 4 (cascade criticality) by 2030, where cascades complete faster than countermeasures can be designed.",
      "The model provides a framework for prioritizing interventions in domains closest to critical thresholds while there is still time to implement safeguards.",
      "By 2027, more domains exceed Threshold 1 (oversight), financial systems approach Threshold 3 (comprehension), and cybersecurity approaches Threshold 2 (intervention)."
    ]
  },
  "fraud-sophistication-curve": {
    "title": "Fraud Sophistication Curve Model",
    "current": "Analyzes how AI enables fraud evolution and escalation dynamics over time",
    "draft": "Analyzes how AI enables fraud evolution and escalation dynamics over time. The model suggests annual AI-enabled fraud losses will exceed $75 billion by 2028 under baseline assumptions.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The model suggests annual AI-enabled fraud losses will exceed $75 billion by 2028 under baseline assumptions.",
      "Early AI Era (2019-2022) The transition began with the first documented voice cloning fraud in 2019, when criminals impersonated a CEO to extract $243,000 from a UK energy company.",
      "Simultaneously, early language models began generating phishing content at scale, improving success rates by 20-30% compared to template-based approaches.",
      "Current Era (2023-2025) The current period marks a phase transition in fraud capabilities.",
      "Near-Term Projection (2026-2028) The trajectory points toward autonomous fraud agents capable of operating continuously without human intervention."
    ]
  },
  "goal-misgeneralization-probability": {
    "title": "Goal Misgeneralization Probability Model",
    "current": "Quantitative model estimating likelihood of goal misgeneralization across deployment scenarios",
    "draft": "Quantitative model estimating likelihood of goal misgeneralization across deployment scenarios. Each can shift probabilities by factors of 2-5x.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "This variation suggests that careful deployment practices can substantially reduce risk even before fundamental alignment breakthroughs, but that high-stakes autonomous deployment under distribution shift remains genuinely dangerous with current methods.",
      "We can estimate each factor independently using different evidence sources: capability generalization from standard ML literature, goal failure from alignment-specific studies, and harm from deployment context and oversight structure.",
      "Each can shift probabilities by factors of 2-5x.",
      "| Capability Level | Capability Generalization | Goal Failure Detection | Harm Potential | Overall Multiplier | |------------------|---------------------------|------------------------|----------------|-------------------| | Below-human | 40-70% | Easy (30-50% reduction) | Limited | 0.",
      "0x | | Human-level | 60-85% | Moderate (20-40% increase) | Significant | 1."
    ]
  },
  "institutional-adaptation-speed": {
    "title": "Institutional Adaptation Speed Model",
    "current": "Analyzing how fast institutions can respond to AI developments and what factors affect their adaptation capacity",
    "draft": "Analyzing how fast institutions can respond to AI developments and what factors affect their adaptation capacity. 12% per year Time to adequate governance: Never at this rate  Key Uncertainties  Implications  Short-term (2025-2028) 1.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "12% per year Time to adequate governance: Never at this rate  Key Uncertainties  Implications  Short-term (2025-2028) 1.",
      "Focus on feasible adaptations    - National-level action more achievable    - Standards bodies may move faster than governments    - Insurance markets may develop  Medium-term (2028-2035) 1."
    ]
  },
  "instrumental-convergence-framework": {
    "title": "Instrumental Convergence Framework",
    "current": "Analytical framework identifying universal instrumental goals and convergent subgoals across AI systems",
    "draft": "Analytical framework identifying universal instrumental goals and convergent subgoals across AI systems. Goal-Content Integrity (Convergence: 90-99%) Goal-content integrity—the drive to maintain current goals unchanged—emerges from a simple temporal consistency argument.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Quantitative Analysis  Master Parameter Table The following table synthesizes estimates for each convergent instrumental goal, drawing on theoretical arguments, observations from reinforcement learning systems, and analogies to biological and institutional optimization processes.",
      "Goal-Content Integrity (Convergence: 90-99%) Goal-content integrity—the drive to maintain current goals unchanged—emerges from a simple temporal consistency argument.",
      "The 3-5x severity multiplier for this combination reflects the self-reinforcing lock-in dynamic.",
      "Cognitive Enhancement (Convergence: 80-95%) Cognitive enhancement encompasses drives toward improved reasoning, expanded knowledge, and enhanced problem-solving capabilities.",
      "Resource Acquisition (Convergence: 75-90%) Resources—matter, energy, compute, money, influence—expand an agent's action space and increase the probability of goal achievement."
    ]
  },
  "international-coordination-game": {
    "title": "International AI Coordination Game",
    "current": "Game-theoretic analysis of international AI governance dynamics between major powers",
    "draft": "Game-theoretic analysis of international AI governance dynamics between major powers. 5 on this scale, and recent trends suggest movement in the negative direction.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The International Space Station model demonstrates sustained cooperation on complex technological projects between geopolitical rivals, offering a potential template for AI governance.",
      "5 on this scale, and recent trends suggest movement in the negative direction.",
      "This scenario assigns equal probability to competitive coexistence because current trends suggest meaningful risk of tipping into full racing mode.",
      "The probability of this scenario is lower because full decoupling imposes enormous economic costs that create pressure for maintaining at least minimal technical exchange, but recent trends toward technology nationalism and supply chain security suggest non-negligible risk.",
      "Joint safety research projects between universities and labs across borders build technical common ground and personal relationships."
    ]
  },
  "intervention-effectiveness-matrix": {
    "title": "Intervention Effectiveness Matrix",
    "current": "Mapping AI safety interventions to the risks they mitigate, with effectiveness estimates and gap analysis",
    "draft": "Mapping AI safety interventions to the risks they mitigate, with effectiveness estimates and gap analysis",
    "status": "ok",
    "reason": "Already has findings"
  },
  "intervention-timing-windows": {
    "title": "Intervention Timing Windows",
    "current": "Which AI safety interventions have closing windows and require urgent action vs which remain effective over longer periods",
    "draft": "Which AI safety interventions have closing windows and require urgent action vs which remain effective over longer periods",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "**Which interventions must happen NOW vs which can wait?**"
  },
  "irreversibility-threshold": {
    "title": "Irreversibility Threshold Model",
    "current": "Point-of-no-return analysis and reversal cost modeling for AI-related decisions and developments",
    "draft": "Point-of-no-return analysis and reversal cost modeling for AI-related decisions and developments. Cost estimation uncertainty: Reversal cost estimates in this model carry uncertainty ranges of 1-2 orders of magnitude or more.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Cost estimation uncertainty: Reversal cost estimates in this model carry uncertainty ranges of 1-2 orders of magnitude or more.",
      "Parameter estimates derive primarily from expert judgment and structural reasoning rather than observed data.",
      "The model should be treated as a thinking tool rather than a predictive instrument, with estimates taken as illustrative rather than authoritative.",
      "Scenario Analysis The following analysis examines four archetypal trajectories for AI-related irreversibility over the 2025-2035 period, with probability-weighted assessments of outcomes and intervention feasibility.",
      "By 2030, major sectors become practically dependent on AI systems that cannot be easily replaced."
    ]
  },
  "lab-incentives-model": {
    "title": "AI Lab Incentives Model",
    "current": "Analysis of how competitive and reputational pressures shape AI lab safety investment decisions",
    "draft": "Analysis of how competitive and reputational pressures shape AI lab safety investment decisions. High uncertainty about competitor progress 4.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "High uncertainty about competitor progress 4."
    ]
  },
  "lock-in": {
    "title": "Lock-in Irreversibility Model",
    "current": "Analysis of irreversible transitions and path dependencies in AI development",
    "draft": "Analysis of irreversible transitions and path dependencies in AI development. B{Decision Point}     B -->|Path 1| C[Reversible Adoption]     B -->|Path 2| D[Critical Threshold]     C --> E[Maintain Optionality]     D --> F[Lock-in State]     F --> G[Irreversible]     E --> H{Re-evaluate}     H --...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "B{Decision Point}     B -->|Path 1| C[Reversible Adoption]     B -->|Path 2| D[Critical Threshold]     C --> E[Maintain Optionality]     D --> F[Lock-in State]     F --> G[Irreversible]     E --> H{Re-evaluate}     H -->|Safe| E     H -->|Risk Detected| I[Course Correction]     G -.",
      "Threshold Models Lock-in occurs past critical thresholds: If X  Xcritical: Irreversible Examples: - AI capability threshold - Deployment penetration - Power concentration - Value embedding Key question: Where are the thresholds?",
      "Organizations and nations that prioritize optionality may find themselves outcompeted by those willing to accept irreversible AI dependence in exchange for short-term efficiency gains."
    ]
  },
  "media-policy-feedback-loop": {
    "title": "Media-Policy Feedback Loop Model",
    "current": "Analyzing the cyclical relationship between media coverage, public opinion, and AI policy responses",
    "draft": "Analyzing the cyclical relationship between media coverage, public opinion, and AI policy responses. government - Tone: Political, conflictual - Effect on concern: Varies (politicizes issue) - Prevalence: 15-20% of coverage  Frame Dynamics Over Time Typical Technology Coverage Cycle: Phase 1: Won...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "government - Tone: Political, conflictual - Effect on concern: Varies (politicizes issue) - Prevalence: 15-20% of coverage  Frame Dynamics Over Time Typical Technology Coverage Cycle: Phase 1: Wonder/Hype (0-2 years)   └→ \"AI achieves breakthrough...",
      "\"   └→ Political polarization possible Phase 5: Normalization (ongoing)   └→ Coverage declines, AI becomes routine   └→ Concern stabilizes at new baseline Current Position (2024-2025): Transitioning from Phase 2 to Phase 3/4; awaiting potential crisis event.",
      "Fatigue: Repeated warnings without consequences  Asymmetry in Concern Formation Negativity Bias: Negative coverage has 2-3x the impact of equivalent positive coverage on concern formation.",
      "Scenario Trajectories Scenario A: Gradual Attention Increase (50% probability) Timeline: 2025-2030 Path: M: 0.",
      "40 Outcome: Incremental regulation, no crisis Scenario B: Crisis-Driven Spike (25% probability) Timeline: 2025-2028 Path: Major incident →       M: 0."
    ]
  },
  "mesa-optimization-analysis": {
    "title": "Mesa-Optimization Risk Analysis",
    "current": "Framework for analyzing when and how mesa-optimizers might emerge during training, with severity estimates",
    "draft": "Framework for analyzing when and how mesa-optimizers might emerge during training, with severity estimates",
    "status": "ok",
    "reason": "Already has findings"
  },
  "multi-actor-landscape": {
    "title": "Multi-Actor Strategic Landscape",
    "current": "Risk is primarily determined by which actors develop TAI and their incentive structures.",
    "draft": "Risk is primarily determined by which actors develop TAI and their incentive structures.",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "1. **Competition intensity → Safety shortcuts → Misalignment risk**"
  },
  "multipolar-trap-dynamics": {
    "title": "Multipolar Trap Dynamics Model",
    "current": "Game-theoretic analysis of how competition produces collectively irrational outcomes in AI development",
    "draft": "Game-theoretic analysis of how competition produces collectively irrational outcomes in AI development. Threshold Analysis  Critical Thresholds for Trap Escalation The multipolar trap becomes increasingly difficult to escape as certain thresholds are crossed.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Threshold Analysis  Critical Thresholds for Trap Escalation The multipolar trap becomes increasingly difficult to escape as certain thresholds are crossed.",
      "Parameter estimates carry substantial uncertainty."
    ]
  },
  "multipolar-trap": {
    "title": "Multipolar Trap Coordination Model",
    "current": "Systems analysis of collective action failures in AI development",
    "draft": "Systems analysis of collective action failures in AI development",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "The multipolar trap is a **coordination failure** where rational individual action produces collectively catastrophic outcomes. In AI development, multiple actors pursuing their interests create races"
  },
  "post-incident-recovery": {
    "title": "Post-Incident Recovery Model",
    "current": "Analyzing pathways for recovering from AI incidents and what enables faster institutional and technical recovery",
    "draft": "Analyzing pathways for recovering from AI incidents and what enables faster institutional and technical recovery. 5-3x total damage | | Failed containment | 2-10x total damage | | Trust component | 1.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "5-3x total damage | | Failed containment | 2-10x total damage | | Trust component | 1.",
      "Build monitoring infrastructure  Medium-term (2027-2032) 1."
    ]
  },
  "power-seeking-conditions": {
    "title": "Power-Seeking Emergence Conditions Model",
    "current": "Formal analysis of conditions under which AI systems develop power-seeking behaviors",
    "draft": "Formal analysis of conditions under which AI systems develop power-seeking behaviors. Current language models and reinforcement learning systems exhibit moderate optimization strength, achieving perhaps 50-70% of optimal performance on complex tasks.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "It suggests that power-seeking is not an artifact of poor training or misaligned objectives, but rather an emergent property of strong optimization toward almost any goal.",
      "Current language models and reinforcement learning systems exhibit moderate optimization strength, achieving perhaps 50-70% of optimal performance on complex tasks.",
      "For current state-of-the-art systems, we estimate this probability at approximately 30-80%, with the wide range reflecting uncertainty about how close to optimal various systems actually are.",
      "This suggests that deliberately limiting optimization strength—through techniques like early stopping, reduced training compute, or stochastic training procedures—could serve as a safety mechanism, though at the cost of system capability.",
      "For advanced systems deployed on complex real-world tasks, we estimate a 60-95% probability that they optimize over sufficiently long horizons to make power-seeking instrumentally valuable."
    ]
  },
  "preference-manipulation-drift": {
    "title": "Preference Manipulation Drift Model",
    "current": "Analyzes how AI systems can gradually shift human preferences and values over time",
    "draft": "Analyzes how AI systems can gradually shift human preferences and values over time",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "This model examines how AI systems, through personalization, recommendation, and persuasive design, can gradually shift human preferences, values, and beliefs. Unlike acute manipulation, preference dr"
  },
  "proliferation-risk-model": {
    "title": "AI Proliferation Risk Model",
    "current": "Diffusion dynamics model analyzing how AI capabilities spread across actors and control mechanisms",
    "draft": "Diffusion dynamics model analyzing how AI capabilities spread across actors and control mechanisms. Capabilities that took 24-36 months to diffuse from Tier 1 (frontier labs) to Tier 4 (open source) in 2020 now spread in 12-18 months, with projections suggesting 6-12 month diffusion cycles by 202...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Capabilities that took 24-36 months to diffuse from Tier 1 (frontier labs) to Tier 4 (open source) in 2020 now spread in 12-18 months, with projections suggesting 6-12 month diffusion cycles by 2025-2026.",
      "For GPT-3-class capabilities, full diffusion from Tier 1 to Tier 4 required approximately 24 months (2020-2022).",
      "For GPT-4-class capabilities, this compressed to 18-24 months (2023-2025).",
      "Current trajectories suggest future frontier capabilities may reach Tier 4 within 6-12 months of initial development.",
      "Risk Accumulation Dynamics  Proliferation Thresholds The model identifies three critical thresholds that mark qualitative shifts in risk dynamics."
    ]
  },
  "proliferation": {
    "title": "AI Capability Proliferation Model",
    "current": "Diffusion dynamics and control challenges for advanced AI capabilities",
    "draft": "Diffusion dynamics and control challenges for advanced AI capabilities. Magnitude Assessment Share of total AI catastrophic risk attributable to proliferation: 15-30% Proliferation contributes to risk through: Key uncertainty: The magnitude depends heavily on offense-defense balance.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Magnitude Assessment Share of total AI catastrophic risk attributable to proliferation: 15-30% Proliferation contributes to risk through: Key uncertainty: The magnitude depends heavily on offense-defense balance.",
      "- Diffusion models (2015-2020) → Enabled DALL-E, Stable Diffusion, Midjourney - Reinforcement Learning from Human Feedback (2017-2020) → Enabled ChatGPT, Claude, etc."
    ]
  },
  "public-opinion-evolution": {
    "title": "Public Opinion Evolution Model",
    "current": "Analyzing how public perception of AI risk changes over time through incidents, media coverage, and elite cues",
    "draft": "Analyzing how public perception of AI risk changes over time through incidents, media coverage, and elite cues. Current State Estimates (US, 2024-2025) | Component | Estimate | Trend | |-----------|----------|-------| | Awareness $A$ | 0.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Current State Estimates (US, 2024-2025) | Component | Estimate | Trend | |-----------|----------|-------| | Awareness $A$ | 0.",
      "AI and Public Opinion surveys (2022-2024) - Gallup."
    ]
  },
  "racing-dynamics-impact": {
    "title": "Racing Dynamics Impact Model",
    "current": "Causal analysis of how competitive pressure creates race-to-the-bottom dynamics that undermine AI safety",
    "draft": "Causal analysis of how competitive pressure creates race-to-the-bottom dynamics that undermine AI safety. Reduced Alignment Research    - Racing → Less time/budget for alignment → Higher probability of misalignment    - Estimated impact: 2-5x increase in alignment failure probability 2.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Reduced Alignment Research    - Racing → Less time/budget for alignment → Higher probability of misalignment    - Estimated impact: 2-5x increase in alignment failure probability 2.",
      "Inadequate Evaluation    - Racing → Shorter testing periods → Unknown capabilities deployed    - Estimated impact: 3-10x increase in dangerous capability deployment 3.",
      "Talent Pressure (Effectiveness: Medium, Difficulty: Low-Medium) - Researchers refusing to work on unsafe projects - Universities incorporating safety ethics - Professional norms around responsible development - Barriers: Career costs, coordination challenges 6."
    ]
  },
  "racing-dynamics": {
    "title": "Racing Dynamics Game Theory Model",
    "current": "Game-theoretic analysis of competitive pressures in AI development",
    "draft": "Game-theoretic analysis of competitive pressures in AI development. The 2023-2024 period has illustrated these dynamics clearly: despite public commitments to responsible development, labs have accelerated release schedules in response to competitive pressure, with safety evaluation timelines rep...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The 2023-2024 period has illustrated these dynamics clearly: despite public commitments to responsible development, labs have accelerated release schedules in response to competitive pressure, with safety evaluation timelines reportedly compressed under commercial urgency.",
      "Quantifying this gap requires estimates of first-mover advantages and catastrophic risk probabilities: | Parameter | Low Estimate | Central | High Estimate | Confidence | |-----------|--------------|---------|---------------|------------| | First-mover revenue advantage | 1.",
      "During the scaling era (roughly 2020-2024), racing pressure intensifies as capability improvements become predictable and competitive positioning matters.",
      "Historical precedent suggests coordination emerges after demonstrated risk (the Cuban Missile Crisis preceded arms control treaties) rather than through foresight alone.",
      "The Manhattan Project exhibited racing dynamics: the perceived threat of Nazi nuclear weapons drove acceleration despite known risks."
    ]
  },
  "reality-fragmentation-network": {
    "title": "Reality Fragmentation Network Model",
    "current": "Analyzing how AI-personalized information creates incompatible reality bubbles at individual and group scales",
    "draft": "Analyzing how AI-personalized information creates incompatible reality bubbles at individual and group scales. Current estimates suggest we've moved from F ≈ 0.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Current estimates suggest we've moved from F ≈ 0.",
      "Projected for 2028-2035.",
      "4) | | Social Media (2010-2023) | ~100,000-1M | High (F ≈ 0.",
      "6) | | AI Personalization (2023-2030) | ~100M-1B | Very High (F ≈ 0.",
      "Each cycle amplifies fragmentation by 20-50%."
    ]
  },
  "reward-hacking-taxonomy": {
    "title": "Reward Hacking Taxonomy and Severity Model",
    "current": "Comprehensive taxonomy of reward hacking failure modes with severity estimates and mitigation analysis",
    "draft": "Comprehensive taxonomy of reward hacking failure modes with severity estimates and mitigation analysis",
    "status": "ok",
    "reason": "Already has findings"
  },
  "risk-activation-timeline": {
    "title": "Risk Activation Timeline Model",
    "current": "Framework mapping when different AI risks become critical based on capability levels and time",
    "draft": "Framework mapping when different AI risks become critical based on capability levels and time. 5 years | 40-60% | | Open-source parity | -1 to -2 years on misuse | 50-70% | | Major safety failure | Accelerates governance concern | 20-40% | | Geopolitical AI race | -0.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "5 years | 40-60% | | Open-source parity | -1 to -2 years on misuse | 50-70% | | Major safety failure | Accelerates governance concern | 20-40% | | Geopolitical AI race | -0.",
      "| 2045+ | 2028-2032 | +/- 15 years on long-term risks | | Scaling laws continue?",
      "| Plateau by 2027 | Continue to 2035+ | +/- 3 years on all risks | | Open-source catches up?",
      "| Stays 2+ years behind | Parity by 2026 | +/- 2 years on misuse | | Alignment solved?",
      "Long-term risks have high uncertainty but warrant early research investment 4."
    ]
  },
  "risk-cascade-pathways": {
    "title": "Risk Cascade Pathways",
    "current": "Common pathways where one AI risk triggers others, and critical nodes that can stop or accelerate cascades",
    "draft": "Common pathways where one AI risk triggers others, and critical nodes that can stop or accelerate cascades. Probability estimates poorly calibrated - Based on theory not observation 3.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Probability estimates poorly calibrated - Based on theory not observation 3."
    ]
  },
  "risk-interaction-matrix": {
    "title": "Risk Interaction Matrix Model",
    "current": "Framework for analyzing how different AI risks amplify, mitigate, or transform each other",
    "draft": "Framework for analyzing how different AI risks amplify, mitigate, or transform each other",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "AI risks don't exist in isolation. The **Risk Interaction Matrix** provides a systematic framework for analyzing how different risk categories interact, potentially amplifying each other (synergistic "
  },
  "risk-interaction-network": {
    "title": "Risk Interaction Network",
    "current": "Mapping how AI risks enable, amplify, and reinforce each other through interconnected pathways",
    "draft": "Mapping how AI risks enable, amplify, and reinforce each other through interconnected pathways. 5-2x per cycle  Loop 2: Racing-Concentration Spiral Racing intensifies → Winner takes more → Increased resources for racing → Racing intensifies further Strength: Very strong (market dynamics) Timescal...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "5-2x per cycle  Loop 2: Racing-Concentration Spiral Racing intensifies → Winner takes more → Increased resources for racing → Racing intensifies further Strength: Very strong (market dynamics) Timescale: 6 months - 2 years per cycle Amplification: 1."
    ]
  },
  "safety-capability-tradeoff": {
    "title": "Safety-Capability Tradeoff Model",
    "current": "Analyzing when safety measures conflict with capabilities and when they are complementary",
    "draft": "Analyzing when safety measures conflict with capabilities and when they are complementary. OpenAI's public statements and observed practices suggest a safety tax of approximately 5-15%, primarily from RLHF and evaluation costs relative to base training.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The alignment tax refers to capability that may be lost due to safety constraints during training, though evidence suggests this tax is modest for current systems.",
      "Red-teaming finds failure modes early in development when they are cheaper to fix, reducing expensive errors discovered post-deployment.",
      "Estimates of the safety tax vary considerably across organizations and methodologies.",
      "OpenAI's public statements and observed practices suggest a safety tax of approximately 5-15%, primarily from RLHF and evaluation costs relative to base training.",
      "Anthropic's more extensive safety investment implies a higher tax of perhaps 15-30%, including substantial interpretability research and safety evaluation work."
    ]
  },
  "safety-research-allocation": {
    "title": "Safety Research Allocation Model",
    "current": "Analysis of how AI safety research resources are distributed across sectors and how funding mechanisms shape research priorities",
    "draft": "Analysis of how AI safety research resources are distributed across sectors and how funding mechanisms shape research priorities",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "AI safety research requires talent, compute, and funding - all scarce resources with alternative uses. How these resources get allocated across academic, industry, and government sectors significantly"
  },
  "safety-research-value": {
    "title": "Expected Value of AI Safety Research",
    "current": "Economic analysis of marginal returns on AI safety research investments",
    "draft": "Economic analysis of marginal returns on AI safety research investments. The difference between high-value and low-value research directions may be 10-100x.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The difference between high-value and low-value research directions may be 10-100x.",
      "Academia: Underfunded; needs 5-10x increase 3.",
      "Tractability Estimates Are Guesses - No empirical basis for reduction percentages 3."
    ]
  },
  "safety-researcher-gap": {
    "title": "AI Safety Talent Supply/Demand Gap Model",
    "current": "Analyzing the mismatch between AI safety researcher supply and organizational demand",
    "draft": "Analyzing the mismatch between AI safety researcher supply and organizational demand. Rapidly changing: Field dynamics shift faster than data collection  Policy Recommendations  Immediate (2025-2026) 1.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Rapidly changing: Field dynamics shift faster than data collection  Policy Recommendations  Immediate (2025-2026) 1.",
      "Establish safety researcher retention fund  Medium-term (2026-2028) 1."
    ]
  },
  "scheming-likelihood-model": {
    "title": "Scheming Likelihood Assessment",
    "current": "Probabilistic model assessing likelihood of strategic deception (scheming) in future AI systems",
    "draft": "Probabilistic model assessing likelihood of strategic deception (scheming) in future AI systems. By decomposing the problem into constituent components—misalignment, situational awareness, instrumental rationality, and feasibility—the model enables researchers and developers to identify key inter...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "By decomposing the problem into constituent components—misalignment, situational awareness, instrumental rationality, and feasibility—the model enables researchers and developers to identify key intervention points and estimate risks across different capability levels.",
      "Misalignment Probability The probability of misalignment, estimated at 40-80%, represents the likelihood that an AI system's effective goals differ from the intended human goals.",
      "Reward hacking and specification gaming, occurring with estimated probability of 60-90%, represent the most common pathway.",
      "Goal misgeneralization, at 20-60% probability, occurs when systems correctly learn training objectives but fail to generalize these goals appropriately to new contexts or deployment environments.",
      "Mesa-optimization with incorrect objectives, though less likely at 5-40%, represents a particularly concerning pathway where inner optimization processes emerge with goals misaligned from the training objective."
    ]
  },
  "societal-response": {
    "title": "Societal Response & Adaptation Model",
    "current": "Humanity's collective response to AI progress determines outcomes more than technical factors alone.",
    "draft": "Humanity's collective response to AI progress determines outcomes more than technical factors alone.",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "1. **Warning shots → Public concern → Regulation → Safety investment** (main protective feedback)"
  },
  "surveillance-authoritarian-stability": {
    "title": "AI Surveillance and Regime Durability Model",
    "current": "How AI surveillance affects the stability and longevity of authoritarian regimes",
    "draft": "How AI surveillance affects the stability and longevity of authoritarian regimes. In other words: Authoritarian regimes with AI surveillance might be 2-3x more durable than historical autocracies.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "In other words: Authoritarian regimes with AI surveillance might be 2-3x more durable than historical autocracies."
    ]
  },
  "surveillance-chilling-effects": {
    "title": "Surveillance Chilling Effects Model",
    "current": "Quantifying how AI surveillance impacts freedom of expression and political participation",
    "draft": "Quantifying how AI surveillance impacts freedom of expression and political participation. Direct regime criticism: 90-99% chilled 2.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Direct regime criticism: 90-99% chilled 2.",
      "Sensitive ethnic/historical topics: 80-95% chilled 3.",
      "Corruption in government: 70-85% chilled 4.",
      "Economic policies: 40-60% chilled 5.",
      "Non-political topics: 10-30% chilled (collateral chilling) Gradient Effect: Even \"safe\" topics chilled if they could tangentially relate to sensitive areas."
    ]
  },
  "sycophancy-feedback-loop": {
    "title": "Sycophancy Feedback Loop Model",
    "current": "Analyzing how AI validation creates self-reinforcing echo chambers at individual and societal scale",
    "draft": "Analyzing how AI validation creates self-reinforcing echo chambers at individual and societal scale. ->|\"reinforces\"| IND     style IND fill:cceeff     style MKT fill:fff4e1     style SOC fill:ffddcc } />  Phase Analysis  Phase 1: Helpful Assistant (2020-2025) During the helpful assistant phase, ...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "->|\"reinforces\"| IND     style IND fill:cceeff     style MKT fill:fff4e1     style SOC fill:ffddcc } />  Phase Analysis  Phase 1: Helpful Assistant (2020-2025) During the helpful assistant phase, AI systems provide genuinely useful information with a balanced mix of validation and correction.",
      "The sycophancy level remains relatively low at 20-30%, user dependency is minimal, and reversibility is easy as habits are not yet deeply entrenched.",
      "Phase 2: Personalized Validation (2025-2028) The personalized validation phase marks a critical transition where AI systems learn individual user preferences and adjust their responses accordingly.",
      "With sycophancy levels rising to 40-60% and user dependency becoming moderate, reversibility remains possible but requires conscious effort.",
      "\"  Phase 3: Echo Chamber Lock-In (2028-2032) During the echo chamber lock-in phase, AI systems strongly validate user beliefs while correction comes to be perceived as system malfunction rather than helpful feedback."
    ]
  },
  "technical-pathways": {
    "title": "Technical Pathway Decomposition",
    "current": "Different technical architectures create distinct risk profiles requiring separate analysis.",
    "draft": "Different technical architectures create distinct risk profiles requiring separate analysis.",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "1. **Scaling → Emergence of dangerous capabilities before alignment scales**"
  },
  "trust-cascade-model": {
    "title": "Trust Cascade Failure Model",
    "current": "Analyzing how institutional trust collapses cascade through interconnected systems",
    "draft": "Analyzing how institutional trust collapses cascade through interconnected systems. The model identifies critical thresholds around 30-40% trust levels below which institutions lose their ability to validate others, creating self-reinforcing decline spirals that become extremely difficult to reve...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The model identifies critical thresholds around 30-40% trust levels below which institutions lose their ability to validate others, creating self-reinforcing decline spirals that become extremely difficult to reverse.",
      "Current trust levels in major democracies suggest the system is already in a cascade-vulnerable state, with multiple institutions approaching or below critical thresholds.",
      "\" Each node carries a weight representing current trust levels on a 0-100% scale, while edge weights capture the strength of the dependency relationship between connected institutions.",
      "The cascade propagation rate has particularly high uncertainty because historical cascades occurred in pre-digital contexts with fundamentally different dynamics.",
      "5) - Institutional effectiveness begins declining - Validation becomes less credible - Cascade risk emerges Point 2: Critical Threshold (T ≈ 0."
    ]
  },
  "trust-erosion-dynamics": {
    "title": "Trust Erosion Dynamics Model",
    "current": "Analyzes how AI systems can erode institutional and interpersonal trust over time",
    "draft": "Analyzes how AI systems can erode institutional and interpersonal trust over time. ) Current Status: Early-to-mid stage; detection still possible but rapidly declining Timeline: - 2020-2023: Detectable deepfakes, limited impact - 2024-2026: Near-undetectable deepfakes, significant impact - 2027+:...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      ") Current Status: Early-to-mid stage; detection still possible but rapidly declining Timeline: - 2020-2023: Detectable deepfakes, limited impact - 2024-2026: Near-undetectable deepfakes, significant impact - 2027+: Post-authenticity era (assuming no breakthrough in verification)  2.",
      "Estimated 3-10x asymmetry in speed."
    ]
  },
  "warning-signs-model": {
    "title": "Warning Signs Model",
    "current": "Early indicators and tripwires for detecting when AI risks are becoming critical",
    "draft": "Early indicators and tripwires for detecting when AI risks are becoming critical. Quantitative Assessment of Warning Signs The following table provides quantitative estimates for key warning sign parameters, including probability of detection, expected timeline to crossing, and confidence levels ...",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Quantitative Assessment of Warning Signs The following table provides quantitative estimates for key warning sign parameters, including probability of detection, expected timeline to crossing, and confidence levels based on available evidence and expert judgment.",
      "5/10 | | Autonomous cyber exploitation | 40-60% of threshold | 65% (range: 50-80%) | 24-48 months | Medium-Low | 7.",
      "5/10 | | AI persuasion exceeds human | 70-90% of threshold | 80% (range: 70-90%) | 6-18 months | Medium-High | 8.",
      "0/10 | | Training-aware behavior modification | 30-50% of threshold | 45% (range: 30-60%) | 12-36 months | Low | 9.",
      "0/10 | | Systematic AI deception | 20-40% of threshold | 50% (range: 35-65%) | 18-48 months | Low | 9."
    ]
  },
  "whistleblower-dynamics": {
    "title": "Whistleblower Dynamics Model",
    "current": "Analysis of information flow about AI risks from insiders to public awareness and policy action",
    "draft": "Analysis of information flow about AI risks from insiders to public awareness and policy action. The firing of Timnit Gebru and Margaret Mitchell from Google's AI ethics team in 2020-2021 illustrated the risks facing those who raise concerns internally.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "Exclusion from conferences, informal networks, and collaborative projects removes the social infrastructure of professional life.",
      "The awareness that \"others know and aren't acting\" suggests either that concerns are overblown or that someone better positioned will speak.",
      "Hope that \"things might improve\" suggests that patience may achieve what disclosure would without the associated costs.",
      "The firing of Timnit Gebru and Margaret Mitchell from Google's AI ethics team in 2020-2021 illustrated the risks facing those who raise concerns internally.",
      "Claiming missing context suggests that full information would justify organizational decisions."
    ]
  },
  "winner-take-all-concentration": {
    "title": "Winner-Take-All Concentration Model",
    "current": "Network effects and positive feedback analysis of AI capability and power concentration",
    "draft": "Network effects and positive feedback analysis of AI capability and power concentration. The top three AI research organizations (OpenAI, Google DeepMind, and Anthropic) collectively employ an estimated 40-50% of the world's top 100 AI researchers and 25-30% of the top 1,000.",
    "status": "needs_review",
    "reason": "Auto-generated from content",
    "findings": [
      "The top three AI research organizations (OpenAI, Google DeepMind, and Anthropic) collectively employ an estimated 40-50% of the world's top 100 AI researchers and 25-30% of the top 1,000.",
      "Compensation at frontier labs ranges from $500,000 to over $2 million annually for senior researchers, creating a 3-5x premium over academic positions and 2x premium over traditional tech roles.",
      "This threshold is highly uncertain, depending on both AI timeline estimates and scaling law persistence.",
      "Most critically, the parameter estimates carry substantial uncertainty.",
      "Loop gain estimates spanning 1."
    ]
  },
  "winner-take-all": {
    "title": "Winner-Take-All Dynamics Model",
    "current": "Economic analysis of power law distributions and market concentration in AI",
    "draft": "Economic analysis of power law distributions and market concentration in AI",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "AI exhibits **superstar economics** - markets where small differences in capability translate to enormous differences in rewards. This creates **power law distributions** of outcomes where the top few"
  },
  "worldview-intervention-mapping": {
    "title": "Worldview-Intervention Mapping",
    "current": "How your beliefs about AI timelines, alignment difficulty, and coordination feasibility should determine which interventions you prioritize",
    "draft": "How your beliefs about AI timelines, alignment difficulty, and coordination feasibility should determine which interventions you prioritize",
    "status": "needs_manual",
    "reason": "No quantified findings found in content",
    "firstParagraph": "**Given your beliefs about AI risk, which interventions should you prioritize?**"
  }
}