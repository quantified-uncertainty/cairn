---
title: CAIRN
description: Strategic intelligence for AI safety prioritization - a encyclopedic resource for understanding AI risks and interventions
pageTemplate: splash
hero:
  tagline: Encyclopedia for AI safety prioritization
  actions:
    - text: Explore All Content
      link: /explore/
      icon: seti:notebook
    - text: AI Transition Model
      link: /ai-transition-model/
      icon: right-arrow
      variant: minimal
---
import {R, EntityLink} from '../../components/wiki';
import { Card, CardGrid } from '@astrojs/starlight/components';

## What Is CAIRN?

**CAIRN** (**C**omprehensive **A**I **I**mpact & **R**isk **N**avigator) is an encyclopedic resource for AI safety. It helps funders, researchers, and policymakers understand the landscape of AI risks and interventions.

The site contains 400+ pages covering risks, technical approaches, governance proposals, key debates, and the organizations and researchers working on these problems.

---

## Main Sections

<CardGrid>
  <Card title="AI Transition Model" icon="warning">
    A [causal framework](/ai-transition-model/) mapping how AI development could lead to different outcomes.

    - [Key Factors](/ai-transition-model/factors/) — What shapes AI trajectories
    - [Scenarios](/ai-transition-model/scenarios/) — Possible futures
  </Card>

  <Card title="Knowledge Base" icon="open-book">
    Encyclopedic coverage of AI safety topics:

    - [Risks](/knowledge-base/risks/) — Accident, misuse, structural, epistemic
    - [Responses](/knowledge-base/responses/) — Technical and governance approaches
    - [Models](/knowledge-base/models/) — Analytical frameworks
  </Card>

  <Card title="Key Debates" icon="comment-alt">
    Structured arguments on contested questions:

    - [Is AI X-Risk Real?](/knowledge-base/debates/is-ai-xrisk-real/)
    - [Why Alignment Might Be Hard](/knowledge-base/debates/formal-arguments/why-alignment-hard/)
    - [Why Alignment Might Be Easy](/knowledge-base/debates/formal-arguments/why-alignment-easy/)
  </Card>

  <Card title="Field Reference" icon="seti:folder">
    Who's working on what:

    - [Organizations](/knowledge-base/organizations/) — Labs, research orgs, government
    - [People](/knowledge-base/people/) — Key researchers and their positions
  </Card>
</CardGrid>

---

## Featured Pages

High-quality pages worth reading:

| Topic | Page | Description |
|-------|------|-------------|
| Risk | [Deceptive Alignment](/knowledge-base/risks/accident/deceptive-alignment/) | AI systems that appear aligned during training but pursue different goals when deployed |
| Risk | [Racing Dynamics](/knowledge-base/risks/structural/racing-dynamics/) | How competition between labs may compromise safety |
| Response | [AI Control](/knowledge-base/responses/alignment/ai-control/) | Using untrusted AI safely through monitoring and restrictions |
| Response | [Export Controls](/knowledge-base/responses/governance/compute-governance/export-controls/) | Restricting AI chip exports as a governance lever |
| Capability | [Language Models](/knowledge-base/capabilities/language-models/) | Current capabilities and safety implications of LLMs |

[Browse all content →](/explore/)

---

## Limitations

**This resource reflects an AI safety community perspective.** It takes seriously the possibility of existential risk from AI and maps the arguments, organizations, and research from that viewpoint.

What it does well:
- Steelmans the case for AI existential risk
- Maps the landscape of AI safety research and governance
- Presents the range of views within the AI safety community

What it does less well:
- Representing perspectives that reject the x-risk framing entirely
- Engaging deeply with AI ethics/fairness concerns
- Covering non-Western perspectives on AI development

**Alternative viewpoints:**
<R id="9b1ab7f63e6b1b35">Gary Marcus's Substack</R> (AI skepticism) •
<R id="4ca01f329c8b25a4">Yann LeCun's posts</R> (AGI skepticism) •
<R id="81595c2c950080a6">Timnit Gebru et al.</R> (AI ethics)

[Read full transparency statement →](/about/)
