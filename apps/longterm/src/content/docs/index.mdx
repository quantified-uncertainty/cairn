---
title: CAIRN
description: Strategic intelligence for AI safety prioritization - a encyclopedic resource for understanding AI risks and interventions
pageTemplate: splash
hero:
  tagline: Encyclopedia for AI safety prioritization
  actions:
    - text: Explore All Content
      link: /explore/
      icon: seti:notebook
    - text: AI Transition Model
      link: /ai-transition-model/
      icon: right-arrow
      variant: minimal
---
import {R, EntityLink} from '@components/wiki';
import { Card, CardGrid } from '@astrojs/starlight/components';

## What Is CAIRN?

**CAIRN** (**C**omprehensive **A**I **I**mpact & **R**isk **N**avigator) is an encyclopedic resource for AI safety. It helps funders, researchers, and policymakers understand the landscape of AI risks and interventions.

The site contains 400+ pages covering risks, technical approaches, governance proposals, key debates, and the organizations and researchers working on these problems.

---

## Main Sections

<CardGrid>
  <Card title="AI Transition Model" icon="warning">
    A <EntityLink id="__index__/ai-transition-model">causal framework</EntityLink> mapping how AI development could lead to different outcomes.

    - <EntityLink id="__index__/ai-transition-model/factors">Key Factors</EntityLink> — What shapes AI trajectories
    - <EntityLink id="__index__/ai-transition-model/scenarios">Scenarios</EntityLink> — Possible futures
  </Card>

  <Card title="Knowledge Base" icon="open-book">
    Encyclopedic coverage of AI safety topics:

    - <EntityLink id="risks">Risks</EntityLink> — Accident, misuse, structural, epistemic
    - <EntityLink id="responses">Responses</EntityLink> — Technical and governance approaches
    - <EntityLink id="models">Models</EntityLink> — Analytical frameworks
  </Card>

  <Card title="Key Debates" icon="comment-alt">
    Structured arguments on contested questions:

    - <EntityLink id="debates/is-ai-xrisk-real">Is AI X-Risk Real?</EntityLink>
    - <EntityLink id="debates/formal-arguments/why-alignment-hard">Why Alignment Might Be Hard</EntityLink>
    - <EntityLink id="debates/formal-arguments/why-alignment-easy">Why Alignment Might Be Easy</EntityLink>
  </Card>

  <Card title="Field Reference" icon="seti:folder">
    Who's working on what:

    - <EntityLink id="organizations">Organizations</EntityLink> — Labs, research orgs, government
    - <EntityLink id="people">People</EntityLink> — Key researchers and their positions
  </Card>
</CardGrid>

---

## Featured Pages

High-quality pages worth reading:

| Topic | Page | Description |
|-------|------|-------------|
| Risk | <EntityLink id="risks/accident/deceptive-alignment">Deceptive Alignment</EntityLink> | AI systems that appear aligned during training but pursue different goals when deployed |
| Risk | <EntityLink id="risks/structural/racing-dynamics">Racing Dynamics</EntityLink> | How competition between labs may compromise safety |
| Response | <EntityLink id="responses/alignment/ai-control">AI Control</EntityLink> | Using untrusted AI safely through monitoring and restrictions |
| Response | <EntityLink id="responses/governance/compute-governance/export-controls">Export Controls</EntityLink> | Restricting AI chip exports as a governance lever |
| Capability | <EntityLink id="capabilities/language-models">Language Models</EntityLink> | Current capabilities and safety implications of LLMs |

[Browse all content →](/explore/)

---

## Limitations

**This resource reflects an AI safety community perspective.** It takes seriously the possibility of existential risk from AI and maps the arguments, organizations, and research from that viewpoint.

What it does well:
- Steelmans the case for AI existential risk
- Maps the landscape of AI safety research and governance
- Presents the range of views within the AI safety community

What it does less well:
- Representing perspectives that reject the x-risk framing entirely
- Engaging deeply with AI ethics/fairness concerns
- Covering non-Western perspectives on AI development

**Alternative viewpoints:**
<R id="9b1ab7f63e6b1b35">Gary Marcus's Substack</R> (AI skepticism) •
<R id="4ca01f329c8b25a4">Yann LeCun's posts</R> (AGI skepticism) •
<R id="81595c2c950080a6">Timnit Gebru et al.</R> (AI ethics)

[Read full transparency statement →](/about/)
