---
title: CAIRN
description: Strategic intelligence for AI safety prioritization - a decision-support tool for understanding AI risks and interventions
pageTemplate: splash
hero:
  tagline: Strategic intelligence for AI safety prioritization
  actions:
    - text: Start Here
      link: /getting-started/
      icon: right-arrow
    - text: Explore All Content
      link: /explore/
      icon: information
      variant: minimal
---
import {R, EntityLink} from '../../components/wiki';

import { Card, CardGrid } from '@astrojs/starlight/components';

## What Is CAIRN?

**CAIRN** â€” **C**omprehensive **A**I **I**mpact & **R**isk **N**avigator â€” is a **decision-support tool** for AI safety prioritization. It helps funders, researchers, and policymakers answer: *"Where should the next marginal dollar or researcher-hour go?"*

Unlike traditional wikis that aim for comprehensive documentation, CAIRN focuses on **strategic clarity**: surfacing the key uncertainties and cruxes that, if resolved, would most change how resources should be allocated across AI safety interventions.

---

## Core Content

<CardGrid>
  <Card title="ðŸŽ¯ AI Transition Model" icon="warning">
    A <EntityLink id="__index__/ai-transition-model">causal framework</EntityLink> mapping how root factors shape AI outcomes.

    - <EntityLink id="__index__/ai-transition-model/factors">Root Factors</EntityLink> - What drives AI trajectories
    - <EntityLink id="__index__/ai-transition-model/scenarios">Scenarios</EntityLink> - Pathways to different outcomes
    - [Interactive Views](/ai-transition-model-views/graph/) - Explore the model visually
  </Card>

  <Card title="ðŸ’¡ Critical Insights" icon="star">
    High-value knowledge contributions that shift how informed people think:

    - [Insight Database](/insight-hunting/insights/) - 1000+ extracted insights
    - [Critical Insights Framework](/project/critical-insights/) - What makes knowledge valuable
    - [Gap Analysis](/insight-hunting/gap-analysis/) - Where research is needed
  </Card>

  <Card title="â“ Key Uncertainties" icon="comment-alt">
    Cruxes that drive disagreement and prioritization:

    - 53 documented cruxes - Key uncertainties mapped
    - Key debates - Structured arguments on contested questions
    - [Worldview Analysis](/project/vision/) - How assumptions lead to priorities
  </Card>

  <Card title="ðŸ›¡ï¸ Safety Approaches" icon="approve-check">
    Technical and governance solutions:

    - Technical approaches (interpretability, RLHF, AI control)
    - Governance (compute governance, international coordination)
    - Browse "Interventions" in the sidebar for all approaches
  </Card>

  <Card title="ðŸ“ Key Parameters" icon="setting">
    Foundational variables AI affects in both directions:

    - <EntityLink id="__index__/ai-transition-model">All parameters</EntityLink> (alignment robustness, racing intensity, societal trust)
    - A symmetric framework connecting risks and interventions
    - Track what matters, not just what could go wrong
  </Card>

  <Card title="ðŸ¢ Field Reference" icon="seti:folder">
    Who's working on what:

    - Labs, research orgs, government bodies
    - Key researcher profiles and positions
    - Browse "Organizations" and "People" in the sidebar
  </Card>
</CardGrid>

---

## Analysis & Visualization

<CardGrid>
  <Card title="ðŸ“Š History" icon="seti:clock">
    AI safety timeline from the Dartmouth conference to the present day. Browse "History" in sidebar.
  </Card>

  <Card title="ðŸ—ºï¸ Entity Graph" icon="seti:pipeline">
    [Visual dependency graph](/dashboard/graph/) showing how entities connect.
  </Card>

  <Card title="ðŸ”® Scenarios" icon="puzzle">
    Future projections for how AI development might unfold. Browse "Future Projections" in sidebar.
  </Card>

  <Card title="ðŸ“š Explore" icon="document">
    [Browse all entries](/explore/) â€” risks, responses, organizations, people, and cruxes.
  </Card>
</CardGrid>

---

## Learning Paths

**New to AI safety?** Start with:
1. <EntityLink id="__index__/getting-started">Introduction</EntityLink> - What is AI safety and why it matters
2. <EntityLink id="__index__/ai-transition-model">AI Transition Model</EntityLink> - Framework for understanding AI outcomes
3. Browse "Risks" in sidebar - What could go wrong

**Want to contribute?** Explore:
1. Browse "Interventions" in sidebar - What can you do?
2. [Explore all content](/explore/) - Browse the database
3. Browse "Organizations" in sidebar - Who's working on this

**Looking for depth?** Try:
1. <EntityLink id="__index__/ai-transition-model">Key Parameters</EntityLink> - The variables that matter for AI outcomes
2. Browse "Key Debates" in sidebar - Strongest arguments on each side
3. <EntityLink id="research-agendas">Research agendas</EntityLink> - Compare approaches

---

## Key Numbers

| Question | Estimates |
|----------|-----------|
| **P(transformative AI by 2040)** | 40-80% (varies by source) |
| **P(doom) estimates** | 5-90% (wide disagreement) |
| **AI safety researchers** | ~300-1000 FTE |
| **Annual safety funding** | ~\$100-500M |
| **Frontier lab safety spend** | ~\$50-200M combined |

See the [dashboard](/dashboard/) for more details.

---

## Featured People

Key voices in AI safety:

- **<EntityLink id="dario-amodei">Dario Amodei</EntityLink>** - Anthropic CEO, "10-25% doom"
- **<EntityLink id="eliezer-yudkowsky">Eliezer Yudkowsky</EntityLink>** - MIRI, most pessimistic public voice
- **<EntityLink id="paul-christiano">Paul Christiano</EntityLink>** - ARC founder, influential alignment researcher
- **<EntityLink id="geoffrey-hinton">Geoffrey Hinton</EntityLink>** - "Godfather of AI", recent safety advocate
- **<EntityLink id="stuart-russell">Stuart Russell</EntityLink>** - UC Berkeley, *Human Compatible* author

Browse "People" in the sidebar to see all researchers.

---

## This Wiki Is...

âœ… **Comprehensive** â€” Covers technical, governance, and strategic perspectives

âœ… **Structured** â€” Organized by cruxes, not just topics

âœ… **Parameter-oriented** â€” Tracks foundational variables, not just risks

âœ… **Interactive** â€” Timeline, risk maps, argument maps

âœ… **Practical** â€” Career and funding guidance

---

## Limitations & Perspective

**This wiki is not neutral.** It was created within the AI safety community and reflects that perspective. While we strive to present counterarguments fairly, readers should be aware:

**What this wiki does well:**
- Steelmans the case for AI existential risk
- Maps the landscape of AI safety arguments, organizations, and research
- Presents the range of views *within* the AI safety community

**What this wiki does less well:**
- Representing perspectives that reject the x-risk framing entirely
- Engaging deeply with AI ethics/fairness concerns (often dismissed as "near-term")
- Covering non-Western perspectives on AI development
- Quantifying uncertainty honestly (probability estimates should be treated as rough intuitions)

**Key assumptions embedded in this wiki:**
- That "existential risk" is a coherent concept for AI
- That the theoretical arguments for concern (orthogonality, instrumental convergence) are basically sound
- That the AI safety community's research agenda is on the right track

**If you're skeptical of these assumptions**, this wiki may still be useful for understanding what AI safety researchers believe and whyâ€”but you should seek out alternative perspectives as well.

**Recommended alternative viewpoints:**
- <R id="9b1ab7f63e6b1b35">Gary Marcus's Substack</R> â€” AI skepticism
- <R id="81595c2c950080a6">Timnit Gebru et al.'s work</R> â€” AI ethics perspective
- <R id="4ca01f329c8b25a4">Yann LeCun's posts</R> â€” Skepticism of AGI/x-risk framing
- <R id="10202ee006b2ebdf">Emily Bender's work</R> â€” Linguistic critique of LLM capabilities

**[Read our full transparency statement â†’](/about/)**

---

## Contributing

This is an open project. Key areas where contributions would be valuable:
- Adding researcher profiles
- Updating organization pages
- Improving argument maps
- Adding sources and citations

---

<CardGrid>
  <Card title="Explore by Topic" icon="magnifier">
    Browse the sidebar to explore specific topics, risks, and organizations.
  </Card>

  <Card title="Start Learning" icon="open-book">
    <EntityLink id="__index__/getting-started">Begin with the introduction â†’</EntityLink>
  </Card>
</CardGrid>
