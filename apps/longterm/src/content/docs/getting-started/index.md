---
title: Start Here
description: A guide to navigating this AI safety wiki
slug: getting-started
sidebar:
  label: Start Here
  order: 0
---
import {EntityLink} from '../../../components/wiki';

This wiki maps the landscape of AI existential risk—the arguments, key uncertainties, organizations, and interventions. Here's how to navigate it.

## Core Framework: Key Parameters

The wiki is organized around **<EntityLink id="__index__/ai-transition-model">Key Parameters</EntityLink>**—foundational variables that AI development affects in both directions. This framework connects:

- **Risks** — What decreases these parameters (51 documented risks)
- **Responses** — What increases or protects them (technical & governance approaches)

Parameters include things like alignment robustness, racing intensity, societal trust, and human agency. Start with the <EntityLink id="__index__/ai-transition-model">Parameters overview</EntityLink> to understand the analytical framework.

## Main Sections

| Section | What's There |
|---------|--------------|
| **<EntityLink id="__index__/ai-transition-model">Key Parameters</EntityLink>** | 22 foundational variables with trends, risks, and interventions |
| **[Explore All Content](/explore/)** | Browse risks, responses, organizations, people, debates, and cruxes |

## Quick Paths

**Want to understand the risk argument?**
→ <EntityLink id="__index__/ai-transition-model">AI Transition Model</EntityLink> presents the framework for understanding AI outcomes

**Want to see what can be done?**
→ Browse interventions in the sidebar under "Interventions"

**Want to understand disagreements?**
→ Browse key debates in the sidebar under "Key Debates"

**Want data and estimates?**
→ Browse metrics in the sidebar under "Key Metrics"

## Key Numbers

| Question | Range |
|----------|-------|
| P(transformative AI by 2040) | 40-80% |
| P(doom) estimates | 5-90% |
| AI safety researchers | ~300-1000 FTE |
| Annual safety funding | ~$100-500M |

## This Wiki's Perspective

This wiki was created within the AI safety community and reflects that perspective. It:

- Maps arguments, organizations, and research in the field
- Presents the range of views *within* AI safety
- Uses the Key Parameters framework to connect risks and responses
- Does not claim neutrality—see the [About page](/about/) for limitations

If you're skeptical of the AI safety framing, this wiki can help you understand what researchers believe and why.

## Browse

- **[Explore All Content](/explore/)** — Searchable database
- **[Entity Graph](/dashboard/graph/)** — Visual relationships
