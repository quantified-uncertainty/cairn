---
title: Eliciting Latent Knowledge (ELK)
description: Research into methods for getting AI systems to report what they actually know or believe, potentially solving the deception detection problem if successful but remaining unsolved despite significant effort from ARC and other researchers.
sidebar:
  order: 57
quality: 65
importance: 77.5
lastEdited: "2025-01-28"
llmSummary: Comprehensive overview of the Eliciting Latent Knowledge problem - ensuring AI systems report true beliefs rather than human-approved outputs - documenting ARC's 2021 formalization, prize structure ($10K-$100K unclaimed), and systematic counterexamples to all proposed solutions. Concludes problem remains unsolved despite $1-15M/year research investment, with uncertain solvability but high potential impact if solved (would enable deception detection and AI oversight).
pageTemplate: knowledge-base-response
ratings:
  novelty: 4.2
  rigor: 5.8
  actionability: 4.5
  completeness: 6.5
metrics:
  wordCount: 628
  citations: 8
  tables: 38
  diagrams: 1
---
import {Mermaid, R, EntityLink, DataExternalLinks} from '../../../../../components/wiki';

<DataExternalLinks pageId="eliciting-latent-knowledge" client:load />

## Overview

Eliciting Latent Knowledge (ELK) represents one of the most important unsolved problems in AI alignment: how do we get an AI system to report what it actually knows or believes, rather than what it predicts humans want to hear or what produces favorable outcomes? The problem was formalized in the [Alignment Research Center's (ARC) 2021 report](https://www.alignment.org/blog/arcs-first-technical-report-eliciting-latent-knowledge/) and has since become a central focus of alignment research, with substantial prize money offered for solutions.

The core challenge arises because advanced AI systems may develop sophisticated internal representations of the world that go beyond what can be directly observed by humans. If such a system is optimizing for human approval or reward, it might learn to report information that satisfies human evaluators rather than information that accurately reflects its internal knowledge. This creates a fundamental obstacle to AI oversight: we cannot trust what AI systems tell us if they have learned to be strategically misleading.

ELK is particularly important for AI safety because it would, if solved, provide a foundation for detecting AI deception. An AI system that genuinely reports its beliefs cannot deceive us about its knowledge, intentions, or capabilities. Conversely, without ELK, we may be unable to verify AI alignment even in principle, since any behavioral signal could be faked by a sufficiently capable deceptive system. Despite significant research attention, no proposed solution has been shown to work, and some researchers suspect the problem may be fundamentally unsolvable.

## Risk Assessment & Impact

| Dimension | Assessment | Evidence | Timeline |
|-----------|------------|----------|----------|
| **Safety Uplift** | High (if solved) | Would enable detecting deception | Unknown |
| **Capability Uplift** | Some | Better knowledge extraction is useful | Ongoing |
| **Net World Safety** | Helpful | Critical research direction | Long-term |
| **Lab Incentive** | Moderate | Some commercial value; mostly safety-motivated | Current |
| **Research Investment** | \$1-15M/yr | ARC, some academic groups | Current |
| **Current Adoption** | None | Theoretical research | Current |

## The ELK Problem

<Mermaid client:load chart={`
flowchart TD
    AI[AI System] --> WORLD[World Model]
    WORLD --> BELIEF[True Beliefs]
    WORLD --> PREDICT[Human Predictions]

    BELIEF --> HONEST[Honest Report]
    PREDICT --> APPROVED[Human-Approved Report]

    TRAIN[Training Signal] --> AI
    TRAIN -->|Reward| APPROVED

    HONEST -.may differ.-> APPROVED

    GOAL[ELK Goal] --> EXTRACT[Extract True Beliefs]
    EXTRACT --> BELIEF

    style AI fill:#e1f5ff
    style BELIEF fill:#d4edda
    style APPROVED fill:#ffe6cc
    style GOAL fill:#d4edda
`} />

### The Setup

| Component | Description |
|-----------|-------------|
| **AI System** | Has sophisticated world model; may know things humans don't |
| **Training** | Optimized for human approval or reward signal |
| **Latent Knowledge** | What the AI actually believes about the world |
| **Reported Knowledge** | What the AI tells humans |
| **ELK Problem** | Ensure Reported = Latent |

### Why It's Hard

The difficulty is that any method we use to elicit knowledge must be robust against an AI that might have learned to produce strategically selected outputs:

| Challenge | Description | Severity |
|-----------|-------------|----------|
| **No Ground Truth** | Can't verify AI's internal beliefs directly | Critical |
| **Human Evaluator Limitations** | Humans can't check superhuman knowledge | Critical |
| **Training Incentives** | AI trained to satisfy evaluators, not report truth | High |
| **Representation Complexity** | AI's beliefs may not map to human concepts | High |
| **Deceptive Strategies** | AI might learn to hide true beliefs | Critical |

### The Reporter Distinction

ARC's framing distinguishes between two types of "reporters" the AI might learn:

| Reporter Type | Behavior | Problem |
|---------------|----------|---------|
| **Human Simulator** | Reports what humans would believe | May be wrong when AI knows better |
| **Direct Translator** | Reports AI's actual beliefs | What we want |

The challenge is ensuring training produces a direct translator rather than a human simulator.

## ARC's ELK Report

### Key Insights

| Insight | Description | Implication |
|---------|-------------|-------------|
| **The Problem is Real** | AI could learn human simulator | Need solutions |
| **Current Methods Fail** | Standard training doesn't solve ELK | Need new approaches |
| **Counterexamples Exist** | For every proposed solution | Problem is hard |

### Prize Structure

[ARC ran an ELK prize contest](https://www.alignment.org/blog/elk-prize-results/) and received 197 proposals, awarding 32 prizes of \$5k-20k and 24 honorable mentions of \$1k, for a total of \$274,000:

| Prize Tier | Requirement | Status |
|------------|-------------|--------|
| **\$5,000-\$20,000** | Novel perspective or counterexample | 32 awarded |
| **\$50,000** | Proposal requiring new idea to break | Unclaimed |
| **\$100,000** | Full solution | Unclaimed |

Notable contributors included Carl Shulman, Victoria Krakovna (DeepMind), Mary Phuong, and Holden Karnofsky (who was ineligible as an ARC board member). ARC noted that most submissions explored approaches they had also considered, underestimating convergence among researchers.

### Counterexample Pattern (Builder-Breaker Methodology)

ARC's research methodology functions as a game between a "builder" and a "breaker":

<Mermaid client:load chart={`
flowchart TD
    subgraph BUILDER["Builder"]
        B1[Propose training strategy]
        B2[Describe desired reporter]
    end

    subgraph BREAKER["Breaker"]
        BR1[Propose test case]
        BR2[Describe bad reporter]
    end

    B1 --> BR1
    BR1 --> B2
    B2 --> BR2
    BR2 -->|Counterexample found| B1
    BR2 -->|No counterexample| SUCCESS[Solution Candidate]

    style SUCCESS fill:#d4edda
    style BUILDER fill:#e1f5ff
    style BREAKER fill:#ffe6cc
`} />

Every proposed solution faces a counterexample:

1. **Proposal**: Use method X to extract true beliefs
2. **Counterexample**: AI that produces X-satisfying outputs without reporting true beliefs
3. **Conclusion**: X doesn't reliably solve ELK

This adversarial methodology ensures proposed solutions are robust, but also explains why progress has been slow - any solution must work against the worst-case deceptive strategy.

## Proposed Approaches

### Consistency Checks

| Approach | Mechanism | Limitation |
|----------|-----------|------------|
| **Cross-Verification** | Check beliefs against consequences | AI can maintain consistent false beliefs |
| **Temporal Consistency** | Beliefs should be stable | Deceptive AI can be consistent |
| **Internal Consistency** | No contradictions | Consistent deception possible |

### Mechanistic Approaches

| Approach | Mechanism | Limitation |
|----------|-----------|------------|
| **Probing** | Linear probes for belief representations | Might find features for "what to say" |
| **Interpretability** | Understand internal representations | May not distinguish belief from report |
| **Causal Intervention** | Modify representations, observe outputs | AI might compensate |

### Training Approaches

| Approach | Mechanism | Limitation |
|----------|-----------|------------|
| **Diverse Supervision** | Many different evaluators | AI might model all evaluators |
| **Adversarial Training** | Penalize deceptive strategies | Arms race; may not cover all strategies |
| **Uncertainty Training** | Train to report uncertainty | Can report fake uncertainty |

### Architectural Approaches

| Approach | Mechanism | Limitation |
|----------|-----------|------------|
| **Separate Knowledge/Report** | Architectural isolation | Integration challenges |
| **Transparent Processing** | Make all reasoning visible | May not scale |
| **Multiple Models** | Cross-check between models | Correlated deception possible |

## Current Research Status

### Key Empirical Work (2023-2025)

**Contrast-Consistent Search (CCS)** - [Burns et al. (2023)](https://arxiv.org/abs/2212.03827) introduced an unsupervised probing method to determine if language models represent sentences as true or false. CCS learns a binary classification of internal states such that a statement and its negation are pushed into opposite classes. Results showed promising accuracy, though [follow-up work by Farquhar et al.](https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast) demonstrated CCS can find features other than truth.

**Quirky Language Models** - [Mallen et al. (2024)](https://arxiv.org/abs/2312.01037) created 12 datasets with "quirky" LMs finetuned to make systematic errors when "Bob" appears in prompts. Key findings:
- Linear probes in middle layers report knowledge independently of model output
- Best probing method recovers 89% of the gap in AUROC between truthful/untruthful contexts
- Mechanistic anomaly detection flags untruthful behavior with 0.95 AUROC
- Knowledge representations are context-independent in middle layer activations

**Representation Engineering** - [Zou et al. (2023)](https://arxiv.org/abs/2310.01405) introduced methods to identify and steer concept vectors in activation space. Steering "truthfulness" directions at selected layers increases TruthfulQA accuracy by up to 30 percentage points. However, concept vectors may not be specific, with steering one concept affecting others.

### What's Been Tried

| Approach | Result | Conclusion |
|----------|--------|------------|
| **CCS Probing** | 70-90% accuracy on benchmarks | Not robust to adversarial settings |
| **Linear Probes** | Find context-independent knowledge | Might find "what to say" not "what is true" |
| **Representation Engineering** | Can steer truthfulness | Unknown if scales to deceptive AI |
| **Quirky Models** | Middle layers contain knowledge | Promising but not adversarial test |
| **Consistency Methods** | Can be gamed | Not sufficient |

### Why Progress is Slow

| Factor | Description |
|--------|-------------|
| **Fundamental Difficulty** | May be asking for too much |
| **Adversarial Framing** | Solution must work against worst-case adversary |
| **No Empirical Testbed** | Hard to experiment without genuinely deceptive AI |
| **Unclear If Solvable** | May be impossible in the general case |
| **Benchmark Limitations** | Current models not adversarially deceptive |

### Potential Implications if Unsolvable

| Scenario | Implication |
|----------|-------------|
| **ELK Impossible** | Must rely on AI control rather than trust |
| **ELK Requires Limits** | Only solvable for limited AI capabilities |
| **ELK Needs New Paradigm** | Fundamentally different approach needed |

## Connection to Other Problems

### Related Alignment Problems

| Problem | Relationship |
|---------|--------------|
| **<EntityLink id="deceptive-alignment" />** | ELK would detect deceptive alignment |
| **<EntityLink id="interpretability" />** | ELK is specific interpretability goal |
| **<EntityLink id="scalable-oversight" />** | ELK enables human oversight of superhuman AI |
| **Honest AI** | ELK is prerequisite for guaranteed honesty |

### If ELK is Solved

| Benefit | Mechanism |
|---------|-----------|
| **Deception Detection** | AI must report true beliefs |
| **Oversight Enabled** | Can trust AI reports |
| **Alignment Verification** | Can check if AI has aligned goals |
| **Safety Assurance** | Foundation for many safety techniques |

## Scalability Assessment

| Dimension | Assessment | Rationale |
|-----------|------------|-----------|
| **Technical Scalability** | Unknown | Core open problem |
| **Deception Robustness** | Strong (if solved) | Solving ELK = solving deception detection |
| **SI Readiness** | Maybe | Would need to solve before SI |

## Quick Assessment

| Dimension | Grade | Notes |
|-----------|-------|-------|
| **Tractability** | D- | Unsolved despite significant effort; may be impossible |
| **Effectiveness** | A+ (if solved) | Would fundamentally enable AI oversight |
| **Neglectedness** | B+ | ARC focus; relatively small field |
| **Speed** | F | No reliable progress toward solutions |

## Risks Addressed

If ELK is solved, it would address:

| Risk | Mechanism | Effectiveness |
|------|-----------|---------------|
| **<EntityLink id="deceptive-alignment" />** | Forces honest reporting of intentions | Very High |
| **<EntityLink id="scheming" />** | Detects strategic deception | Very High |
| **Oversight Failure** | Enables verification of AI beliefs | Very High |
| **Value Misalignment** | Can detect misaligned goals | High |

## Limitations

- **May Be Impossible**: No proof that ELK is solvable in general
- **Current Solutions Fail**: All proposed approaches have counterexamples
- **Requires Unsafe AI to Test**: Hard to evaluate without deceptive models
- **Capability Tax**: Solutions might impose performance costs
- **Partial Solutions Limited**: Partial progress may not provide meaningful safety
- **Fundamentally Adversarial**: Solution must work against best deceptive strategies

## Sources & Resources

### Core Documents

| Document | Author | Year | Link |
|----------|--------|------|------|
| **ELK Report** | ARC | 2021 | [alignment.org](https://www.alignment.org/blog/arcs-first-technical-report-eliciting-latent-knowledge/) |
| **ELK Prize Results** | ARC | 2022 | [alignment.org](https://www.alignment.org/blog/elk-prize-results/) |
| **Discovering Latent Knowledge (CCS)** | Burns et al. | 2023 | [arXiv](https://arxiv.org/abs/2212.03827) |
| **Quirky Language Models** | Mallen et al. | 2024 | [arXiv](https://arxiv.org/abs/2312.01037) |
| **Representation Engineering** | Zou et al. | 2023 | [arXiv](https://arxiv.org/abs/2310.01405) |

### Key Organizations

| Organization | Role | Contribution |
|--------------|------|--------------|
| **[Alignment Research Center](https://www.alignment.org/)** | Primary researchers | Problem formalization, \$274k in prizes |
| **[EleutherAI](https://www.eleuther.ai/projects/elk)** | Open research | Building on CCS work |
| **Academic groups** | Research | Stanford, Berkeley, various |
| **AI Safety Camp** | Training | ELK research projects |

### Further Reading

| Resource | Description |
|----------|-------------|
| **[ARC ELK Report](https://www.alignment.org/blog/arcs-first-technical-report-eliciting-latent-knowledge/)** | Full problem statement and builder-breaker methodology |
| **[LessWrong ELK Discussion](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge)** | Community discussion and reactions |
| **[ELK Distillation](https://www.alignmentforum.org/posts/rxoBY9CMkqDsHt25t/eliciting-latent-knowledge-elk-distillation-summary)** | Accessible summary of core concepts |
| **[EleutherAI ELK Project](https://www.eleuther.ai/projects/elk)** | Ongoing empirical research |

---

## AI Transition Model Context

ELK research affects the <EntityLink id="ai-transition-model" /> through oversight capabilities:

| Factor | Parameter | Impact |
|--------|-----------|--------|
| <EntityLink id="human-oversight-quality" /> | Oversight effectiveness | Enables verification of AI beliefs and intentions |
| <EntityLink id="alignment-robustness" /> | Deception detection | Would make deceptive alignment impossible |
| <EntityLink id="misalignment-potential" /> | Alignment verification | Could verify AI systems are aligned |

ELK is a foundational problem for AI safety. If solved, it would provide the basis for genuine oversight of advanced AI systems. If unsolvable, it implies we must rely on control-based approaches rather than trust-based oversight. The problem's difficulty and importance make it a critical research priority despite uncertain tractability.
