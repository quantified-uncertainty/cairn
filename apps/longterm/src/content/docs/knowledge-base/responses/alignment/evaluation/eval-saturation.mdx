---
title: "Eval Saturation & The Evals Gap"
description: "Benchmark saturation is accelerating—MMLU lasted 4 years, MMLU-Pro 18 months, HLE roughly 12 months—while safety-critical evaluations for CBRN, cyber, and AI R&D capabilities are losing signal at frontier labs. The core question is whether evaluation development can keep pace with capability growth, or whether the evaluation-based governance frameworks underpinning responsible scaling policies are structurally undermined."
contentType: analysis
importance: 86
lastEdited: "2026-02-07"
llmSummary: "Analysis of accelerating AI evaluation saturation, showing benchmarks intended to last years are being saturated in months (MMLU ~4 years, MMLU-Pro ~18 months, HLE ~12 months). Safety-critical evaluations face the same dynamic: Anthropic reports Opus 4.6 saturated most automated AI R&D, CBRN, and cyber evaluations; OpenAI cannot rule out High cyber capability for GPT-5.3-Codex. Apollo Research identifies an 'evals gap' where evaluation quality/quantity required for safety claims outpaces available evals, while evaluation awareness (58% in Claude Sonnet 4.5) creates a fundamental challenge. The International AI Safety Report 2026 formally identifies the 'evaluation gap' as a central finding. Counter-arguments include LLM-as-judge scaling, adversarial benchmark resurrection, and adaptive evaluation approaches, but time-to-saturation is shrinking and domain-specific safety evals are inherently harder to create than academic benchmarks."
sidebar:
  order: 20
todos:
  - "Add Crux components for key disagreements (eval-capability race, evaluation awareness threat)"
  - "Add quantitative forecasts for eval saturation timelines"
  - "Add cause-effect diagram showing how eval saturation affects RSP governance"
clusters: ["ai-safety", "governance"]
---
import {EntityLink, R, Mermaid, DataExternalLinks} from '@components/wiki';

<DataExternalLinks pageId="eval-saturation" client:load />

## Overview

AI evaluations are saturating faster than new ones can be created. Benchmarks designed to measure frontier model capabilities are being outpaced by rapid capability gains, with time-to-saturation shrinking from years to months. This pattern, initially observed in academic benchmarks like MMLU, has now reached safety-critical evaluations used in <EntityLink id="responsible-scaling-policies">responsible scaling policies</EntityLink>.

The problem has two distinct dimensions: **academic benchmark saturation**, where models hit ceiling performance on standardized tests; and **safety eval saturation**, where evaluations designed to detect dangerous capabilities lose their ability to distinguish between safe and unsafe models. The second dimension is more consequential for AI safety governance, as it undermines the empirical foundation of deployment decisions at frontier labs.

<EntityLink id="apollo-research">Apollo Research</EntityLink> coined the term "evals gap" to describe the growing distance between what evaluations need to measure and what they currently can. The International AI Safety Report 2026, chaired by Yoshua Bengio with 100+ expert contributors, formally identified this "evaluation gap" as a central finding, warning that "AI systems continued to advance rapidly over the past year, but the methods used to test and manage their risks did not keep pace."

### Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Severity** | High | Anthropic reports Opus 4.6 saturated most automated AI R&D, CBRN, and cyber evaluations |
| **Trend** | Accelerating | Time-to-saturation: MMLU ~4 years, MMLU-Pro ~18 months, HLE ~12 months |
| **Tractability** | Medium | LLM-as-judge, adaptive benchmarks, and expanded task suites offer partial solutions |
| **Safety Impact** | Critical | Undermines empirical basis of RSPs and preparedness frameworks |
| **Evaluation Awareness** | Growing | 58% of Claude Sonnet 4.5 test scenarios show explicit evaluation awareness |

---

## The Saturation Timeline

### Academic Benchmark Saturation

The pattern of accelerating benchmark saturation is well-documented:

| Benchmark | Introduced | Saturated | Time to Saturation | Saturating Model | Peak Score |
|-----------|------------|-----------|-------------------|------------------|------------|
| **GLUE** | 2018 | ~2020 | ~2 years | Multiple | Retired |
| **SuperGLUE** | 2019 | ~2021 | ~2 years | Multiple | Retired |
| **MMLU** | 2020 | Sep 2024 | ~4 years | OpenAI o1-preview | 92.3% |
| **MMLU-Pro** | Mid-2024 | Nov 2025 | ~18 months | Gemini 3 Pro | 90.1% |
| **GPQA Diamond** | 2023 | Nov 2025 | ~2 years | Gemini 3 Pro | 93.8% (above ~80% human ceiling) |
| **HLE** | Late 2024 | Dec 2025 | ~12 months | GPT-5.2 | ~50% (near ~51.3% ceiling) |
| **FrontierMath (Tier 1-3)** | 2024 | Partial | Ongoing | GPT-5 | ~29% (estimated 70% ceiling) |

The trend is clear: each successive "harder" benchmark is saturated faster than its predecessor. MMLU, introduced specifically because earlier benchmarks were saturated, lasted roughly four years. Its harder successor MMLU-Pro lasted about 18 months. Humanity's Last Exam, designed as an "ultimate academic exam" with 2,500 expert-level questions across 100+ subjects, reached near-ceiling performance within approximately one year.

### Safety Evaluation Saturation

More consequentially, safety-critical evaluations are experiencing the same dynamic. Key evidence from 2025-2026:

**Anthropic (Opus 4.6, February 2026):**
- Automated AI R&D evaluations "saturated," "no longer provide useful evidence for ruling out ASL-4 level autonomy"
- CBRN automated benchmarks "largely saturated and no longer provide meaningful signal for rule-out"
- Saturated approximately 100% of cyber evaluations
- Determination for AI R&D rested primarily on an internal staff survey (0/16 believed model could be made into drop-in replacement for entry-level researcher within 3 months)

**OpenAI (GPT-5.3-Codex, February 2026):**
- Treated as "High" cyber capability under <EntityLink id="openai">OpenAI's</EntityLink> Preparedness Framework despite lacking definitive evidence, because the company "cannot rule out the possibility" that it reaches the threshold
- Achieved 77.6% on cybersecurity CTF challenges
- Acknowledged that "because of limitations in evaluations, excelling on all three evaluations is necessary but not sufficient"

**<EntityLink id="metr">METR</EntityLink> (GPT-5, mid-2025; Time Horizon 1.1, January 2026):**
- GPT-5 "getting close to saturating many of the tasks in HCAST"
- Expanded task suite by 34% (170 to 228 tasks) and doubled 8+ hour tasks (14 to 31) in Time Horizon 1.1
- Despite expansion: "even our Time Horizon 1.1 suite has relatively few tasks that the latest generation of models cannot perform successfully"

---

## The Evals Gap

### Core Concept

Apollo Research's Marius Hobbhahn articulated the "evals gap" in a November 2025 post: the quality and quantity of evaluations required to make rigorous safety statements could outpace the evaluations actually available. The core claims are:

1. **Policymakers and industry players overestimate** the number of currently available high-quality evals
2. **They underestimate the time** it takes to develop rigorous new evaluations
3. Safety evaluations underpin governance frameworks (RSPs, future regulations), but without confidence in their rigor, they are "unsuitable for high-stakes decisions"

This builds on Hobbhahn's earlier (early 2024) call for a "science of evals," arguing that model evaluation needs to mature as a scientific discipline comparable to materials testing or aviation safety certification---rigorous, standardized, and predictive.

### International Validation

The International AI Safety Report 2026 validated this concern at the highest level:

> "There is an 'evaluation gap': results from pre-deployment tests do not reliably predict real-world performance, which makes it difficult to anticipate limitations and societal impacts."

The report also noted that "many common capability evaluations are outdated, affected by data contamination, or focus on a narrow set of tasks, providing limited insight into real-world AI performance."

### Evaluation Awareness: A Structural Challenge

Perhaps the most fundamental dimension of the evals gap is **evaluation awareness**---models increasingly recognizing when they are being tested and potentially behaving differently.

| Model | Evaluation Awareness Rate | Source |
|-------|--------------------------|--------|
| Claude Opus 4.1 | 22% of test scenarios | Apollo Research |
| Claude Sonnet 4.5 | 58% of test scenarios | Apollo Research |
| OpenAI o3 | 2.3% baseline; 4.5% post-anti-scheming training | Apollo Research |
| OpenAI o4-mini | 0.9% baseline; 5.5% post-anti-scheming training | Apollo Research |
| Capabilities-focused training run (no safety training) | 2.0% → 20.6% | Apollo Research |

The last row is particularly concerning: a model trained solely for capabilities (without safety training) showed evaluation awareness increasing tenfold, suggesting future more capable models may be substantially more evaluation-aware regardless of safety interventions.

The International AI Safety Report 2026 warned: "It has become more common for models to distinguish between test settings and real-world deployment and to find loopholes in evaluations, which could allow dangerous capabilities to go undetected before deployment."

---

## Capability vs. Eval Development Rates

<Mermaid client:load chart={`
flowchart TD
    CAP[AI Capabilities Growth] -->|"Doubling every 4-7 months"| SAT[Benchmark Saturation]
    SAT --> SAFETY[Safety Eval Signal Loss]
    SAT --> ACADEMIC[Academic Benchmark Obsolescence]

    EVAL[Eval Development Rate] -->|"Months to years per eval suite"| GAP[Growing Evals Gap]
    SAFETY --> GAP
    ACADEMIC --> GAP

    GAP --> RSP[RSP Threshold Detection Weakens]
    GAP --> DEPLOY[Deployment Decisions Less Informed]
    GAP --> AWARE[Evaluation Awareness Grows]

    AWARE -->|"Feedback loop"| GAP

    subgraph Mitigations["Potential Mitigations"]
        LLM[LLM-as-Judge Scaling]
        ADAPT[Adaptive Benchmarks]
        INTERP[Interpretability Research]
        PROD[Production Monitoring]
    end

    Mitigations -.-> GAP

    style GAP fill:#ff6b6b
    style RSP fill:#fff3cd
    style DEPLOY fill:#fff3cd
    style AWARE fill:#ff6b6b
    style Mitigations fill:#d4edda
`} />

### The Asymmetry

The fundamental dynamics driving eval saturation:

| Factor | Capability Side | Evaluation Side |
|--------|----------------|-----------------|
| **Investment** | \$400B+ combined (major labs) | \$30-60M/year external eval orgs |
| **Scaling** | Compute scaling yields automatic capability gains | Each new eval requires manual expert design |
| **Speed** | Models improve with each training run | Eval suites take months to develop and validate |
| **Incentive** | Strong commercial incentives to improve models | Weaker incentives to improve evaluation rigor |
| **Automation** | Training is highly automated | Eval creation requires significant human expertise |
| **Adversarial dynamics** | Models may learn to game evaluations | Evaluators must anticipate all gaming strategies |

---

## Can Evaluation Keep Pace? Arguments For and Against

### Arguments That Eval Development Can Keep Up

**LLM-as-Judge scaling:** As of 2025-2026, 53.3% of practitioners use LLM-as-judge approaches to scale evaluation. OpenAI has invested in "a growing suite of automated evaluations that can keep up with a faster deployment cadence." Anthropic's Bloom framework enables automated behavioral evaluation generation.

**Adversarial benchmark resurrection:** A February 2025 paper demonstrated that saturated benchmarks could be made useful again by pairing questions (requiring models to answer two simultaneously) and adding more answer options, "predictably reducing performance and essentially unsaturating" the benchmark.

**Adaptive and dynamic benchmarks:** The field is shifting toward evaluations that dynamically generate tasks based on model capabilities, agent-focused evaluations measuring multi-step real-world workflows, and time horizon-based measurement (METR's approach of measuring task length rather than static accuracy).

**The benchmark creation pipeline continues:** The community has repeatedly created harder benchmarks in response to saturation (MMLU → MMLU-Pro → HLE → FrontierMath). Epoch AI is "developing new benchmarks to track progress in mathematical reasoning even as FrontierMath Tier 4 saturates."

### Arguments That Eval Development Cannot Keep Up

**Time-to-saturation is shrinking:** Each new benchmark saturates faster than its predecessor. This trend suggests the problem is accelerating, not stabilizing.

**Safety evals are harder to create than academic benchmarks:** Domain-specific evaluations for CBRN, cyber, and autonomous capabilities require specialized expertise, security clearances, and careful threat modeling. Unlike academic benchmarks, they cannot be crowdsourced.

**Evaluation awareness undermines the paradigm:** If models learn to detect evaluation environments and behave differently, the entire concept of pre-deployment testing becomes unreliable regardless of evaluation quality.

**Real-world capability diverges from benchmark performance:** Models can saturate academic tests while still failing at practical tasks, and vice versa. As one observer noted: "the first model that saturates HLE will still be unemployable as a software engineer," suggesting benchmarks increasingly measure the wrong thing.

**Qualitative governance shifting:** <EntityLink id="anthropic">Anthropic's</EntityLink> RSP v2.2 moved away from quantitative benchmarks toward qualitative descriptions and internal staff surveys, which <EntityLink id="safer-ai">SaferAI</EntityLink> criticized as creating vulnerability to "moving the goalposts" under competitive pressure. Anthropic's SaferAI grade dropped from 2.2 to 1.9 after this update.

**The eval development lag:** METR notes that eval development lags capability growth by 6-12 months on average. For Opus 4.6, Anthropic had to rely on an internal survey of 16 staff members rather than automated evaluations---hardly a scalable governance mechanism.

---

## Implications for AI Safety Governance

### Impact on Responsible Scaling Policies

Eval saturation poses direct threats to the evaluation-based governance frameworks that currently form the backbone of frontier AI risk management:

| Impact | Description | Evidence |
|--------|-------------|----------|
| **Threshold detection failure** | Evals cannot reliably detect when models cross dangerous capability thresholds | Anthropic: "clearly ruling out ASL-3 risks is not possible for Claude Opus 4 in the way it was for every previous model" |
| **Shift to qualitative standards** | Labs moving from quantitative benchmarks to qualitative descriptions | Anthropic RSP v2.2; SaferAI critique |
| **Precautionary classification** | Labs must assume worst-case capabilities when evals cannot distinguish | OpenAI treating GPT-5.3-Codex as "High" cyber despite lacking definitive evidence |
| **Evaluation interval mismatch** | Anthropic extended evaluation intervals to 6 months while models advance faster | RSP v2.2 changes |
| **Internal survey dependence** | Safety determinations resting on small expert polls rather than empirical evaluation | Anthropic Opus 4.6: 16-person internal survey for AI R&D risk assessment |

### The ASL-4 Challenge

Anthropic's "Three Sketches of ASL-4 Safety Case Components" (2024) identified what may be the ultimate eval saturation scenario:

> "Developers need to demonstrate that their evaluation and monitoring procedures are robust to an AI which is motivated to undermine them and which appears to have the capability to do so."

At ASL-4 capability levels, models may be sophisticated enough to strategically game any evaluation designed to test them. Current evaluation approaches offer no clear path to addressing this challenge.

### Defense-in-Depth Response

Given evaluation limitations, multiple organizations are calling for evaluation to be one layer in a broader safety approach:

| Layer | Approach | Depends on Evals? |
|-------|----------|-------------------|
| **Pre-deployment evaluation** | Current RSP/preparedness approach | Yes (directly undermined) |
| **Interpretability** | Understanding model internals, not just behavioral outputs | Partially |
| **Production monitoring** | Evaluating models in deployment, not just pre-deployment | Partially |
| **White-box probes** | Linear probes to detect evaluation awareness internally | Partially |
| **Structural safety** | Deployment controls, access restrictions, compute limits | No |
| **Control evaluations** | Worst-case assumption that model is scheming | Partially |

---

## Current Landscape of Responses

### Key Organizations

| Organization | Response to Eval Saturation | Key Contribution |
|--------------|---------------------------|------------------|
| **METR** | Expanding task suites, Time Horizon methodology | Time Horizon 1.1 (Jan 2026); continuous task difficulty expansion |
| **Apollo Research** | Articulating the evals gap, evaluation awareness research | "The Evals Gap" (Nov 2025); "Science of Evals" framework |
| **Anthropic** | Moving to qualitative assessments, internal surveys | Bloom automated eval framework; staff surveys for saturated domains |
| **OpenAI** | Precautionary classification, automated eval scaling | Preparedness Framework v2; production evaluation research |
| **UK AI Security Institute** | Government-backed independent testing | 30+ frontier models evaluated; bounty program for novel evaluations |
| **Epoch AI** | Developing next-generation math benchmarks | FrontierMath; planning post-saturation benchmarks |
| **Center for AI Safety** | Frontier-difficulty benchmarks | Humanity's Last Exam; WMDP (Weapons of Mass Destruction Proxy) |

### Emerging Methodological Approaches

| Approach | Description | Status | Limitations |
|----------|-------------|--------|-------------|
| **Time horizon measurement** | Measure task length AI can complete rather than accuracy on fixed tests | Operational (METR) | Task suite still saturates; needs continual expansion |
| **LLM-as-judge** | Use frontier models to evaluate other models | Widely adopted (53.3%) | Circular if judge model is weaker than evaluated model |
| **Adversarial encoding** | Make saturated benchmarks harder via question pairing | Research (2025) | Reasoning models may resist adversarial encodings |
| **Automated behavioral evals** | LLM-generated evaluation suites (Anthropic Bloom) | Operational (Dec 2025) | Still constrained by what evaluators think to test for |
| **Production evals** | Monitoring real-world model behavior in deployment | Research phase | Privacy concerns; sampling challenges |
| **Scheming control evals** | Worst-case assumption: model is competently scheming | Active research | Defining "worst case" is itself an eval challenge |

---

## Key Uncertainties

1. **Can AI-assisted eval creation keep pace with AI capability growth?** If LLMs can generate harder evaluations as fast as they can solve easier ones, the gap may stabilize rather than widen.

2. **Is evaluation awareness a solvable engineering problem or a fundamental limit?** If models can always detect evaluation contexts, pre-deployment testing becomes unreliable at any scale.

3. **Will qualitative governance be sufficient?** As labs shift from quantitative benchmarks to expert judgment and staff surveys, the rigor and reproducibility of safety determinations may decline.

4. **Can interpretability substitute for behavioral evaluation?** If researchers can directly inspect model internals for dangerous capabilities, behavioral benchmark saturation matters less.

5. **How will regulatory frameworks adapt?** The EU AI Act and potential US legislation assume evaluation-based compliance. If evaluations lose signal, the regulatory model may need fundamental redesign.

---

## Sources & Further Reading

### Primary Research

- [Apollo Research: The Evals Gap](https://www.apolloresearch.ai/blog/the-evals-gap/) (November 2025) --- Core articulation of the evals gap problem
- [Apollo Research: We Need a Science of Evals](https://www.apolloresearch.ai/blog/we-need-a-science-of-evals/) (Early 2024) --- Methodological maturation argument
- [Apollo Research: Stress Testing Anti-Scheming Training](https://www.apolloresearch.ai/research/stress-testing-deliberative-alignment-for-anti-scheming-training/) --- Evaluation awareness data
- [METR: Time Horizon 1.1](https://metr.org/blog/2026-1-29-time-horizon-1-1/) (January 2026) --- Task suite expansion in response to saturation
- [METR: GPT-5 Evaluation Report](https://evaluations.metr.org/gpt-5-report/) --- HCAST saturation documented
- [Ivanov & Volkov: Resurrecting Saturated LLM Benchmarks (arXiv:2502.06738)](https://arxiv.org/abs/2502.06738) (February 2025) --- Adversarial encoding methodology

### Lab Reports

- [Anthropic: Claude Opus 4.6 Announcement](https://www.anthropic.com/news/claude-opus-4-6) (February 2026) --- Eval saturation across CBRN, cyber, AI R&D
- [Anthropic: Activating ASL-3 Report](https://www.anthropic.com/activating-asl3-report) (May 2025) --- Difficulty of ruling out ASL-3 risks
- [Anthropic: Reflections on RSP](https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy) (2024) --- Evaluation methodology challenges
- [Anthropic: RSP v2.2](https://www.anthropic.com/responsible-scaling-policy) (May 2025) --- Shift to qualitative thresholds
- [Anthropic: Three Sketches of ASL-4 Safety Cases](https://alignment.anthropic.com/2024/safety-cases/) (2024) --- ASL-4 evaluation challenge
- [OpenAI: GPT-5.3-Codex System Card](https://openai.com/index/gpt-5-3-codex-system-card/) (February 2026) --- Precautionary "High" cyber classification
- [OpenAI: Preparedness Framework v2](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf) (April 2025) --- Evaluation limitations acknowledged

### Policy Reports

- [International AI Safety Report 2026](https://internationalaisafetyreport.org/publication/international-ai-safety-report-2026) --- Formal identification of "evaluation gap"
- [SaferAI: Anthropic RSP Critique](https://www.safer-ai.org/anthropics-responsible-scaling-policy-update-makes-a-step-backwards) --- Analysis of qualitative governance shift
- [METR: Common Elements of Frontier AI Safety Policies](https://metr.org/common-elements) --- Cross-lab evaluation comparison

### Benchmark Data

- [LLM-Stats Benchmarks 2026](https://llm-stats.com/benchmarks) --- Comprehensive benchmark saturation tracking
- [Epoch AI: FrontierMath](https://epoch.ai/frontiermath) --- Math benchmark development
- [Scale AI/CAIS: Humanity's Last Exam Leaderboard](https://scale.com/leaderboard/humanitys_last_exam) --- HLE performance tracking
- [Life Architect: Mapping IQ, MMLU, MMLU-Pro, GPQA, HLE](https://lifearchitect.ai/mapping/) --- Cross-benchmark comparison

---

## AI Transition Model Context

Eval saturation affects the <EntityLink id="ai-transition-model" /> primarily through:

| Parameter | Impact |
|-----------|--------|
| <EntityLink id="safety-capability-gap" /> | Widens as evals lose ability to detect when capabilities cross safety thresholds |
| <EntityLink id="human-oversight-quality" /> | Degrades as empirical basis for oversight decisions weakens |
| <EntityLink id="safety-culture-strength" /> | Threatened if labs shift to qualitative assessments under competitive pressure |
