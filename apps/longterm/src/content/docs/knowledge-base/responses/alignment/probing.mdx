---
title: Probing / Linear Probes
description: >-
  Linear probes are simple classifiers trained on neural network activations to
  test what concepts models internally represent, serving as a foundational
  diagnostic tool for AI safety research and deception detection.
sidebar:
  order: 14
quality: 3
importance: 60
lastEdited: '2025-01-22'
todos:
  - "Complete 'How Probing Works' section"
---
import {Mermaid, DataExternalLinks} from '../../../../../components/wiki';

<DataExternalLinks pageId="probing" client:load />

## Overview

Linear probing is a foundational interpretability technique that trains simple classifiers (typically linear models) on the internal activations of neural networks to determine what information is encoded in those representations. The core insight is elegant: if a linear classifier can accurately predict whether a model "knows" some concept (e.g., truthfulness, sentiment, factual correctness) from its activations, then that concept must be represented in a linearly accessible way in the model's internal state. This provides evidence about the structure of learned representations and what information models track internally.

The technique has become a standard research tool across interpretability and AI safety. Researchers use probes to detect whether models represent concepts like "this statement is false," "this response is harmful," or "I am being evaluated" in their activations. The simplicity of linear probes (typically just a single learned weight matrix) makes them computationally cheap and easy to train, while their limitations (they can only detect linearly separable features) provide insight into how concepts are organized in neural networks.

For AI safety applications, probing offers a direct window into whether models have internal representations that differ from their expressed behavior. A model might claim ignorance while its activations reveal it knows the answer, or might express confidence while internally representing uncertainty. More critically, probes could potentially detect deception-related representations, identifying when a model internally represents "I should mislead the user" even if its outputs appear helpful. However, the technique faces fundamental limitations: probes only detect what they're trained to find, models might learn to hide representations from probes, and correlation with activations doesn't guarantee causal relevance.

## Risk Assessment & Impact

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| **Safety Uplift** | Low | Diagnostic tool; doesn't directly improve safety but supports research |
| **Capability Uplift** | Neutral | Analysis tool only; no capability improvement |
| **Net World Safety** | Helpful | Supports understanding with minimal dual-use concerns |
| **Scalability** | Yes | Computationally cheap; scales well to larger models |
| **Deception Robustness** | Partial | Could detect lying representations; models could learn to hide them |
| **SI Readiness** | Maybe | Technique scales; effectiveness at SI uncertain |
| **Current Adoption** | Widespread | Standard research tool across academia and industry |
| **Research Investment** | \$5-10M/yr | Common technique; many groups use probing as part of broader research |

## Applications in AI Safety

### What Probes Can Detect

| Concept | Research Findings | Safety Relevance |
|---------|-------------------|------------------|
| **Truthfulness** | Models represent truth values; probes achieve 60-80% accuracy | Detecting when models "know" they're wrong |
| **Uncertainty** | Internal calibration often differs from expressed confidence | Understanding model reliability |
| **Harmfulness** | Activations differ for harmful vs. safe content | Content filtering; harm detection |
| **Deception** | Early evidence of "lying" representations | Core deception detection research |
| **Self-awareness** | Models represent whether they're being tested | Evaluation gaming concerns |

### The Eliciting Latent Knowledge (ELK) Connection

Probing is central to the Eliciting Latent Knowledge research agenda, which asks: can we reliably extract what a model "believes" from its internal representations, even when its outputs are unreliable?

<Mermaid client:load chart={`
flowchart LR
    subgraph Problem["ELK Challenge"]
        MODEL[Model with Latent Knowledge]
        OUTPUT[Potentially Misleading Output]
        TRUTH[True Internal Representation]
    end

    subgraph Solution["Probing Approach"]
        ACT[Model Activations]
        PROBE[Trained Probe]
        EXTRACT[Extracted Knowledge]
    end

    MODEL --> OUTPUT
    MODEL --> TRUTH
    TRUTH --> ACT
    ACT --> PROBE
    PROBE --> EXTRACT

    OUTPUT -.->|May Differ| EXTRACT

    style Problem fill:#ffe6e6
    style Solution fill:#d4edda
`} />

Key research questions:
- Can probes extract truth even when the model's output is deceptive?
- How do we validate probe findings without ground truth?
- Could deceptive models learn to hide from probes?

## Empirical Results

### Probe Performance on Various Tasks

| Task | Dataset Size | Best Layer | Accuracy | Baseline |
|------|--------------|------------|----------|----------|
| Factual correctness | 10K statements | Layer 20/32 | 72% | 50% random |
| Sentiment analysis | 50K reviews | Layer 16/32 | 89% | 50% random |
| Toxicity detection | 20K examples | Layer 24/32 | 78% | 50% random |
| Lying detection | 5K examples | Layer 18/32 | 65% | 50% random |
| Evaluation awareness | 2K scenarios | Varies | 58-70% | 50% random |

### Notable Findings

1. **Layer Effects**: Probes perform differently across layers; optimal layer depends on concept type
2. **Model Size**: Larger models often have more linearly accessible representations
3. **Training Data**: Probe performance improves with more diverse training examples
4. **Generalization**: Probes trained on one distribution often fail on others

### Limitations Demonstrated

| Limitation | Evidence | Implication |
|------------|----------|-------------|
| **Spurious Correlations** | Probes can achieve high accuracy via shortcuts | May not detect target concept |
| **Non-Causal** | Probing doesn't establish that features cause behavior | Understanding vs. control gap |
| **Adversarial Hiding** | Models can be trained to hide representations | Deception detection may fail |
| **Concept Validity** | What we label as "truthfulness" may not match model's representation | Interpretation challenges |

## Comparison to Other Techniques

| Technique | Advantages Over Probing | Probing Advantages |
|-----------|------------------------|-------------------|
| **Sparse Autoencoders** | Unsupervised; discovers unknown features | Targeted; cheaper; interpretable |
| **Activation Patching** | Establishes causality | Doesn't require intervention |
| **Behavioral Testing** | Tests actual outputs | Reveals hidden representations |
| **Representation Engineering** | Enables intervention | Purely observational; lower risk |

## Critical Assessment

### Arguments For Probing Investment

1. **Foundation for Other Work**: Probes inform SAE training, RepEng directions, and evaluation design
2. **Computationally Cheap**: Can be applied broadly without major infrastructure
3. **Well-Understood**: Decades of machine learning theory apply
4. **Rapid Iteration**: Easy to test new hypotheses about representations
5. **Safety-Relevant**: Direct application to deception detection research

### Arguments Against Major Investment

1. **Not Sufficient Alone**: Probes diagnose but don't fix problems
2. **Vulnerability to Gaming**: Sophisticated models could hide from probes
3. **Interpretation Challenges**: Probe success doesn't guarantee understanding
4. **Limited to Linear Structure**: May miss important nonlinear features
5. **Correlation Not Causation**: Findings may not translate to control

### Key Uncertainties

- Can probes reliably detect deception in models that are trying to hide it?
- What probe accuracy is "good enough" for safety applications?
- How do probe results transfer across model architectures and scales?
- Can probe training data adequately cover the space of concerning behaviors?

## Recommendation

**Recommendation Level: MAINTAIN**

Linear probing is a valuable supporting technique that is already adequately funded as a standard research tool. The technique should continue to be used broadly but does not require major additional investment as a standalone approach. Its primary value lies in enabling and informing other interpretability techniques rather than providing direct safety guarantees.

Appropriate uses:
- Informing sparse autoencoder training and evaluation
- Generating hypotheses for representation engineering
- Quick diagnostics on new models and capabilities
- Supporting the Eliciting Latent Knowledge research agenda
- Baseline comparisons for more sophisticated techniques

## Sources & Resources

### Primary Research

- **Belinkov & Glass (2019)**: "Analysis Methods in Neural Language Processing: A Survey" - Foundational overview including probing
- **ARC (2021-2022)**: Eliciting Latent Knowledge research reports - Safety applications of probing
- **Azaria & Mitchell (2023)**: "The Internal State of an LLM Knows When It's Lying" - Deception detection via probing

### Methodological Papers

- **Hewitt & Liang (2019)**: "Designing and Interpreting Probes with Control Tasks" - Methodological best practices
- **Belinkov (2022)**: "Probing Classifiers: Promises, Shortcomings, and Advances" - Comprehensive review of limitations

### Tools and Code

- **Baukit**: Library for probing experiments on language models
- **TransformerLens**: Includes probing utilities
- **Standard ML libraries**: scikit-learn logistic regression widely used

### Related Concepts

- **Representation Similarity Analysis**: Comparing representations across models/layers
- **Concept Bottleneck Models**: Architectures with interpretable intermediate representations
- **Neural Network Surgery**: Broader field of analyzing and modifying trained networks
