---
title: Sparse Autoencoders (SAEs)
description: Sparse autoencoders extract interpretable features from neural network activations using sparsity constraints. Anthropic's 2024 research extracted 34 million features from Claude 3 Sonnet with 90% interpretability scores, while OpenAI trained 16 million latent SAEs on GPT-4. Safety applications include detecting deception and manipulation features, though DeepMind's 2025 research found SAEs underperform simple probes on some downstream tasks.
sidebar:
  order: 12
quality: 60
importance: 72.5
lastEdited: 2025-01-28
llmSummary: Comprehensive review of sparse autoencoders (SAEs) for mechanistic interpretability, covering Anthropic's 34M features from Claude 3 Sonnet (90% interpretability), OpenAI's 16M latent GPT-4 SAEs, and DeepMind's 1T+ parameter Gemma Scope releases. Despite promising safety applications like deception detection, DeepMind's 2025 negative results showed SAEs underperforming simple probes on downstream tasks, leading to research deprioritization.
ratings:
  novelty: 4.2
  rigor: 6.8
  actionability: 5.5
  completeness: 7.5
metrics:
  wordCount: 1192
  citations: 45
  tables: 31
  diagrams: 2
---
import {Mermaid, DataExternalLinks, R} from '../../../../../components/wiki';

<DataExternalLinks pageId="sparse-autoencoders" client:load />

## Overview

Sparse Autoencoders (SAEs) represent a breakthrough technique in mechanistic interpretability that addresses the fundamental challenge of neural network polysemanticity. In modern language models, individual neurons often respond to multiple unrelated concepts (e.g., a single neuron activating for both "the Golden Gate Bridge" and "requests for help"), making direct interpretation of neural activations extremely difficult. SAEs solve this by learning to decompose dense, polysemantic activations into sparse, monosemantic feature vectors where each dimension corresponds to a single interpretable concept.

The technique works by training an auxiliary neural network to reconstruct model activations through a bottleneck that encourages sparse representations. When trained on billions of activation samples, SAEs discover features that correspond to human-interpretable concepts ranging from concrete entities like "San Francisco" to abstract notions like "deception in political contexts." Anthropic's landmark 2024 work extracted over 34 million interpretable features from Claude 3 Sonnet, with automated evaluation finding that 90% of high-activating features have clear human-interpretable explanations.

For AI safety, SAEs offer a potentially transformative capability: the ability to directly detect safety-relevant cognition inside models. Researchers have identified features corresponding to lying, manipulation, security vulnerabilities, power-seeking behavior, and sycophancy. If SAE research scales successfully, it could provide the foundation for runtime monitoring systems that flag concerning internal states, deception detection during training, and verification that alignment techniques actually work at the mechanistic level.

## Risk Assessment & Impact

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| **Safety Uplift** | Low (now) | Promising tool but impact depends on broader mech interp field progress |
| **Capability Uplift** | Neutral | Analysis tool only; does not improve model capabilities |
| **Net World Safety** | Helpful | Supports interpretability research with minimal dual-use concerns |
| **Scalability** | Partial | Works on 100B+ parameter models; dictionary size grows with model scale |
| **Deception Robustness** | Partial | Could find deception-related features but not guaranteed |
| **SI Readiness** | Unknown | Depends on whether interpretability scales to superintelligent systems |
| **Current Adoption** | Experimental | Active research tool at Anthropic, Apollo, DeepMind; not deployed operationally |
| **Research Investment** | \$10-30M/yr | Subset of mech interp budget; Anthropic leads with Apollo, independents contributing |

## Research Comparison: Major SAE Studies

The following table compares key research efforts in sparse autoencoder development across major AI labs:

| Organization | Publication | Model Target | Features Extracted | Key Findings | Scale/Cost |
|--------------|-------------|--------------|-------------------|--------------|------------|
| [Anthropic](https://transformer-circuits.pub/2024/scaling-monosemanticity/) | May 2024 | Claude 3 Sonnet | 34 million | 90% of high-activating features interpretable; safety-relevant features for deception, sycophancy identified | 8B activations; 16x expansion |
| [OpenAI](https://openai.com/index/extracting-concepts-from-gpt-4/) | June 2024 | GPT-4 | 16 million | Smooth scaling laws; k-sparse architecture eliminates dead latents; 10% compute equivalence loss | 40B tokens trained |
| [DeepMind](https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/) | August 2024 | Gemma 2 (2B-27B) | 30+ million | JumpReLU architecture; open-source release for community research | 20 PiB activations; 15% of Gemma 2 9B training compute |
| [DeepMind](https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/) | December 2024 | Gemma 3 (270M-27B) | 1+ trillion params | Combines SAEs with transcoders; analyzes jailbreaks and chain-of-thought | 110 PB activation data |
| [EleutherAI](https://blog.eleuther.ai/autointerp/) | 2024 | GPT-2, open models | 1.5 million | Automated interpretation at \$1,300 (Llama 3.1) vs \$8,500 (Claude 3.5); open tools | 97% cost reduction vs prior methods |

### SAE Architecture Evolution

| Architecture | Year | Key Innovation | Trade-offs |
|--------------|------|----------------|------------|
| Vanilla ReLU + L1 | 2023 | Original formulation | Dead latents; requires penalty tuning |
| [Gated SAE](https://arxiv.org/abs/2309.08600) | 2024 | Separate magnitude/selection paths | Better reconstruction-sparsity frontier |
| JumpReLU | 2024 | Threshold activation function | State-of-the-art for Gemma Scope |
| [BatchTopK](https://arxiv.org/abs/2406.04093) | 2024 | Directly set sparsity without penalty | Few dead latents; training stability |
| [Transcoders](https://arxiv.org/abs/2501.18823) | 2025 | Predict next-layer activations | Better for analyzing computations vs representations |

## How SAEs Work

### Technical Architecture

SAEs are encoder-decoder neural networks trained to reconstruct activation vectors through a sparse intermediate representation:

<Mermaid client:load chart={`
flowchart TD
    ACT[Model Activations] --> ENC[Encoder]
    ENC --> SPARSE[Sparse Features]
    SPARSE --> DEC[Decoder]
    DEC --> RECON[Reconstructed Activations]

    SPARSE --> INTERP[Interpretable Concepts]
    INTERP --> SAFETY[Safety-Relevant Features]
    INTERP --> KNOWLEDGE[Factual Knowledge]
    INTERP --> REASONING[Reasoning Patterns]

    SAFETY --> DECEPTION[Deception Detection]
    SAFETY --> MANIPULATION[Manipulation]
    SAFETY --> HARMFUL[Harmful Intent]

    style ACT fill:#e1f5ff
    style SPARSE fill:#d4edda
    style SAFETY fill:#fff3cd
    style INTERP fill:#f0f0f0
`} />

The key innovation is the sparsity constraint: during training, the encoder is penalized for activating too many features simultaneously. This forces the network to find a small set of highly relevant features for any given input, naturally leading to monosemantic representations where each feature captures a distinct concept.

### SAE Research Pipeline

The following diagram illustrates the complete pipeline from training SAEs to safety applications:

<Mermaid client:load chart={`
flowchart TD
    subgraph Training["Training Phase"]
        LM[Language Model] --> ACT[Collect Activations]
        ACT --> STORE[Store 10-100+ PB Data]
        STORE --> TRAIN[Train SAE]
        TRAIN --> FEAT[Extract Features]
    end

    subgraph Analysis["Analysis Phase"]
        FEAT --> AUTO[Automated Interpretation]
        AUTO --> VALID[Human Validation]
        VALID --> CATALOG[Feature Catalog]
    end

    subgraph Applications["Safety Applications"]
        CATALOG --> MONITOR[Runtime Monitoring]
        CATALOG --> STEER[Activation Steering]
        CATALOG --> VERIFY[Alignment Verification]
        CATALOG --> CIRCUIT[Circuit Analysis]
    end

    subgraph Tools["Research Tools"]
        CATALOG --> NEURON[Neuronpedia]
        CATALOG --> LENS[SAELens/TransformerLens]
    end

    style Training fill:#e1f5ff
    style Analysis fill:#d4edda
    style Applications fill:#fff3cd
    style Tools fill:#f0f0f0
`} />

### Training Process

| Stage | Description | Computational Cost |
|-------|-------------|-------------------|
| **Activation Collection** | Record model activations on billions of tokens | Storage-intensive (110 PB for Gemma Scope) |
| **SAE Training** | Train encoder-decoder with L1 sparsity penalty | \$1-10M compute for frontier models |
| **Feature Analysis** | Automated labeling using interpretable AI | \$1,300-8,500 per 1.5M features |
| **Validation** | Verify features have causal effects via steering | Medium compute; manual effort |

### Key Technical Parameters

- **Expansion Factor**: Ratio of SAE dictionary size to original activation dimensions (typically 4-64x)
- **Sparsity Penalty (L1)**: Strength of penalty on feature activation; higher values yield sparser but potentially less accurate reconstructions
- **Reconstruction Loss**: How well SAE outputs match original activations; fundamental accuracy metric
- **Dead Features**: Features that never activate; indicator of training difficulties

## Major Research Milestones

### Anthropic's Scaling Monosemanticity (May 2024)

[The landmark result](https://transformer-circuits.pub/2024/scaling-monosemanticity/) that demonstrated SAEs work at frontier model scale. This represented a major scaling milestone—eight months prior, Anthropic had only demonstrated SAEs on a small one-layer transformer, and it was unclear whether the method would scale to production models.

| Metric | Result | Context |
|--------|--------|---------|
| Model | Claude 3 Sonnet | 3.0 version released March 2024 |
| Features Extracted | 1M, 4M, and 34M | Three SAE sizes tested |
| Automated Interpretability Score | 90% | High-activating features with clear explanations |
| Training Data | 8 billion residual-stream activations | From diverse text corpus |
| Expansion Factor | 83x to 2833x | Ratio of features to residual stream dimension |
| Average Features per Token | ~300 | Sparse from thousands of dense activations |

The resulting features exhibit remarkable abstraction: they are multilingual, multimodal, and generalize between concrete and abstract references. Critically, researchers found safety-relevant features including:

| Feature Category | Examples Found | Safety Relevance |
|-----------------|----------------|------------------|
| Deception | Lying, dishonesty patterns | Direct alignment concern |
| Security | Code backdoors, vulnerabilities | Dual-use risk |
| Manipulation | Persuasion, bias injection | Influence operations |
| Power-seeking | Goal-directed behavior patterns | Instrumental convergence |
| Sycophancy | Agreement regardless of truth | Reward hacking indicator |

### DeepMind's Gemma Scope (August 2024) and Gemma Scope 2 (December 2024)

[Gemma Scope](https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/) represented the first major open-source SAE release, followed by the substantially larger [Gemma Scope 2](https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/)—described as the largest open-source interpretability release by an AI lab to date.

| Metric | Gemma Scope (Aug 2024) | Gemma Scope 2 (Dec 2024) |
|--------|------------------------|--------------------------|
| Model Coverage | Gemma 2 (2B, 9B, 27B) | Gemma 3 (270M to 27B) |
| Total Features | 30+ million | Comparable scale |
| Training Compute | ~15% of Gemma 2 9B training | Not disclosed |
| Storage Requirements | 20 PiB activations | 110 PB activation data |
| Total Parameters | Hundreds of billions | 1+ trillion across all SAEs |
| Architecture | JumpReLU SAEs | SAEs + Transcoders |

Gemma Scope 2 introduces **transcoders** alongside SAEs—a key advancement that predicts next-layer activations rather than reconstructing current activations. This enables analysis of multi-step computations and behaviors like chain-of-thought faithfulness. The release specifically targets safety-relevant capabilities: analyzing jailbreak mechanisms, understanding refusal behaviors, and evaluating reasoning faithfulness.

### Negative Results and Limitations

[DeepMind's March 2025 announcement](https://deepmindsafetyresearch.medium.com/negative-results-for-sparse-autoencoders-on-downstream-tasks-and-deprioritising-sae-research-6cadcfc125b9) that they were deprioritizing SAE research highlights important limitations:
- SAEs underperformed simple linear probes for detecting harmful intent in user prompts
- Features may not be functionally important for the behaviors researchers care about
- Expensive to train at scale with diminishing returns on some safety applications

### Quantified Performance Metrics

| Metric | Value | Source | Notes |
|--------|-------|--------|-------|
| Feature interpretability rate | 90% | [Anthropic 2024](https://transformer-circuits.pub/2024/scaling-monosemanticity/) | High-activating features with clear explanations |
| Average features per token | ~300 | Anthropic 2024 | Sparse representation from thousands of dense activations |
| Reconstruction loss (GPT-4 SAE) | 10% compute equivalent | [OpenAI 2024](https://openai.com/index/extracting-concepts-from-gpt-4/) | Language modeling loss increase when SAE substituted |
| Automated interpretation cost | \$1,300-8,500 per 1.5M features | [EleutherAI 2024](https://blog.eleuther.ai/autointerp/) | Llama 3.1 vs Claude 3.5 Sonnet |
| Prior interpretation methods | ~\$200,000 per 1.5M features | EleutherAI 2024 | 97% cost reduction achieved |
| Storage requirements (Gemma Scope 2) | 110 PB | [DeepMind 2024](https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/) | Largest open-source interpretability release |
| Dead latent rate (TopK architecture) | Near zero | [OpenAI 2024](https://arxiv.org/abs/2406.04093) | vs significant dead features with ReLU+L1 |

## Activation Steering Applications

SAE features enable direct intervention in model behavior by clamping or modifying specific feature activations during inference. This "steering" capability represents a practical application beyond interpretability:

| Steering Application | Method | Effectiveness | Trade-offs |
|---------------------|--------|---------------|------------|
| [Refusal steering](https://arxiv.org/abs/2411.11296) | Amplify refusal-mediating features | Improves jailbreak resistance | Degrades general capabilities |
| [SAE-Targeted Steering](https://arxiv.org/abs/2411.02193) | Optimize steering vectors for specific effects | Outperforms baseline methods | Requires feature identification |
| Feature Guided Activation Additions | SAE-guided vector construction | Better coherence than CAA | More complex pipeline |
| [Graph-regularized SAEs (GSAE)](https://arxiv.org/abs/2512.06655) | Spectral vector bank with dual-gating | Selective, stable steering | Added architectural complexity |

Research has uncovered a **fundamental tension**: features mediating safety behaviors like refusal appear entangled with general capabilities. Steering for improved safety often degrades benchmark performance, suggesting safety-relevant features may not be cleanly separable from capability-relevant ones.

## Safety Applications

### Deception Detection

SAEs could enable direct detection of deceptive cognition by identifying when deception-related features activate:

| Application | Mechanism | Current Status |
|-------------|-----------|----------------|
| **Runtime Monitoring** | Flag when deception features activate during inference | Theoretical; not deployed |
| **Training-time Detection** | Identify deceptive patterns during fine-tuning | Experimental research |
| **Alignment Verification** | Confirm models have learned intended values | Early-stage research |
| **Red-teaming Augmentation** | Find adversarial prompts that activate concerning features | Growing usage |

### Limitations for Safety

Even with successful SAE development, fundamental challenges remain:
- **Coverage**: SAEs may not capture all safety-relevant features
- **Adversarial Robustness**: Sophisticated models might learn to hide concerning cognition
- **Interpretation Accuracy**: Human labels may not capture true feature meanings
- **Causal Relevance**: Features that activate for deception may not cause deceptive outputs

## Key Research Groups

| Organization | Focus | Key Contributions | Scale |
|--------------|-------|-------------------|-------|
| **Anthropic** | Leading SAE development and scaling | [Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/); Claude SAEs; original monosemanticity work | 34M features; frontier models |
| **OpenAI** | Scaling methodology | [GPT-4 SAEs](https://openai.com/index/extracting-concepts-from-gpt-4/); TopK architecture; scaling laws research | 16M latents; 40B token training |
| **DeepMind** | Open-source tools and benchmarking | [Gemma Scope](https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/) 1 & 2; JumpReLU; [negative results](https://deepmindsafetyresearch.medium.com/negative-results-for-sparse-autoencoders-on-downstream-tasks-and-deprioritising-sae-research-6cadcfc125b9) | 1T+ parameters; 110 PB data |
| **EleutherAI** | Open-source interpretability | [Automated interpretation](https://blog.eleuther.ai/autointerp/); Delphi; Sparsify; cost reduction | 97% cost reduction |
| **MATS Alumni** | Foundational research | [Original SAE paper](https://arxiv.org/abs/2309.08600); SAELens; community tools | Open-source ecosystem |
| **Neuronpedia** | Visualization and tooling | [Feature explorer](https://www.neuronpedia.org/); 50M+ searchable latents; API access | 4+ TB of data hosted |

### Arguments For Prioritization

1. **Unique Capability**: SAEs may be necessary for detecting sophisticated deception that behavioral evals cannot catch
2. **No Capability Uplift**: Pure safety research with minimal dual-use concerns
3. **Proven at Scale**: Works on 100B+ parameter models, suggesting path to frontier
4. **Foundation for Other Work**: Enables representation engineering, activation steering, and monitoring

### Arguments Against Prioritization

1. **May Not Scale**: Fundamental limits on interpretability possible
2. **Expensive**: Significant compute and researcher time required
3. **Limited Safety Impact So Far**: No operational safety applications despite years of research
4. **Alternative Approaches**: Linear probes and behavioral methods may be more cost-effective
5. **False Confidence Risk**: Partial interpretability might create false assurance

### Key Uncertainties

| Uncertainty | Current Evidence | Importance | Resolution Timeline |
|-------------|------------------|------------|---------------------|
| **Causal relevance of features** | Mixed; steering works but effects entangled | Critical for safety applications | 2025-2027 |
| **Adversarial robustness** | Untested; models could learn to evade feature detection | High for deployment | Unknown |
| **Coverage completeness** | Current SAEs capture subset of model behavior | Medium; partial coverage may suffice | 2025-2026 |
| **Scaling to superintelligent systems** | No evidence; extrapolation uncertain | Very high for long-term safety | Depends on AI timeline |
| **Transcoders vs SAEs** | [Early evidence favors transcoders](https://arxiv.org/abs/2501.18823) for some applications | Medium; may be complementary | 2025 |
| **Feature universality across models** | Similar features found across architectures | Medium for transfer learning | 2025-2026 |

## Recommendation

**Recommendation Level: INCREASE**

SAEs represent one of the most promising technical approaches to the fundamental problem of understanding AI cognition. While current safety applications remain limited, the potential for detecting sophisticated deception justifies increased investment. The technique has no meaningful capability uplift, making it a safe area for expanded research.

Priority areas for additional investment:
- Scaling to larger models and more comprehensive feature coverage
- Developing robust automated evaluation methods
- Building operational monitoring systems based on SAE features
- Investigating adversarial robustness of SAE-based detection

## Sources & Resources

### Primary Research

| Source | Organization | Date | Key Contribution |
|--------|--------------|------|------------------|
| [Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/) | Anthropic | May 2024 | 34M features from Claude 3 Sonnet; safety-relevant feature discovery |
| [Extracting Concepts from GPT-4](https://openai.com/index/extracting-concepts-from-gpt-4/) | OpenAI | June 2024 | 16M latent SAE; scaling laws; TopK architecture |
| [Scaling and Evaluating Sparse Autoencoders](https://arxiv.org/abs/2406.04093) | OpenAI | June 2024 | Technical methodology paper; k-sparse autoencoders |
| [Gemma Scope](https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/) | DeepMind | August 2024 | Open-source SAEs for Gemma 2; JumpReLU architecture |
| [Gemma Scope 2](https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/) | DeepMind | December 2024 | Largest open release; SAEs + transcoders for Gemma 3 |
| [Sparse Autoencoders Find Highly Interpretable Features](https://arxiv.org/abs/2309.08600) | Anthropic/MATS | September 2023 | Foundational SAE methodology paper |

### Critical Perspectives and Limitations

| Source | Organization | Date | Key Finding |
|--------|--------------|------|-------------|
| [Negative Results for SAEs on Downstream Tasks](https://deepmindsafetyresearch.medium.com/negative-results-for-sparse-autoencoders-on-downstream-tasks-and-deprioritising-sae-research-6cadcfc125b9) | DeepMind | March 2025 | SAEs underperform linear probes; led to SAE research deprioritization |
| [Open Source Automated Interpretability](https://blog.eleuther.ai/autointerp/) | EleutherAI | 2024 | 97% cost reduction for feature interpretation; open tools |
| [Transcoders Beat Sparse Autoencoders](https://arxiv.org/abs/2501.18823) | Various | January 2025 | Skip transcoders Pareto-dominate SAEs for interpretability |

### Tools and Platforms

| Tool | URL | Description |
|------|-----|-------------|
| Neuronpedia | [neuronpedia.org](https://www.neuronpedia.org/) | Interactive SAE feature explorer; 50M+ searchable latents; live inference testing |
| SAELens | [github.com/jbloomAus/SAELens](https://github.com/jbloomAus/SAELens) | SAE training library; supports multiple architectures |
| TransformerLens | [github.com/neelnanda-io/TransformerLens](https://github.com/neelnanda-io/TransformerLens) | Interpretability library with SAE integration |
| Delphi | [github.com/EleutherAI/delphi](https://github.com/EleutherAI/delphi) | Automated feature interpretation pipeline |
| EleutherAI Sparsify | [github.com/EleutherAI/sparsify](https://github.com/EleutherAI/sparsify) | On-the-fly activation training without caching |

### Foundational Reading

- [Towards Monosemanticity](https://transformer-circuits.pub/2023/monosemantic-features) (Anthropic, 2023) - Original demonstration of SAEs extracting interpretable features from a one-layer transformer
- [An Intuitive Explanation of Sparse Autoencoders](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) (Adam Karvonen, 2024) - Accessible introduction to SAE concepts
- [A Survey on Sparse Autoencoders](https://arxiv.org/abs/2503.05613) (2025) - Comprehensive review of SAE methods and applications
