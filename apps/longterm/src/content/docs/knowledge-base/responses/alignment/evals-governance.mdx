---
title: Evals-Based Deployment Gates
description: Evals-based deployment gates require AI models to pass safety evaluations before deployment or capability scaling. The EU AI Act mandates conformity assessments for high-risk systems with fines up to EUR 35M, while NIST's TEVV framework and UK AISI's Inspect tools provide evaluation infrastructure. Third-party evaluators like METR test autonomous capabilities, though only 3 of 7 major labs substantively test for dangerous capabilities according to the 2025 AI Safety Index.
importance: 82
quality: 65
lastEdited: "2026-01-28"
sidebar:
  order: 29
pageTemplate: knowledge-base-response
llmSummary: Comprehensive analysis of evaluation-based deployment gates as an AI governance mechanism, documenting that only 3 of 7 major labs substantively test for dangerous capabilities despite EU AI Act mandating evaluations with fines up to EUR 35M. Key findings include UK AISI achieving 50% success on apprentice-level cyber tasks (up from 10% in early 2024) and fundamental limitations including inability to test for unknown risks and vulnerability to gaming by sophisticated models.
ratings:
  novelty: 4.5
  rigor: 7
  actionability: 7.5
  completeness: 7.5
metrics:
  wordCount: 944
  citations: 39
  tables: 54
  diagrams: 3
---
import {Mermaid, R, EntityLink, DataExternalLinks, DataInfoBox} from '../../../../../components/wiki';

<DataExternalLinks pageId="evals-governance" client:load />

## Quick Assessment

| Dimension | Rating | Notes |
|-----------|--------|-------|
| **Tractability** | Medium-High | Evaluation infrastructure exists; standardization ongoing |
| **Scalability** | High | Applies across model sizes and deployment contexts |
| **Current Maturity** | Medium | EU AI Act in force; NIST framework developing; UK AISI operational |
| **Time Horizon** | 1-3 years | Major deadlines 2025-2027 |
| **Key Proponents** | EU AI Office, UK AISI, NIST, METR | Active evaluation programs |
| **Enforcement Gap** | High | Only 3 of 7 major labs substantively test for dangerous capabilities |

*Sources: [2025 AI Safety Index](https://futureoflife.org/ai-safety-index-summer-2025/), [EU AI Act](https://artificialintelligenceact.eu/)*

## Overview

Evals-based deployment gates are a governance mechanism that requires AI systems to pass specified safety evaluations before being deployed or scaled further. Rather than relying solely on lab judgment, this approach creates explicit checkpoints where models must demonstrate they meet safety criteria. The EU AI Act, US Executive Order 14110, and various voluntary lab commitments all incorporate elements of evaluation-gated deployment.

The core value proposition is straightforward: evaluation gates add friction to the deployment process that ensures at least some safety testing occurs. They create a paper trail of safety evidence, enable third-party verification, and provide a mechanism for regulators to enforce standards. Without such gates, the default is continuous deployment with safety as an afterthought.

However, evals-based gates face fundamental limitations. Evaluations can only test for risks we anticipate and can operationalize into tests. They may create a false sense of security if models can pass evaluations while still being dangerous. Deceptive models could specifically behave well during evaluations. And the pressure to deploy creates incentives to design evaluations that models can pass. Evals-based gates are valuable as one component of AI governance but should not be confused with comprehensive safety assurance.

## Evaluation Governance Frameworks Comparison

The landscape of AI evaluation governance is rapidly evolving, with different jurisdictions and organizations taking distinct approaches. The following table compares major frameworks:

| Framework | Jurisdiction | Scope | Legal Status | Enforcement | Key Requirements |
|-----------|--------------|-------|--------------|-------------|------------------|
| **[EU AI Act](https://artificialintelligenceact.eu/)** | European Union | High-risk AI, GPAI models | Binding regulation | Fines up to EUR 35M or 7% global turnover | Conformity assessment, risk management, technical documentation |
| **[US EO 14110](https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence)** | United States | Dual-use foundation models above 10^26 FLOP | Executive order (rescinded Jan 2025) | Reporting requirements | Safety testing, red-team results reporting |
| **[UK AISI](https://www.aisi.gov.uk/)** | United Kingdom | Frontier AI models | Voluntary (with partnerships) | Reputation, access agreements | Pre-deployment evaluation, adversarial testing |
| **[NIST AI RMF](https://www.nist.gov/artificial-intelligence)** | United States | All AI systems | Voluntary framework | None (guidance only) | Risk identification, measurement, management |
| **[Anthropic RSP](https://www.anthropic.com/index/anthropics-responsible-scaling-policy)** | Industry (Anthropic) | Internal models | Self-binding | Internal governance | ASL thresholds, capability evaluations |
| **[OpenAI Preparedness](https://openai.com/preparedness)** | Industry (OpenAI) | Internal models | Self-binding | Internal governance | Capability tracking, risk categorization |

### Framework Maturity and Coverage

| Framework | Dangerous Capabilities | Alignment Testing | Third-Party Audit | Post-Deployment | International Coordination |
|-----------|----------------------|-------------------|-------------------|-----------------|---------------------------|
| EU AI Act | Required for GPAI with systemic risk | Not explicitly required | Required for high-risk | Mandatory monitoring | EU member states |
| US EO 14110 | Required above threshold | Not specified | Recommended | Not specified | Bilateral agreements |
| UK AISI | Primary focus | Included in suite | AISI serves as evaluator | Ongoing partnerships | Co-leads International Network |
| NIST AI RMF | Guidance provided | Guidance provided | Recommended | Guidance provided | Standards coordination |
| Lab RSPs | Varies by lab | Varies by lab | Partial (METR, Apollo) | Varies by lab | Limited |

## Risk Assessment & Impact

| Dimension | Rating | Assessment |
|-----------|--------|------------|
| **Safety Uplift** | Medium | Creates accountability; limited by eval quality |
| **Capability Uplift** | Tax | May delay deployment |
| **Net World Safety** | Helpful | Adds friction and accountability |
| **Lab Incentive** | Weak | Compliance cost; may be required |
| **Scalability** | Partial | Evals must keep up with capabilities |
| **Deception Robustness** | Weak | Deceptive models could pass evals |
| **SI Readiness** | No | Can't eval SI safely |

### Research Investment

- **Current Investment**: \$10-30M/yr (policy development; eval infrastructure)
- **Recommendation**: Increase (needs better evals and enforcement)
- **Differential Progress**: Safety-dominant (adds deployment friction for safety)

## How Evals-Based Gates Work

Evaluation gates create checkpoints in the AI development and deployment pipeline:

<Mermaid client:load chart={`
flowchart TD
    A[Model Development] --> B[Pre-Deployment Evaluation]

    B --> C[Capability Evals]
    B --> D[Safety Evals]
    B --> E[Alignment Evals]

    C --> F{Pass All Gates?}
    D --> F
    E --> F

    F -->|Yes| G[Approved for Deployment]
    F -->|No| H[Blocked]

    H --> I[Remediation]
    I --> B

    G --> J[Deployment with Monitoring]
    J --> K[Post-Deployment Evals]
    K --> L{Issues Found?}
    L -->|Yes| M[Deployment Restricted]
    L -->|No| N[Continue Operation]

    style F fill:#ffddcc
    style H fill:#ffcccc
    style G fill:#d4edda
`} />

### Gate Types

| Gate Type | Trigger | Requirements | Example |
|-----------|---------|--------------|---------|
| **Pre-Training** | Before training begins | Risk assessment, intended use | EU AI Act high-risk requirements |
| **Pre-Deployment** | Before public release | Capability and safety evaluations | Lab RSPs, EO 14110 reporting |
| **Capability Threshold** | When model crosses defined capability | Additional safety requirements | Anthropic ASL transitions |
| **Post-Deployment** | After deployment, ongoing | Continued monitoring, periodic re-evaluation | Incident response requirements |

### Evaluation Categories

| Category | What It Tests | Purpose |
|----------|---------------|---------|
| **Dangerous Capabilities** | CBRN, cyber, persuasion, autonomy | Identify capability risks |
| **Alignment Properties** | Honesty, corrigibility, goal stability | Assess alignment |
| **Behavioral Safety** | Refusal behavior, jailbreak resistance | Test deployment safety |
| **Robustness** | Adversarial attacks, edge cases | Assess reliability |
| **Bias and Fairness** | Discriminatory outputs | Address societal concerns |

## Current Implementations

### Regulatory Requirements by Jurisdiction

The regulatory landscape for AI evaluation has developed significantly since 2023, with binding requirements in the EU and evolving frameworks elsewhere.

#### EU AI Act Requirements (Binding)

| Requirement Category | Specific Obligation | Deadline | Penalty for Non-Compliance |
|---------------------|---------------------|----------|---------------------------|
| **GPAI Model Evaluation** | Documented adversarial testing to identify systemic risks | August 2, 2025 | Up to EUR 15M or 3% global turnover |
| **High-Risk Conformity** | Risk management system across entire lifecycle | August 2, 2026 (Annex III) | Up to EUR 35M or 7% global turnover |
| **Technical Documentation** | Development, training, and evaluation traceability | August 2, 2025 (GPAI) | Up to EUR 15M or 3% global turnover |
| **Incident Reporting** | Track, document, report serious incidents to AI Office | Upon occurrence | Up to EUR 15M or 3% global turnover |
| **Cybersecurity** | Adequate protection for GPAI with systemic risk | August 2, 2025 | Up to EUR 15M or 3% global turnover |

*Source: [EU AI Act Implementation Timeline](https://artificialintelligenceact.eu/implementation-timeline/)*

#### US Requirements (Executive Order 14110, rescinded January 2025)

| Requirement | Threshold | Reporting Entity | Status |
|-------------|-----------|-----------------|--------|
| **Training Compute Reporting** | Above 10^26 FLOP | Model developers | Rescinded |
| **Biological Sequence Models** | Above 10^23 FLOP | Model developers | Rescinded |
| **Computing Cluster Reporting** | Above 10^20 FLOP capacity with 100 Gbps networking | Data center operators | Rescinded |
| **Red-Team Results** | Dual-use foundation models | Model developers | Rescinded |

*Note: EO 14110 was rescinded by President Trump in January 2025. Estimated training cost at 10^26 FLOP threshold: \$70-100M per model ([Anthropic estimate](https://www.congress.gov/crs-product/R47843)).*

#### UK Approach (Voluntary with Partnerships)

| Activity | Coverage | Access Model | Key Outputs |
|----------|----------|--------------|-------------|
| **Pre-deployment Testing** | 30+ frontier models tested since November 2023 | Partnership agreements with labs | Evaluation reports, risk assessments |
| **Inspect Framework** | Open-source evaluation tools | Publicly available | Used by governments, companies, academics |
| **Cyber Evaluations** | Model performance on apprentice to expert tasks | Pre-release access | Performance benchmarks (50% apprentice success 2025 vs 10% early 2024) |
| **Biological Risk** | CBRN capability assessment | Pre-release access | Risk categorization |
| **Self-Replication** | Purpose-built benchmarks for agentic behavior | Pre-release access | Early warning indicators |

*Source: [UK AISI 2025 Year in Review](https://www.aisi.gov.uk/blog/our-2025-year-in-review)*

### Lab Internal Gates

| Lab | Pre-Deployment Process | External Evaluation |
|-----|----------------------|---------------------|
| **Anthropic** | ASL evaluation, internal red team, external eval partnerships | METR, Apollo Research |
| **OpenAI** | Preparedness Framework evaluation, safety review | METR, partnerships |
| **Google DeepMind** | Frontier Safety Framework evaluation | Some external partnerships |

### Third-Party Evaluators

| Organization | Focus | Access Level | Funding Model |
|--------------|-------|--------------|---------------|
| **[METR](https://metr.org/)** | Autonomous capabilities | Pre-deployment access at Anthropic, OpenAI | Non-profit; does not accept monetary compensation from labs |
| **Apollo Research** | Alignment, scheming | Evaluation partnerships | Non-profit research |
| **[UK AISI](https://www.aisi.gov.uk/)** | Comprehensive evaluation | Voluntary pre-release partnerships | UK Government |
| **US AISI (NIST)** | Standards, coordination | NIST AI Safety Consortium | US Government |

*Note: According to the [2025 AI Safety Index](https://futureoflife.org/ai-safety-index-summer-2025/), only 3 of 7 major AI firms (Anthropic, OpenAI, Google DeepMind) report substantive testing for dangerous capabilities. One reviewer expressed "low confidence that dangerous capabilities are being detected in time to prevent significant harm, citing minimal overall investment in external third-party evaluations."*

### Evaluation Governance Ecosystem

<Mermaid client:load chart={`
flowchart TD
    subgraph Regulators["Regulatory Bodies"]
        EUAI[EU AI Office]
        NIST[NIST/US AISI]
        UKAISI[UK AISI]
    end

    subgraph Standards["Standards & Frameworks"]
        AIACT[EU AI Act<br/>Binding Requirements]
        RMF[NIST AI RMF<br/>Voluntary Framework]
        INSPECT[UK Inspect Tools<br/>Open Source]
    end

    subgraph Labs["AI Developers"]
        ANT[Anthropic<br/>RSP/ASL System]
        OAI[OpenAI<br/>Preparedness Framework]
        GDM[Google DeepMind<br/>Frontier Safety Framework]
    end

    subgraph ThirdParty["Third-Party Evaluators"]
        METR[METR<br/>Autonomous Capabilities]
        APOLLO[Apollo Research<br/>Alignment Testing]
    end

    subgraph Enforcement["Enforcement Mechanisms"]
        FINES[Fines<br/>Up to EUR 35M/7%]
        REPORTS[Public Reports]
        ACCESS[Market Access]
    end

    EUAI --> AIACT
    NIST --> RMF
    UKAISI --> INSPECT

    AIACT --> FINES
    AIACT --> ACCESS

    Labs --> ThirdParty
    ThirdParty --> REPORTS
    UKAISI --> Labs

    METR --> ANT
    METR --> OAI
    APOLLO --> ANT
    APOLLO --> OAI

    style AIACT fill:#d4edda
    style FINES fill:#ffcccc
    style ThirdParty fill:#fff3cd
`} />

## Limitations and Challenges

### Fundamental Issues

| Limitation | Explanation | Severity |
|------------|-------------|----------|
| **Unknown unknowns** | Can't test for unanticipated risks | High |
| **Evaluation gaming** | Models (or developers) optimize for passing evals | High |
| **Deceptive alignment** | Models could behave well only during evaluation | High |
| **Threshold calibration** | Hard to know where to set pass/fail criteria | Medium |
| **Speed of development** | Evals lag behind capability development | Medium |
| **Capability elicitation** | May not reveal true model capabilities | Medium |

### The Gaming Problem

Evaluation gates create incentives to game rather than genuinely satisfy:

| Actor | Gaming Approach | Countermeasure |
|-------|-----------------|----------------|
| **Developers** | Design evals models can pass | Third-party evaluation |
| **Models** | Sandbag during evaluation | Capability elicitation research |
| **Labs** | Interpret results favorably | External verification |
| **Regulators** | Set easy-to-meet thresholds | Expert input on standards |

### Deceptive Models and Evaluations

| Concern | Explanation |
|---------|-------------|
| **Evaluation awareness** | Sophisticated models may detect evaluation context |
| **Sandbagging** | Models may hide capabilities during testing |
| **Strategic behavior** | Models may behave well when watched, poorly otherwise |
| **Interpretation gaming** | Ambiguous results interpreted favorably |

## Key Cruxes

### Crux 1: Can Evaluations Provide Meaningful Safety Assurance?

| Position: Yes | Position: Limited |
|--------------|-------------------|
| Something is better than nothing | Can only test for known risks |
| Creates accountability structure | Deceptive models can pass |
| Forces safety consideration | May create false confidence |
| Can improve over time | Fundamental limits on what can be tested |

### Crux 2: Will Evaluations Keep Up With Capabilities?

| Position: Yes | Position: No |
|--------------|--------------|
| Eval science is rapidly improving | Capabilities advancing faster |
| Investment in eval research growing | Novel risks emerge constantly |
| AI can help create evals | Evals always lag capabilities |
| Can focus on most important risks | Can't predict what matters |

### Crux 3: Should Gates Be Mandatory or Voluntary?

| Mandatory | Voluntary |
|-----------|-----------|
| Creates level playing field | More flexible, adaptive |
| Prevents race to bottom | Industry expertise in implementation |
| Enables enforcement | Less regulatory overhead |
| Public accountability | Can be faster to update |

## Best Practices for Evaluation Gates

### Evaluation Design Principles

| Principle | Implementation |
|-----------|----------------|
| **Multiple evaluators** | Don't rely on single evaluation source |
| **Adversarial testing** | Include red-team evaluation |
| **Unknown test sets** | Don't let developers tune to known tests |
| **Capability elicitation** | Actively try to discover hidden capabilities |
| **Behavioral diversity** | Test across varied contexts |
| **Update regularly** | Evolve evals as understanding improves |

### Gate Implementation

<Mermaid client:load chart={`
flowchart TD
    A[Model Ready for Evaluation] --> B[Internal Evaluation]
    B --> C[Third-Party Evaluation]
    C --> D[Red Team Testing]
    D --> E[Results Synthesis]

    E --> F{Clear Pass?}
    F -->|Yes| G[Document and Deploy]
    F -->|Marginal| H[Enhanced Monitoring]
    F -->|No| I[Block + Remediate]

    G --> J[Post-Deployment Monitoring]
    H --> J
    J --> K[Continuous Evaluation]

    style F fill:#fff3cd
    style I fill:#ffcccc
`} />

### Evaluation Coverage

| Risk Category | Evaluation Approach | Maturity |
|---------------|-------------------|----------|
| **CBRN capabilities** | Domain-specific tests | Medium-High |
| **Cyber capabilities** | Penetration testing, CTF-style | Medium |
| **Persuasion/Manipulation** | Human studies, simulation | Medium |
| **Autonomous operation** | Sandbox environments | Medium |
| **Deceptive alignment** | Behavioral tests | Low |
| **Goal stability** | Distribution shift tests | Low |

## Recent Developments (2024-2025)

### Key Milestones

| Date | Development | Significance |
|------|-------------|--------------|
| **August 2024** | EU AI Act enters into force | First binding international AI regulation |
| **November 2024** | [UK-US joint model evaluation](https://www.aisi.gov.uk/) (Claude 3.5 Sonnet) | First government-to-government collaborative evaluation |
| **January 2025** | US EO 14110 rescinded | Removes federal AI evaluation requirements |
| **February 2025** | EU prohibited AI practices take effect | Enforcement begins for highest-risk categories |
| **June 2025** | [Anthropic-OpenAI joint evaluation](https://alignment.anthropic.com/2025/openai-findings/) | First cross-lab alignment evaluation exercise |
| **July 2025** | [NIST TEVV zero draft](https://www.globalpolicywatch.com/2025/08/nist-welcomes-comments-for-ai-standards-zero-drafts-project/) released | US framework development continues despite EO rescission |
| **August 2025** | EU GPAI model obligations take effect | Mandatory evaluation for general-purpose AI models |

### UK AISI Technical Progress

The UK AI Security Institute has emerged as a leading government evaluator, with measurable progress:

| Capability Domain | Early 2024 Performance | 2025 Performance | Trend |
|-------------------|----------------------|------------------|-------|
| **Cyber (apprentice tasks)** | ~10% success | 50% success | +40 percentage points |
| **Cyber (expert tasks)** | 0% success | First successful completions | Qualitative breakthrough |
| **Models evaluated** | Initial pilots | 30+ frontier models | Scale achieved |
| **International partnerships** | UK-US bilateral | Co-leads International Network | Expanding |

*Source: [UK AISI 2025 Year in Review](https://www.aisi.gov.uk/blog/our-2025-year-in-review)*

### Industry Self-Governance Evolution

| Lab | 2023 Commitments | 2025 Status | Notable Changes |
|-----|------------------|-------------|-----------------|
| **Anthropic** | RSP with ASL thresholds | Active; ASL-3 activated for Claude Opus 4 | Expanding to automated auditing |
| **OpenAI** | Preparedness Framework with third-party audit commitment | Third-party audit provision removed April 2025 | Reduced external accountability |
| **Google DeepMind** | Frontier Safety Framework | Active | Added Frontier Model Forum participation |

*Source: [AI Lab Watch Commitments Tracker](https://ailabwatch.org/)*

## Who Should Work on This?

**Good fit if you believe:**
- Governance structures add meaningful value
- Evaluation science can improve
- Accountability mechanisms help even if imperfect
- Near-term deployment safety matters

**Less relevant if you believe:**
- Evaluations fundamentally can't catch real risks
- Better to focus on alignment research
- Regulatory approaches are too slow
- Gaming makes gates ineffective

## Current Research Priorities

### Evaluation Science

| Priority | Description | Current State | Key Organizations |
|----------|-------------|---------------|-------------------|
| **Capability elicitation** | Methods to reveal hidden capabilities | Active research; UK AISI cyber evals show 50% apprentice-level success (vs 10% early 2024) | UK AISI, METR |
| **Alignment measurement** | Tests for genuine vs. surface alignment | Early stage; first cross-lab exercise completed June 2025 | Anthropic, OpenAI, Apollo |
| **Deception detection** | Behavioral tests for strategic deception | Early stage; automated auditing agents under development | Anthropic, Apollo Research |
| **Automated eval generation** | Scale evaluation creation | Emerging; [Bloom tool](https://alignment.anthropic.com/2025/bloom-auto-evals/) publicly released | Anthropic |
| **Standardization** | Shared eval suites across labs | Limited; UK Inspect tools gaining adoption | UK AISI, NIST |
| **International benchmarks** | Cross-border comparable metrics | [International Scientific AI Safety Report](https://www.aisi.gov.uk/) published 2025 | International Network of AI Safety Institutes |

### Governance Research

| Priority | Description | Current State | Gap |
|----------|-------------|---------------|-----|
| **Threshold calibration** | Where should capability gates be set? | EU: GPAI with systemic risk; US: 10^26 FLOP (rescinded) | No consensus on appropriate thresholds |
| **Enforcement mechanisms** | How to ensure compliance | EU: fines up to EUR 35M/7%; UK: voluntary | Most frameworks lack binding enforcement |
| **International coordination** | Cross-border standards | International Network of AI Safety Institutes co-led by UK/US | China not integrated; limited Global South participation |
| **Liability frameworks** | Consequences for safety failures | EU AI Act includes liability provisions | US and UK lack specific AI liability frameworks |
| **Third-party verification** | Independent safety assessment | Only 3 of 7 labs substantively engage third-party evaluators | Insufficient coverage and consistency |

## Sources & Resources

### Government Frameworks and Standards

| Source | Type | Key Content | Date |
|--------|------|-------------|------|
| [EU AI Act](https://artificialintelligenceact.eu/) | Binding Regulation | High-risk AI requirements, GPAI obligations, conformity assessment | August 2024 (in force) |
| [EU AI Act Implementation Timeline](https://artificialintelligenceact.eu/implementation-timeline/) | Regulatory Guidance | Phased deadlines through 2027 | Updated 2025 |
| [NIST AI RMF](https://www.nist.gov/artificial-intelligence/ai-standards) | Voluntary Framework | Risk management, evaluation guidance | July 2024 (GenAI Profile) |
| [NIST TEVV Zero Draft](https://www.globalpolicywatch.com/2025/08/nist-welcomes-comments-for-ai-standards-zero-drafts-project/) | Draft Standard | Testing, evaluation, verification, validation framework | July 2025 |
| [UK AISI 2025 Review](https://www.aisi.gov.uk/blog/our-2025-year-in-review) | Government Report | 30+ models tested, Inspect tools, international coordination | 2025 |
| [UK AISI Evaluations Update](https://www.aisi.gov.uk/blog/advanced-ai-evaluations-may-update) | Technical Update | Evaluation methodology, cyber and bio capability testing | May 2025 |
| [EO 14110](https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence) | Executive Order (Rescinded) | 10^26 FLOP threshold, reporting requirements | October 2023 |

### Industry Frameworks

| Source | Organization | Key Content | Date |
|--------|--------------|-------------|------|
| [Responsible Scaling Policy](https://www.anthropic.com/index/anthropics-responsible-scaling-policy) | Anthropic | ASL system, capability thresholds | September 2023 |
| [Preparedness Framework](https://openai.com/preparedness) | OpenAI | Risk categorization, deployment decisions | December 2023 |
| [Joint Evaluation Exercise](https://alignment.anthropic.com/2025/openai-findings/) | Anthropic & OpenAI | First cross-lab alignment evaluation | June 2025 |
| [Bloom Auto-Evals](https://alignment.anthropic.com/2025/bloom-auto-evals/) | Anthropic | Automated behavioral evaluation tool | 2025 |
| [Automated Auditing Agents](https://alignment.anthropic.com/2025/automated-auditing/) | Anthropic | AI-assisted safety auditing | 2025 |

### Third-Party Evaluation Organizations

| Organization | Website | Focus Area |
|--------------|---------|------------|
| [METR](https://metr.org/) | metr.org | Autonomous capabilities, pre-deployment testing |
| [Apollo Research](https://www.apolloresearch.ai/) | apolloresearch.ai | Alignment evaluation, deception detection |
| [AI Lab Watch](https://ailabwatch.org/) | ailabwatch.org | Tracking lab safety commitments |
| [Future of Life Institute AI Safety Index](https://futureoflife.org/ai-safety-index-summer-2025/) | futureoflife.org | Cross-lab safety comparison |

### Key Critiques and Limitations

| Critique | Evidence | Implication |
|----------|----------|-------------|
| **Inadequate dangerous capabilities testing** | Only 3 of 7 major labs substantively test ([AI Safety Index 2025](https://futureoflife.org/ai-safety-index-summer-2025/)) | Systematic gaps in coverage |
| **Third-party audit gaps** | OpenAI removed third-party audit commitment in April 2025 ([AI Lab Watch](https://ailabwatch.org/)) | Voluntary commitments may erode |
| **Unknown unknowns** | Cannot test for unanticipated risks | Fundamental limitation of evaluation approach |
| **Regulatory capture risk** | Industry influence on standards development | May result in weak requirements |
| **Evaluation gaming** | Models/developers optimize for passing known evals | May not reflect true safety |
| **International coordination gaps** | No binding global framework exists | Regulatory arbitrage possible |

---

## AI Transition Model Context

Evals-based deployment gates affect the <EntityLink id="ai-transition-model" /> through:

| Parameter | Impact |
|-----------|--------|
| <EntityLink id="safety-culture-strength" /> | Creates formal safety checkpoints |
| <EntityLink id="human-oversight-quality" /> | Provides evidence for oversight decisions |
| <EntityLink id="racing-dynamics" /> | Adds friction that may slow racing |

Evaluation gates are a valuable component of AI governance that creates accountability and evidence requirements. However, they should be understood as one layer in a comprehensive approach, not a guarantee of safety. The quality of evaluations, resistance to gaming, and enforcement of standards all significantly affect their value.
