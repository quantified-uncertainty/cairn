---
title: Evaluation & Detection
description: Methods for testing AI alignment, detecting dangerous capabilities, and identifying deceptive or misaligned behavior.
sidebar:
  label: Overview
  order: 0
---
import {EntityLink} from '@components/wiki';

Evaluation methods assess whether AI systems are aligned and safe to deploy.

**General Evaluation:**
- **<EntityLink id="evals">Evaluations (Evals)</EntityLink>**: Overview of AI evaluation approaches
- **<EntityLink id="alignment-evals">Alignment Evaluations</EntityLink>**: Testing for aligned behavior
- **<EntityLink id="dangerous-cap-evals">Dangerous Capability Evaluations</EntityLink>**: Assessing harmful potential

**Capability Assessment:**
- **<EntityLink id="capability-elicitation">Capability Elicitation</EntityLink>**: Uncovering hidden capabilities
- **<EntityLink id="red-teaming">Red Teaming</EntityLink>**: Adversarial testing for vulnerabilities
- **<EntityLink id="model-auditing">Model Auditing</EntityLink>**: Systematic capability review

**Deception Detection:**
- **<EntityLink id="scheming-detection">Scheming Detection</EntityLink>**: Identifying strategic deception
- **<EntityLink id="sleeper-agent-detection">Sleeper Agent Detection</EntityLink>**: Finding hidden malicious behaviors

**Deployment Decisions:**
- **<EntityLink id="safety-cases">Safety Cases</EntityLink>**: Structured arguments for deployment safety
