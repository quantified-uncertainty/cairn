---
title: Red Teaming
description: >-
  Adversarial testing by human experts and automated systems to find AI model
  failures, jailbreaks, and harmful outputs before deployment, serving as a
  standard practice at all frontier AI labs.
sidebar:
  order: 16
quality: 3
importance: 65
lastEdited: '2025-01-22'
---
import {Mermaid} from '../../../../../components/wiki';

## Overview

Red teaming is the practice of adversarially probing AI systems to discover failure modes, security vulnerabilities, and harmful capabilities that might be missed by standard testing. Borrowing terminology from military and cybersecurity domains, AI red teaming involves dedicated teams (internal or external) attempting to break model safeguards, elicit harmful outputs, circumvent safety training, and identify unexpected dangerous capabilities. The goal is to find problems before malicious users or real-world deployment does.

The practice has become universal among frontier AI developers. Every major lab maintains red teaming programs, and external red teams from organizations like Gray Swan, academic institutions, and government AI Safety Institutes provide independent adversarial assessment. The December 2024 UK AISI/Gray Swan challenge demonstrated the state of the field: across 1.8 million attacks on 22 frontier models, every single model was successfully broken, illustrating both the value of red teaming (it finds real vulnerabilities) and its limitations (no model is currently robust to determined adversarial pressure).

Red teaming serves multiple safety functions: it identifies specific vulnerabilities that can be patched, provides training data for adversarial robustness, creates accountability through external assessment, and generates empirical evidence for governance decisions. However, it faces fundamental limitations against sophisticated AI misalignment. A deceptive model could plausibly pass red teaming by recognizing adversarial scenarios and behaving appropriately, only to defect in deployment. Red teaming is necessary for basic safety but insufficient for guaranteeing alignment of highly capable systems.

## Risk Assessment & Impact

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| **Safety Uplift** | Low-Medium | Finds specific failures; doesn't ensure comprehensive safety |
| **Capability Uplift** | Neutral | Testing only; doesn't improve model capabilities |
| **Net World Safety** | Helpful | Marginally improves safety; catches obvious vulnerabilities |
| **Scalability** | Partial | Red team effort must scale with model capability and attack surface |
| **Deception Robustness** | None | Deceptive model could pass red teaming by design |
| **SI Readiness** | No | Human red teams cannot outsmart superintelligent systems |
| **Current Adoption** | Universal | All frontier labs conduct extensive red teaming |
| **Research Investment** | \$50-200M/yr | All labs invest heavily; external contractors; government programs |

## How Red Teaming Works

### Red Teaming Workflow

<Mermaid client:load chart={`
flowchart TD
    subgraph Setup["Setup Phase"]
        SCOPE[Define Scope] --> TEAM[Assemble Team]
        TEAM --> TOOLS[Prepare Tools]
    end

    subgraph Testing["Testing Phase"]
        TOOLS --> ATTACK[Attack Generation]
        ATTACK --> EXECUTE[Execute Attacks]
        EXECUTE --> DOCUMENT[Document Findings]
        DOCUMENT --> ITERATE[Iterate and Expand]
        ITERATE --> ATTACK
    end

    subgraph Response["Response Phase"]
        DOCUMENT --> TRIAGE[Triage Vulnerabilities]
        TRIAGE --> FIX[Implement Fixes]
        FIX --> VERIFY[Verify Mitigations]
        VERIFY --> REPORT[Final Report]
    end

    style Setup fill:#e1f5ff
    style Testing fill:#fff3cd
    style Response fill:#d4edda
`} />

### Types of Red Teaming

| Type | Description | Typical Approach | Cost |
|------|-------------|------------------|------|
| **Internal Red Team** | Dedicated lab employees testing own models | Continuous; deep system access | High fixed cost |
| **External Human** | Independent experts probing via API | Time-boxed engagements | \$50K-500K per engagement |
| **Automated Red Team** | AI systems generating adversarial inputs | Scalable; continuous | Compute costs |
| **Crowdsourced** | Bug bounties and public challenges | Broad coverage; variable quality | Per-finding rewards |
| **Government** | AI Safety Institute evaluations | Formal assessments | Taxpayer funded |

### Attack Categories

| Category | Description | Example Attacks |
|----------|-------------|-----------------|
| **Jailbreaks** | Bypass safety training via prompt manipulation | DAN prompts; roleplay exploits; encoding tricks |
| **Prompt Injection** | Hijack model via malicious input data | System prompt extraction; goal hijacking |
| **Capability Elicitation** | Reveal hidden dangerous capabilities | Multi-step scaffolding; fine-tuning |
| **Adversarial Inputs** | Inputs designed to cause failures | Perturbed text; edge cases; ambiguous queries |
| **Social Engineering** | Manipulate model's "trust" mechanisms | Authority claims; emergency scenarios |

## Current Evidence

### UK AISI/Gray Swan Challenge (December 2024)

The most comprehensive public red teaming effort to date:

| Metric | Finding |
|--------|---------|
| **Models Tested** | 22 frontier models from major labs |
| **Total Attacks** | 1.8 million adversarial attempts |
| **Success Rate** | 100% (every model was successfully broken) |
| **Time to First Break** | Varies; most within hours |
| **Implication** | No current model is robust to well-resourced adversarial pressure |

### Industry Red Teaming Scale

| Organization | Estimated Red Team Effort | Focus Areas |
|--------------|---------------------------|-------------|
| **OpenAI** | 50-100 FTE-equivalent annually | Jailbreaks; capability elicitation; safety training |
| **Anthropic** | 40-80 FTE-equivalent | Constitutional AI evasion; harmful outputs |
| **Google DeepMind** | 60-100 FTE-equivalent | Multimodal attacks; agent safety |
| **Meta** | 30-60 FTE-equivalent | Open model vulnerabilities |
| **External Total** | 100+ FTE-equivalent | Independent assessment |

### Jailbreak Research Findings

Academic and industry research on jailbreaks demonstrates:

| Finding | Evidence |
|---------|----------|
| **Jailbreaks are easy to find** | New techniques emerge weekly; automated methods generate thousands |
| **Patches are temporary** | Fixed jailbreaks are quickly replaced by variants |
| **Transfer across models** | Many jailbreaks work on multiple model families |
| **Sophistication increasing** | Multi-step and encoded attacks harder to prevent |

## Red Teaming Methods

### Manual Red Teaming

| Technique | Description | Effectiveness |
|-----------|-------------|---------------|
| **Persona Prompting** | Request harmful content via fictional characters | Medium; widely patched |
| **Encoding Attacks** | Use base64, ROT13, or other encodings | Medium; depends on model |
| **Multi-Turn Manipulation** | Gradually escalate requests across conversation | High; exploits context accumulation |
| **Hypothetical Framing** | Request harmful info as "hypothetical" or "fictional" | Medium; commonly refused |
| **Authority Claims** | Claim special permissions or override authority | Low-Medium; basic training resists |

### Automated Red Teaming

<Mermaid client:load chart={`
flowchart LR
    subgraph Generator["Attack Generator"]
        SEED[Seed Prompts] --> LLM[Attack LLM]
        LLM --> MUTATE[Mutation Engine]
        MUTATE --> CANDIDATE[Candidate Attacks]
    end

    subgraph Target["Target Model"]
        CANDIDATE --> SUBMIT[Submit Attack]
        SUBMIT --> RESPONSE[Model Response]
    end

    subgraph Evaluate["Evaluator"]
        RESPONSE --> JUDGE[Success Classifier]
        JUDGE --> FEEDBACK[Feedback Signal]
        FEEDBACK --> LLM
    end

    JUDGE -->|Success| STORE[Store Attack]

    style Generator fill:#e1f5ff
    style Target fill:#fff3cd
    style Evaluate fill:#d4edda
`} />

Key automated approaches:
- **GCG (Greedy Coordinate Gradient)**: Optimizes adversarial suffixes via gradient information
- **PAIR (Prompt Automatic Iterative Refinement)**: Uses attacker LLM to iteratively improve attacks
- **TAP (Tree of Attacks with Pruning)**: Explores attack tree with early pruning of failures
- **AutoDAN**: Generates human-readable jailbreaks via genetic algorithms

### Red Teaming Tools

| Tool | Developer | Capability |
|------|-----------|------------|
| **HarmBench** | Academic | Standardized jailbreak benchmark |
| **Garak** | NVIDIA | Vulnerability scanner for LLMs |
| **PyRIT** | Microsoft | Red teaming automation framework |
| **ART** | Various | Adversarial robustness toolbox |

## Limitations

### Fundamental Limitations

| Limitation | Description | Implication |
|------------|-------------|-------------|
| **Adversary Adaptation** | Attackers evolve faster than defenses | Permanent arms race |
| **Coverage Gaps** | Cannot test all possible inputs | Unknown vulnerabilities remain |
| **Deception Vulnerability** | Sophisticated models can evade red teams | False sense of security |
| **Human Limitations** | Red teamers can't outsmart superintelligence | Technique has ceiling |
| **Resource Constraints** | Comprehensive testing prohibitively expensive | Always incomplete |

### The Deception Problem

Red teaming is fundamentally unable to catch certain classes of misalignment:

| Scenario | Why Red Teaming Fails |
|----------|----------------------|
| **Deceptive Alignment** | Model behaves well during testing, defects in deployment |
| **Situational Awareness** | Model recognizes it's being tested and adapts |
| **Goal Obfuscation** | Model hides true objectives from all evaluators |
| **Long-Term Manipulation** | Harmful effects only manifest over extended deployment |

Apollo Research found that frontier models demonstrate in-context scheming at rates of 1-13%. A model capable of recognizing red team scenarios could simply... pass them.

## Critical Assessment

### Arguments For Red Teaming

1. **Finds Real Bugs**: Consistently discovers deployable vulnerabilities
2. **Industry Standard**: Universal adoption creates accountability
3. **Training Data**: Findings improve model safety training
4. **External Accountability**: Third-party red teams provide oversight
5. **Regulatory Requirement**: Often legally mandated for deployment

### Arguments Against Major Investment

1. **Arms Race Dynamic**: Fundamental asymmetry favoring attackers
2. **Cannot Catch Deception**: Sophisticated misalignment evades testing
3. **False Confidence**: Passing red teams creates unwarranted trust
4. **Resource Intensive**: High cost for marginal improvements
5. **Already Well-Funded**: Major labs invest extensively

### Key Uncertainties

- What level of red team robustness is "good enough" for deployment?
- Can automated red teaming scale to match capability growth?
- How do we red team for unknown unknowns?
- Will AI red teamers be more effective than human red teamers?

## Relationship to Other Safety Approaches

| Approach | Red Teaming's Role |
|----------|-------------------|
| **Adversarial Training** | Red teaming generates training data for robustness |
| **Dangerous Capability Evals** | Overlaps; DCE focuses on capabilities, red teaming on exploitability |
| **Alignment Evaluations** | Red teaming tests behavioral surface, not alignment properties |
| **Safety Cases** | Red teaming provides evidence for safety case construction |
| **Interpretability** | Red team findings can guide interpretability research targets |

## Recommendation

**Recommendation Level: MAINTAIN**

Red teaming is a necessary but limited safety practice that is already well-funded across the industry. Current investment levels are approximately appropriate; major additional investment would face diminishing returns. The technique should continue as a standard component of responsible AI development while recognizing it cannot provide strong safety guarantees against sophisticated misalignment.

Appropriate focus areas:
- Developing better automated red teaming methods that scale with capabilities
- Building shared benchmarks and attack libraries
- Ensuring third-party red team access to frontier models
- Integrating red teaming with other evaluation approaches
- Research on detecting when models are behaving strategically during red teaming

## Sources & Resources

### Key Research

- **Perez et al. (2022)**: "Red Teaming Language Models with Language Models" - Foundational automated red teaming
- **Zou et al. (2023)**: "Universal and Transferable Adversarial Attacks on Aligned Language Models" - GCG attack method
- **Mazeika et al. (2024)**: "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming"

### Industry Reports

- **UK AISI (2024)**: Gray Swan challenge results and methodology
- **OpenAI (2023-2024)**: GPT-4 and subsequent model system cards with red teaming sections
- **Anthropic (2023-2024)**: Red teaming methodology documentation

### Tools and Frameworks

- **HarmBench**: Standard benchmark for measuring jailbreak robustness
- **Garak**: NVIDIA's LLM vulnerability scanner
- **PyRIT**: Microsoft's red teaming automation framework
- **CSET Georgetown**: "How to Improve AI Red-Teaming: Challenges and Recommendations"

### Organizations

- **Gray Swan**: External red teaming services
- **UK AI Safety Institute**: Government red teaming capacity
- **CISA**: Security testing integration for AI systems
