---
title: "Lock-in Probability Model"
description: "Quantitative framework estimating 10-30% cumulative probability of significant AI-enabled lock-in by 2050, with timeline analysis showing a 5-20 year critical window. Compares scenario probabilities across totalitarian, value, economic, and geopolitical lock-in types."
sidebar:
  order: 2
maturity: "Growing"
quality: 44
llmSummary: "Quantitative framework estimating 10-30% cumulative probability of AI-enabled lock-in by 2050, with specific scenario probabilities: totalitarian surveillance (5-15%), value lock-in (10-20%), economic concentration (15-25%), and geopolitical lock-in (10-20%). Identifies 5-20 year critical windows for intervention depending on scenario type."
lastEdited: "2025-12-29"
importance: 58
pageTemplate: "knowledge-base-model"
todos:
  - "Complete 'Quantitative Analysis' section (8 placeholders)"
  - "Complete 'Strategic Importance' section"
  - "Complete 'Limitations' section (6 placeholders)"
ratings:
  novelty: 4.5
  rigor: 5
  actionability: 4
  completeness: 5.5
metrics:
  wordCount: 142
  citations: 11
  tables: 7
  diagrams: 0
---
import {DataInfoBox, Backlinks, R, EntityLink} from '@components/wiki';

<DataInfoBox entityId="lock-in-mechanisms" />

## Overview

This model provides a quantitative framework for assessing AI-enabled lock-in risk, complementing the comprehensive mechanism analysis in <EntityLink id="lock-in">Lock-in Risk</EntityLink>.

**For detailed coverage of mechanisms, current trends, and responses, see <EntityLink id="lock-in">Lock-in Risk</EntityLink>.**

## Probability Estimates by Scenario

The following probability estimates draw on expert assessments from <R id="246e6e1c19b04bbb">Future of Life Institute</R>, <R id="510fbddaf17ab0f9">EA Forum analyses</R>, and <R id="9cf1412a293bfdbe">Future of Humanity Institute</R> research:

| Scenario | Probability by 2050 | Duration if Realized | Key Drivers | Reversibility Window |
|----------|---------------------|---------------------|-------------|---------------------|
| Totalitarian surveillance state | 5-15% | Potentially indefinite | AI-enhanced monitoring, predictive policing, autonomous enforcement | 5-10 years before fully entrenched |
| Value lock-in via AI training | 10-20% | Centuries to millennia | Constitutional AI approaches, training data choices, <R id="e99a5c1697baa07d">RLHF value embedding</R> | 3-7 years during development phase |
| Economic power concentration | 15-25% | Decades to centuries | <R id="d9cd3292030e2674">Network effects, compute monopoly</R>, data advantages | 10-20 years with antitrust action |
| Geopolitical lock-in | 10-20% | Decades to centuries | First-mover AI advantages, regulatory capture | Uncertain, depends on coordination |
| Aligned singleton (positive) | 5-10% | Indefinite | Successful alignment, beneficial governance | N/A (desirable outcome) |
| Misaligned AI takeover | 2-10% | Permanent | Deceptive alignment, capability overhang | Days to weeks at critical juncture |

**Note:** These ranges reflect significant uncertainty. The <R id="510fbddaf17ab0f9">stable totalitarianism analysis</R> suggests extreme scenarios may be below 1%, while other researchers place combined lock-in risk at 10-30%.

## Risk Factor Framework

| Risk Factor | Mechanism | AI Amplification | Reversibility |
|-------------|-----------|------------------|---------------|
| Enforcement capability | Autonomous systems maintain control | 10-100x more comprehensive surveillance; no human defection risk | Very Low |
| Path dependence | Early choices constrain future options | Faster deployment cycles compress decision windows | Low |
| Network effects | Systems become more valuable as adoption grows | AI models compound advantages via data and compute | Low-Medium |
| Value embedding | Preferences encoded during development persist | Constitutional AI approaches embed values during training | Medium |
| Complexity barriers | System understanding requires specialized expertise | AI systems may become inscrutable even to developers | Very Low |

## Timeline Indicators

The <R id="da390c7cc819b788">IMD AI Safety Clock</R> tracks lock-in urgency:

| Date | Clock Position | Key Developments |
|------|----------------|------------------|
| September 2024 | 29 minutes to midnight | Clock launched |
| December 2024 | 26 minutes | AGI timeline acceleration |
| February 2025 | 24 minutes | California SB 1047 vetoed |
| September 2025 | 20 minutes | Agentic AI proliferation |

The nine-minute advance in one year reflects compressed decision timelines.

## Model Limitations

This framework cannot capture:
- Novel lock-in pathways not yet identified
- Interaction effects between scenarios
- Tail risks from capability discontinuities
- Political feasibility of interventions

For intervention analysis, see <EntityLink id="lock-in">Lock-in Risk: Responses</EntityLink>.

## Sources

- <R id="510fbddaf17ab0f9">Stable Totalitarianism: An Overview</R> - EA Forum analysis
- <R id="da390c7cc819b788">IMD AI Safety Clock</R> - Real-time risk tracking
- <R id="713ad72e6bc4d52a">Bostrom on permanent lock-in scenarios</R>
- <R id="297ced45b445881c">Finnveden, Riedel, and Shulman on AI-enabled dictatorship</R>

<Backlinks client:load entityId="lock-in-mechanisms" />
