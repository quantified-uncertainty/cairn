---
title: Knowledge Base
description: Comprehensive documentation of AI safety risks, responses, organizations, and key debates
sidebar:
  label: Overview
  order: 0
---

## Overview

The CAIRN Knowledge Base provides structured documentation of the AI safety landscape, covering risks, interventions, organizations, and key debates. Content is organized to help researchers, funders, and policymakers understand the current state of AI safety and make informed decisions about resource allocation.

## Content Categories

### [Risks](/knowledge-base/risks/accident/scheming/)
Documentation of potential failure modes and hazards from advanced AI systems, organized by type:
- **Accident Risks** - Unintended failures like scheming, deceptive alignment, mesa-optimization
- **Misuse Risks** - Deliberate harmful applications like bioweapons, cyberweapons, disinformation
- **Structural Risks** - Systemic issues like racing dynamics, concentration of power, lock-in
- **Epistemic Risks** - Threats to knowledge and truth like authentication collapse, trust erosion

### [Responses](/knowledge-base/responses/alignment/interpretability/)
Interventions and approaches to address AI risks:
- **Technical Alignment** - Interpretability, RLHF, constitutional AI, AI control
- **Governance** - Compute governance, international coordination, legislation
- **Institutional** - AI safety institutes, standards bodies
- **Epistemic Tools** - Prediction markets, content authentication, coordination technologies

### [Models](/knowledge-base/models/framework-models/carlsmith-six-premises/)
Analytical frameworks for understanding AI risk dynamics:
- **Framework Models** - Carlsmith's six premises, instrumental convergence
- **Risk Models** - Deceptive alignment decomposition, scheming likelihood
- **Dynamics Models** - Racing dynamics impact, feedback loops
- **Societal Models** - Trust erosion, lock-in mechanisms

### [Organizations](/knowledge-base/organizations/labs/openai/)
Profiles of key actors in AI development and safety:
- **AI Labs** - OpenAI, Anthropic, DeepMind, xAI
- **Safety Research Orgs** - MIRI, ARC, Redwood, Apollo Research
- **Government Bodies** - US AISI, UK AISI

### [People](/knowledge-base/people/paul-christiano/)
Profiles of influential researchers and leaders in AI safety.

### [Capabilities](/knowledge-base/capabilities/agentic-ai/)
Documentation of AI capability domains and their safety implications.

### [Debates](/knowledge-base/debates/is-ai-xrisk-real/)
Structured analysis of key disagreements in the field.

### [Cruxes](/knowledge-base/cruxes/accident-risks/)
Key uncertainties that drive disagreement and prioritization decisions.

## How to Use This Knowledge Base

1. **Exploring risks**: Start with the [scheming](/knowledge-base/risks/accident/scheming/) page for the most discussed risk, then browse related accident risks
2. **Understanding responses**: See [interpretability](/knowledge-base/responses/alignment/interpretability/) for a well-documented technical approach
3. **Analytical depth**: The [Carlsmith six-premise model](/knowledge-base/models/framework-models/carlsmith-six-premises/) provides a rigorous framework for AI risk estimation
4. **Browse everything**: Use the [Explore page](/explore/) to search and filter all 1500+ entries

## Quality Indicators

Pages include quality and importance ratings:
- **Quality (0-100)**: How well-developed and accurate the content is
- **Importance (0-100)**: How significant the topic is for AI safety decisions

High-priority pages (quality < importance) are actively being improved.
