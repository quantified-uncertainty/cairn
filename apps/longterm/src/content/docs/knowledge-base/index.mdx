---
title: Knowledge Base
description: Comprehensive documentation of AI safety risks, responses, organizations, and key debates
sidebar:
  label: Overview
  order: 0
---
import {EntityLink} from '../../../components/wiki';


## Overview

The CAIRN Knowledge Base provides structured documentation of the AI safety landscape, covering risks, interventions, organizations, and key debates. Content is organized to help researchers, funders, and policymakers understand the current state of AI safety and make informed decisions about resource allocation.

## Content Categories

### <EntityLink id="risks/accident/scheming">Risks</EntityLink>
Documentation of potential failure modes and hazards from advanced AI systems, organized by type:
- **Accident Risks** - Unintended failures like scheming, deceptive alignment, mesa-optimization
- **Misuse Risks** - Deliberate harmful applications like bioweapons, cyberweapons, disinformation
- **Structural Risks** - Systemic issues like racing dynamics, concentration of power, lock-in
- **Epistemic Risks** - Threats to knowledge and truth like authentication collapse, trust erosion

### <EntityLink id="responses/alignment/interpretability">Responses</EntityLink>
Interventions and approaches to address AI risks:
- **Technical Alignment** - Interpretability, RLHF, constitutional AI, AI control
- **Governance** - Compute governance, international coordination, legislation
- **Institutional** - AI safety institutes, standards bodies
- **Epistemic Tools** - Prediction markets, content authentication, coordination technologies

### <EntityLink id="models/framework-models/carlsmith-six-premises">Models</EntityLink>
Analytical frameworks for understanding AI risk dynamics:
- **Framework Models** - Carlsmith's six premises, instrumental convergence
- **Risk Models** - Deceptive alignment decomposition, scheming likelihood
- **Dynamics Models** - Racing dynamics impact, feedback loops
- **Societal Models** - Trust erosion, lock-in mechanisms

### <EntityLink id="organizations/labs/openai">Organizations</EntityLink>
Profiles of key actors in AI development and safety:
- **AI Labs** - OpenAI, Anthropic, DeepMind, xAI
- **Safety Research Orgs** - MIRI, ARC, Redwood, Apollo Research
- **Government Bodies** - US AISI, UK AISI

### <EntityLink id="people/paul-christiano">People</EntityLink>
Profiles of influential researchers and leaders in AI safety.

### <EntityLink id="capabilities/agentic-ai">Capabilities</EntityLink>
Documentation of AI capability domains and their safety implications.

### <EntityLink id="debates/is-ai-xrisk-real">Debates</EntityLink>
Structured analysis of key disagreements in the field.

### <EntityLink id="cruxes/accident-risks">Cruxes</EntityLink>
Key uncertainties that drive disagreement and prioritization decisions.

## How to Use This Knowledge Base

1. **Exploring risks**: Start with the <EntityLink id="risks/accident/scheming">scheming</EntityLink> page for the most discussed risk, then browse related accident risks
2. **Understanding responses**: See <EntityLink id="responses/alignment/interpretability">interpretability</EntityLink> for a well-documented technical approach
3. **Analytical depth**: The <EntityLink id="models/framework-models/carlsmith-six-premises">Carlsmith six-premise model</EntityLink> provides a rigorous framework for AI risk estimation
4. **Browse everything**: Use the [Explore page](/explore/) to search and filter all 1500+ entries

## Quality Indicators

Pages include quality and importance ratings:
- **Quality (0-100)**: How well-developed and accurate the content is
- **Importance (0-100)**: How significant the topic is for AI safety decisions

High-priority pages (quality < importance) are actively being improved.
