---
title: "Redwood Research"
description: "AI safety lab focused on empirical research, mechanistic interpretability, and pioneering AI control methodology for safety without full alignment"
sidebar:
  order: 12
quality: 72
llmSummary: "Redwood Research pioneered the 'AI control' paradigm (safety without full alignment) and contributed causal scrubbing methodology to interpretability, with 15+ researchers, $5M+ funding, and notable field-building through 20+ fellowship alumni now at major labs. Their control framework shows 70-90% detection rates in toy models and has gained adoption across the safety community since 2023."
lastEdited: "2025-12-24"
importance: 58.5
---
import {DataInfoBox, DisagreementMap, EstimateBox, KeyPeople, KeyQuestions, Section, Backlinks, R, EntityLink, DataExternalLinks} from '../../../../../components/wiki';

<DataExternalLinks pageId="redwood" client:load />

<DataInfoBox entityId="redwood" />

## Overview

Redwood Research is an AI safety lab founded in 2021 that has pioneered the "AI control" research paradigm while making significant contributions to mechanistic interpretability. The organization operates on the pragmatic assumption that alignment may not be fully solved in time, leading to their groundbreaking work on ensuring safety even with potentially misaligned AI systems.

Their trajectory shows remarkable adaptability: beginning with adversarial robustness research, pivoting to mechanistic interpretability with their influential causal scrubbing methodology, and ultimately developing the AI control framework that has gained significant attention across the safety community. With 15+ researchers and \$1M+ in funding, Redwood has trained numerous safety researchers now working across <EntityLink id="anthropic">Anthropic</EntityLink>, <EntityLink id="arc">ARC</EntityLink>, and other major organizations.

The organization's core insight is that traditional alignment approaches may be insufficient for highly capable systems, necessitating "control" measures that prevent catastrophic outcomes even from scheming AI systems.

## Risk and Impact Assessment

| Factor | Assessment | Evidence | Timeline | Source |
|--------|------------|----------|----------|--------|
| **Technical Influence** | High | Causal scrubbing adopted by Anthropic, 100+ citations | 2022-present | <R id="f771d4f56ad4dbaa">Anthropic research</R> |
| **Field Building** | High | 20+ fellowship alumni at major labs | 2021-present | <R id="bff2f5843023e85e">EA Forum tracking</R> |
| **Control Paradigm Adoption** | Medium | Growing interest across labs | 2023-present | <R id="2e0c662574087c2a">AI safety discussions</R> |
| **Policy Relevance** | Medium | Control more implementable than full alignment | 2024+ | <R id="4e428338d4b3dad7">RAND analysis</R> |
| **Research Quality** | High | Rigorous empirical methodology | Ongoing | Multiple peer reviews |

## Key Research Evolution

### Phase 1: Adversarial Robustness (2021-2022)

**Problem Focus**: Language models could be jailbroken despite safety training

| Challenge | Approach | Results | Lessons |
|-----------|----------|---------|---------|
| Harmful content generation | Adversarial training on refusal | Some domain-specific success | Adversarial examples persist |
| Jailbreak robustness | Systematic prompt engineering | Limited scalability | Need internal understanding |
| Behavioral safety | External training constraints | Fundamental brittleness | Surface-level solutions insufficient |

**Key Finding**: Behavioral safety training has fundamental limitations that don't scale to existential safety concerns.

### Phase 2: Mechanistic Interpretability (2022-2023)

**Strategic Pivot**: Understanding internal computations rather than just behavior

**Major Contribution - Causal Scrubbing**:
- **Innovation**: Rigorous testing framework for interpretability hypotheses
- **Method**: Ablate components according to proposed explanation, measure behavioral changes
- **Impact**: Raised standards across interpretability field
- **Adoption**: Used by <EntityLink id="anthropic">Anthropic</EntityLink>, multiple academic groups
- **Citations**: 150+ across major venues

| Research Area | Contribution | Impact | Follow-up Work |
|---------------|-------------|--------|----------------|
| Circuit Analysis | Causal scrubbing methodology | Field standard | Adopted by Anthropic |
| Polysemanticity | Superposition investigation | Theoretical insight | Dictionary learning research |
| Feature Extraction | Automated circuit discovery | Tool development | Open-source implementations |

### Phase 3: AI Control (2023-Present)

**Revolutionary Paradigm**: Safety without requiring full alignment

**Core Framework**:
- **Assumption**: Deploy potentially misaligned but capable AI systems
- **Goal**: Prevent catastrophic outcomes despite potential scheming
- **Analogy**: Cybersecurity mindset applied to AI safety

| Control Component | Description | Research Status | Effectiveness |
|------------------|-------------|-----------------|---------------|
| **Monitoring** | Detect deceptive/scheming behavior | Active research | 70-90% detection in toy models |
| **Protocol Design** | Safe deployment procedures | Framework developed | Testing in controlled settings |
| **Red-team Testing** | Adversarial evaluation of controls | Ongoing | Multiple vulnerabilities identified |
| **Untrusted Monitoring** | AI monitoring AI systems | Early research | Theoretical framework |

## Current Research Portfolio

### AI Control Research

**Key Publications**:
- <R id="187aaa26886ce183">"AI Control: Improving Safety Despite Intentional Subversion"</R> (2024)
- Control evaluation methodologies
- Red-team protocol development

**Research Questions**:
- Can monitoring detect sophisticated deception?
- What deployment protocols minimize catastrophic risk?
- How to design safety cases for potentially misaligned AI?

### Interpretability Methods

**Ongoing Work**:
- Activation steering techniques
- Automated circuit discovery
- <EntityLink id="deceptive-alignment">Deceptive alignment</EntityLink> detection
- Understanding <EntityLink id="situational-awareness">situational awareness</EntityLink>

**Tools Developed**:
- Open-source interpretability libraries
- Circuit analysis frameworks
- Causal intervention methodologies

### Empirical Safety Research

| Project Area | Focus | Status | Collaboration |
|-------------|-------|--------|---------------|
| Sandbagging Detection | Models hiding capabilities | Ongoing | With <EntityLink id="arc">ARC</EntityLink> |
| Jailbreak Robustness | Systematic prompt attacks | Published | Multiple labs |
| Training Dynamics | Misalignment development | Research phase | Academic partners |
| Evaluation Methodology | Safety assessment frameworks | Active | Industry consultation |

## Key Personnel and Expertise

<Section title="Leadership Team">
  <KeyPeople people={[
    { name: "Buck Shlegeris", role: "CEO & Co-founder", expertise: "Strategic direction, empirical methodology" },
    { name: "Ryan Greenblatt", role: "Research Lead", expertise: "AI control, adversarial evaluation" },
    { name: "William Saunders", role: "Research Scientist", expertise: "Interpretability, training dynamics" },
    { name: "Neel Nanda", role: "Former Senior Researcher (now DeepMind)", expertise: "Mechanistic interpretability" },
  ]} />
</Section>

### Buck Shlegeris (CEO)
- **Background**: Software engineering, rationalist community
- **Philosophy**: Empirical focus, rapid iteration, evidence-driven pivots
- **Contribution**: Organizational strategy, research direction setting
- **Public Role**: Active in safety forums, clear research communication

### Ryan Greenblatt
- **Expertise**: Machine learning, adversarial evaluation
- **Key Work**: AI control framework development, red-team methodologies
- **Approach**: Rigorous threat modeling, security-minded safety research

### Research Culture
- **Fellows Program**: Trained 20+ researchers now at major safety organizations
- **Methodology**: Empirical validation, quick pivots based on evidence
- **Collaboration**: Active partnerships with labs and safety organizations

## Current State and Trajectory

### Organizational Metrics

| Metric | 2021 | 2022 | 2023 | 2024 | Projection 2025 |
|--------|------|------|------|------|-----------------|
| Research Staff | 5 | 8 | 12 | 15 | 20-25 |
| Annual Budget | \$1M | \$2M | \$3.5M | \$5M | \$7M |
| Publications | 0 | 3 | 8 | 12 | 15-20 |
| Fellows Trained | 0 | 5 | 12 | 20 | 30+ |

### Research Impact

**Citation Metrics**:
- Causal scrubbing paper: 150+ citations
- AI control framework: 50+ citations (recent publication)
- Total research impact: 500+ citations across work

**Adoption Indicators**:
- Anthropic implementing causal scrubbing methods
- Multiple labs exploring control approaches
- Policy discussions referencing control paradigm

### Strategic Positioning

**2024-2025 Focus**:
- Scaling AI control research
- Developing practical safety protocols
- Building industry partnerships for testing
- Expanding empirical evaluation capabilities

**Funding Trajectory**: Diversifying from grants to include industry partnerships while maintaining research independence.

## Key Uncertainties and Debates

### The Control Paradigm

<Section title="Major Questions">
  <KeyQuestions questions={[
    "Can control measures actually work against superintelligent scheming AI?",
    "Does the control approach create false confidence in deployment safety?",
    "Should resources focus on alignment research rather than control?",
    "How to validate control measures without access to truly dangerous AI?",
    "What is the appropriate balance between control and alignment research?"
  ]} />
</Section>

| Uncertainty | Pessimistic View | Optimistic View | Current Evidence |
|-------------|-----------------|-----------------|------------------|
| **Control Effectiveness** | Superintelligent AI will circumvent any controls | Layered defenses can achieve acceptable risk | Limited to toy model testing |
| **Resource Allocation** | Control distracts from alignment work | Control provides crucial safety layer | Mixed views in safety community |
| **Timeline Relevance** | Won't be ready before dangerous AI | Can develop alongside capabilities | Active research progress |
| **Industry Adoption** | Labs won't implement costly controls | Control more practical than alignment | Early industry interest |

### Technical Challenges

**Monitoring Limitations**:
- Can sophisticated AI hide deceptive behavior?
- How to detect novel attack vectors?
- Scalability of human oversight

**Protocol Design**:
- Balance between safety and capability
- Economic incentives for control adoption
- Verification of control implementations

### Philosophical Questions

**Relationship to Alignment**:
- Complement or substitute for alignment research?
- Risk of reducing alignment investment?
- Long-term vision for AI safety

## Comparative Analysis

### Research Approach Comparison

| Organization | Primary Focus | Timeline View | Methodology | Success Theory |
|-------------|---------------|---------------|-------------|----------------|
| **Redwood** | Control + interpretability | Medium pessimism | Empirical, adaptive | Safety without full alignment |
| **<EntityLink id="anthropic">Anthropic</EntityLink>** | Constitutional AI, scaling | Cautious optimism | Large-scale experiments | Alignment through training |
| **<EntityLink id="arc">ARC</EntityLink>** | Evaluation, alignment theory | Varied | Theory + evaluation | Early detection and preparation |
| **<EntityLink id="miri">MIRI</EntityLink>** | Governance pivot | High pessimism | Theoretical (paused) | Coordination over technical |

### Unique Positioning

**Distinguishing Features**:
- Only major org focused primarily on control
- Empirical methodology with willingness to pivot
- Security-minded approach to safety
- Practical deployment focus

**Strategic Advantages**:
- Complementary to alignment research
- More implementable in near-term
- Appeals to industry safety concerns
- Builds on existing security practices

## Sources and Resources

### Primary Research Publications

| Publication | Year | Impact | Focus |
|-------------|------|--------|-------|
| <R id="f9c9e72b33919831">Causal Scrubbing Paper</R> | 2023 | 150+ citations | Interpretability methodology |
| <R id="187aaa26886ce183">AI Control Framework</R> | 2024 | Foundational | Control paradigm |
| <R id="b13cc372a1b6ac57">Adversarial Robustness Studies</R> | 2022-2023 | Field building | Empirical safety |

### Organizational Resources

| Type | Resource | Access |
|------|----------|--------|
| **Research Portal** | <R id="42e7247cbc33fc4c">Redwood Research</R> | Public |
| **Publications** | <R id="d42c3c74354e7b66">Research Publications</R> | Open access |
| **Code/Tools** | <R id="5827cc57df85aede">GitHub Repository</R> | Open source |
| **Fellowship** | <R id="378b202afd1be3b5">Research Fellowship</R> | Application-based |

### External Analysis

| Source | Focus | Assessment |
|--------|-------|------------|
| <R id="dd0cf0ff290cc68e">Open Philanthropy</R> | Funding analysis | Major safety contribution |
| <R id="80882a02c4f17f9f">Alignment Forum</R> | Technical discussion | Mixed reception of control approach |
| <R id="876bb3bfc6031642">AI Safety Newsletter</R> | Regular coverage | Positive on empirical methodology |
| <R id="1593095c92d34ed8">Future of Humanity Institute</R> | Academic perspective | Valuable addition to safety portfolio |

### Related Safety Organizations

| Organization | Relationship | Collaboration Type |
|-------------|--------------|-------------------|
| **<EntityLink id="anthropic">Anthropic</EntityLink>** | Research collaboration | Interpretability methods sharing |
| **<EntityLink id="arc">ARC</EntityLink>** | Complementary focus | Evaluation methodology |
| **<EntityLink id="chai">CHAI</EntityLink>** | Academic partnership | Research collaboration |
| **<EntityLink id="apollo-research">Apollo Research</EntityLink>** | Similar approach | Evaluation and testing |

### Policy and Governance Context

| Institution | Engagement | Focus |
|------------|------------|-------|
| **<EntityLink id="uk-aisi">UK AISI</EntityLink>** | Consultation | Control methodologies |
| **<EntityLink id="us-aisi">US AISI</EntityLink>** | Framework discussion | Deployment safety |
| **NIST AI RMF** | Input provision | Risk management |
| **Industry Standards** | Development participation | Safety protocols |

<Backlinks client:load entityId="redwood" />