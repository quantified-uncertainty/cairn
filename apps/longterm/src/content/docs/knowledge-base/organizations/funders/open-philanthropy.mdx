---
title: "Open Philanthropy"
description: "Open Philanthropy (renamed Coefficient Giving in November 2025) is the world's largest funder of AI safety research, having directed over $4 billion total across all cause areas since 2017, with approximately $336 million (~12%) to AI safety. Backed by Facebook co-founder Dustin Moskovitz and Cari Tuna through Good Ventures, OP employs 'hits-based giving'—high-risk, high-reward philanthropy—and 'worldview diversification' across four philosophical frameworks. Their AI safety portfolio includes MIRI ($14M+), Redwood Research ($20M+), METR, and university programs, plus board representation at Anthropic via Luke Muehlhauser (2021-2024). Key staff research has shaped field understanding: Ajeya Cotra's Bio Anchors report (2020, updated 2022) forecasts 50% chance of transformative AI by 2040, while Joseph Carlsmith's power-seeking AI analysis estimates >10% existential risk by 2070. In 2025, OP launched a $40M Technical AI Safety RFP across 21 research areas."
sidebar:
  order: 2
quality: 72
llmSummary: "Open Philanthropy (now Coefficient Giving) is the world's largest AI safety funder, having directed ~$336M to AI safety since 2017 out of $4B+ total grantmaking. Their 'hits-based giving' philosophy embraces high-risk, high-reward grants across four worldviews. Major grantees include MIRI ($14M+), Redwood Research ($20M), METR, and university programs. Key research outputs: Ajeya Cotra's Bio Anchors report (50% TAI by 2040), Joseph Carlsmith's power-seeking AI analysis (>10% x-risk by 2070). The 2025 Technical AI Safety RFP offers $40M+ across 21 research areas. Luke Muehlhauser leads AI governance ($100M+/year target), while Holden Karnofsky joined Anthropic in 2025. Primary funding via Good Ventures (Dustin Moskovitz/Cari Tuna, $20B+ committed to Giving Pledge)."
lastEdited: "2026-01-29"
importance: 45
ratings:
  novelty: 4
  rigor: 7
  actionability: 6
  completeness: 7
metrics:
  wordCount: 4200
  citations: 28
  tables: 18
  diagrams: 2
---
import {DataInfoBox, Backlinks, Mermaid} from '../../../../../components/wiki';

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Scale** | Largest AI Safety Funder | ~\$336M to AI safety since 2017; \$4B+ total across all causes |
| **Annual AI Safety Giving** | \$40-100M | Varies by year; multi-year grants cause fluctuations |
| **AI Focus** | Growing Share | ~12% of total historically; increasing priority |
| **Transparency** | Very High | [Public grants database](https://www.openphilanthropy.org/grants/), detailed writeups |
| **Influence** | Field-Defining | Sets research priorities, incubates orgs, shapes talent pipeline |
| **Grantmaking Philosophy** | Hits-Based | High-risk, high-reward; tolerates most grants "failing" |
| **Key Research** | Foundational | Bio Anchors timelines, power-seeking AI risk estimates |
| **Name Change** | November 2025 | Now "Coefficient Giving" with multi-donor fund model |

## Organization Details

| Attribute | Details |
|-----------|---------|
| **Current Name** | Coefficient Giving (as of November 2025) |
| **Former Name** | Open Philanthropy (2019-2025), Open Philanthropy Project (2014-2019) |
| **Type** | Grantmaking and research organization; LLC structure |
| **Founded** | 2011 as GiveWell Labs; renamed Open Philanthropy Project 2014; independent 2017 |
| **Primary Backers** | Dustin Moskovitz (Facebook co-founder, \$12B+ net worth), Cari Tuna |
| **Giving Vehicle** | Good Ventures Foundation (Moskovitz/Tuna foundation) |
| **Giving Pledge** | Moskovitz/Tuna signed in 2010 (youngest signatories); \$20B+ committed |
| **Total Grantmaking** | \$4B+ since founding (as of June 2025) |
| **2025 Giving** | \$600M+ (across all cause areas) |
| **AI Safety Total** | ~\$336M (~12% of total historically) |
| **Staff** | ~100 employees |
| **Structure** | Four worldview-based teams; advisory to Good Ventures |
| **Website** | [coefficientgiving.org](https://coefficientgiving.org/) (formerly openphilanthropy.org) |
| **Grants Database** | [coefficientgiving.org/grants](https://www.openphilanthropy.org/grants/) |

## Overview

Open Philanthropy (OP), renamed Coefficient Giving in November 2025, is the world's largest funder of AI safety research and a dominant force in effective altruism philanthropy. The organization emerged from a 2011 collaboration between GiveWell and Good Ventures—the foundation of Facebook co-founder Dustin Moskovitz and his wife Cari Tuna—when the couple sought guidance on deploying their fortune effectively. The partnership initially operated as GiveWell Labs before becoming the Open Philanthropy Project in 2014 and gaining full independence in 2017.

As of June 2025, Coefficient Giving has directed more than \$4 billion in grants across multiple cause areas, with approximately \$336 million (~12% of total) allocated to AI safety since inception. The organization's grantmaking philosophy explicitly embraces "hits-based giving"—making high-risk, high-reward grants where most may fail but a few successes could be transformative. This approach, borrowed from venture capital investing, has funded unconventional bets that traditional philanthropy would avoid.

Beyond direct grantmaking, OP shapes the AI safety field through foundational research publications, organizational incubation, and talent development. Staff members have produced widely-cited analyses including Ajeya Cotra's [Bio Anchors report](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/) forecasting transformative AI timelines and Joseph Carlsmith's [power-seeking AI risk analysis](https://arxiv.org/abs/2206.13353) estimating existential risk probabilities. The organization's funding decisions effectively set research priorities for the field, making it both funder and intellectual leader.

The November 2025 rebranding to Coefficient Giving signals an expansion toward operating multi-donor funds that other philanthropists can join, moving beyond exclusive reliance on Good Ventures. This evolution reflects both the organization's growth and the recognition that AI safety funding remains concentrated—an estimated 80% of philanthropic AI safety funds flow through Open Philanthropy and the Survival and Flourishing Fund (SFF).

## History and Evolution

### Timeline

| Year | Event | Significance |
|------|-------|--------------|
| **2009** | Cari Tuna meets Dustin Moskovitz | Future philanthropic partnership begins; Tuna at WSJ, Moskovitz post-Facebook |
| **2010** | Moskovitz/Tuna sign Giving Pledge | Youngest signatories; commit to giving majority of \$20B+ fortune |
| **2011** | GiveWell Labs founded | Collaboration between GiveWell and Good Ventures to explore high-impact giving |
| **2014** | Renamed Open Philanthropy Project | Separated branding from GiveWell; formalized as distinct initiative |
| **2015** | AI safety grantmaking begins | First grants to MIRI and AI safety research |
| **2017** | Full independence from GiveWell | Became Open Philanthropy Project LLC; expanded beyond 501(c)(3) grants |
| **2017** | \$30M grant to OpenAI | Holden Karnofsky joins OpenAI board; partnership established |
| **2019** | Shortened name to Open Philanthropy | Dropped "Project" from name |
| **2020** | Bio Anchors report published | Ajeya Cotra's AI timelines framework; foundational field document |
| **2021** | Luke Muehlhauser joins Anthropic board | OP representation on safety-focused AI lab |
| **2021** | Joseph Carlsmith power-seeking AI report | Published analysis of existential risk from AI |
| **2022** | Cotra updates AI timelines | Shortened median from 2050 to 2040 following AI progress |
| **2022** | Post-FTX role as "funder of last resort" | Provided stability after FTX Future Fund collapse |
| **2024** | Muehlhauser resigns Anthropic board | To focus on Open Philanthropy governance work |
| **2025** | \$40M Technical AI Safety RFP launched | January; 21 research areas; largest dedicated safety RFP |
| **2025** | Holden Karnofsky joins Anthropic | January; member of technical staff on responsible scaling |
| **2025** | Renamed Coefficient Giving | November; signals multi-donor expansion |

### The GiveWell Split

The relationship between GiveWell and Open Philanthropy reflects an evolution in effective altruism philanthropy. GiveWell focuses on evidence-backed global health interventions with demonstrated cost-effectiveness, while Open Philanthropy pursues higher-risk, higher-reward opportunities including speculative causes like AI safety where evidence is necessarily limited.

The 2017 separation allowed each organization to optimize for its respective approach: GiveWell for rigorous evidence standards, Open Philanthropy for hits-based giving and worldview diversification. The organizations maintain separate financial resources, staff, and governing bodies, though they share intellectual heritage and some donors.

### Primary Funders: Moskovitz and Tuna

Dustin Moskovitz co-founded Facebook with Mark Zuckerberg in 2004 and later founded Asana, a workplace productivity company. His net worth exceeds \$12 billion. Cari Tuna worked as a Wall Street Journal reporter covering enterprise technology and California's economy before turning to full-time philanthropy. The couple met in 2009 and became the youngest individuals to sign Warren Buffett's Giving Pledge in 2010.

Since then, they have donated more than \$4 billion total, including over \$600 million in 2025 alone. Tuna serves as President of Good Ventures and has been the operational driver of their philanthropic vision, turning Moskovitz's early philanthropic mindset into systematic grantmaking infrastructure.

## AI Safety Portfolio

### Documented Grants to Major Organizations

Based on the [Open Philanthropy grants database](https://www.openphilanthropy.org/grants/), here are confirmed grants to key AI safety organizations:

| Organization | Grant(s) | Amount | Year | Focus |
|--------------|----------|--------|------|-------|
| **MIRI** | General Support | \$7,703,750 | 2017 | Alignment research |
| **MIRI** | General Support | \$3,750,000 (3 years) | Multiple | Alignment research |
| **MIRI** | General Support | \$2,652,500 (2 years) | Multiple | Alignment research |
| **MIRI** | AI Safety Retraining | \$150,000 | 2020s | Training program |
| **Redwood Research** | General Support | \$10,700,000 (18 months) | 2021 | Interpretability, alignment |
| **Redwood Research** | General Support | ~\$10,000,000 | 2023 | Continued support |
| **MATS Research** | General Support | \$23,648,000 (2 years) | 2024 | AI safety training |
| **MATS Research** | Winter Cohort | \$660,000 | 2024-25 | Research mentorship |
| **Center for Long-Term Cybersecurity** | Risk Frameworks | \$200,000 | 2024 | AI risk management |

**Note**: Grant amounts vary significantly year-to-year because OP often makes large multi-year grants. The average annual increase in AI safety funding has been approximately \$13 million.

### Cumulative Funding Estimates by Organization

| Organization | Estimated Total | Confidence | Notes |
|--------------|----------------|------------|-------|
| **MIRI** | \$14-20M+ | High | Multiple documented grants |
| **Redwood Research** | ~\$20M | High | Two major grants confirmed |
| **MATS Research** | \$24M+ | High | Recent large grants |
| **University Programs** | \$30M+ | Medium | Distributed across institutions |
| **Center for AI Safety** | \$10M+ | Medium | Multiple grants |
| **GovAI** | \$10M+ | Medium | Core support over years |
| **80,000 Hours** | \$10M+ | Medium | AI safety career guidance |
| **METR (formerly ARC Evals)** | \$8M+ | Medium | Capability evaluations |
| **Epoch AI** | \$5M+ | Medium | AI forecasting research |

### Annual AI Safety Grantmaking Estimates

| Year | Approximate Amount | Notable Developments |
|------|-------------------|---------------------|
| 2015-2018 | ~\$30M total | Early field-building; first MIRI grants |
| 2019 | ~\$40M | EA community spent ~\$40M globally on AI safety |
| 2020 | ~\$50M | Bio Anchors report; increased priority |
| 2021 | ~\$75M | Anthropic board seat; Redwood founding |
| 2022 | ~\$80M | Power-seeking AI report; Cotra timeline update |
| 2023 | ~\$46M | Post-FTX "funder of last resort" role; median grant \$257K, average \$1.67M |
| 2024 | ~\$50M+ | Technical AI Safety team buildout |
| 2025 | \$40M+ RFP | Largest dedicated safety RFP launched |

**Cumulative AI Safety Total**: Approximately \$336 million (~12% of \$2.8 billion total through 2023)

### Grant Size Distribution

| Metric | Value | Context |
|--------|-------|---------|
| **Median AI Safety Grant** | ~\$257,000 | Typical individual grant |
| **Average AI Safety Grant** | ~\$1.67 million | Skewed by large multi-year grants |
| **Largest Grants** | \$10-25M | Multi-year organizational support |
| **Smallest Grants** | \$50-200K | Individual researchers, small projects |

### 2025 Technical AI Safety RFP

In January 2025, Open Philanthropy launched a major [Request for Proposals](https://www.openphilanthropy.org/request-for-proposals-technical-ai-safety-research/) offering at least \$40 million for technical AI safety research, with willingness to spend substantially more depending on application quality.

| RFP Details | Information |
|-------------|-------------|
| **Total Funding** | \$40M minimum; potentially much more |
| **Grant Range** | API credits for experiments → seed funding for new orgs |
| **Duration** | Project-dependent |
| **Research Areas** | [21 specific areas](https://www.openphilanthropy.org/tais-rfp-research-areas/) defined |
| **Application Deadline** | April 15, 2025 (EOIs); July 15, 2025 (revise/resubmit) |
| **Application Process** | 300-word Expression of Interest → 2-week review → full proposal invitation |
| **Team Lead** | Peter Favaloro |

### RFP Research Areas (21 Topics)

The [detailed research area guide](https://www.openphilanthropy.org/tais-rfp-research-areas/) organizes work into categories:

| Category | Example Research Areas |
|----------|----------------------|
| **Reward/Feedback Learning** | Reward hacking of human oversight; scalable oversight failures |
| **Control Methods** | Control evaluations (using models to catch misbehavior); AI containment |
| **Alignment Foundations** | Making models never try to violate rules; goal specification |
| **Interpretability** | Understanding model internals; detecting deception |
| **Evaluation Methods** | Dangerous capability testing; safety benchmarks |
| **Governance Technical** | Compute governance; model auditing methods |

The RFP emphasizes:
- Research on LLMs trained to perform well according to human feedback where humans may make mistakes
- Using other models to catch when LLMs misbehave (control evaluations)
- Approaches that make models never try to violate rules in the first place

### Application Encouragement

OP explicitly encourages applications from researchers new to safety:
> "Whether you're an expert on one of these research topics or you've barely thought about them, we encourage you to apply. Over the last few years, many researchers have switched into safety research and produced impactful work."

The RFP is partly an experiment to inform OP about funding demand, so they suggest a low bar for EOI submission—even if applicants aren't certain they'd accept a grant if offered.

### Other Active RFPs

| RFP | Focus | Status |
|-----|-------|--------|
| **AI Governance RFP** | Policy, international coordination | Ongoing |
| **Capability Evaluations RFP** | Improving eval methods | Ongoing |
| **Technical AI Safety RFP** | 21 research areas | Closed April 2025 |

## Anthropic Relationship

Open Philanthropy's relationship with Anthropic represents one of its most significant—and debated—commitments to AI safety.

### Investment and Governance History

| Date | Event | Details |
|------|-------|---------|
| **May 2021** | Anthropic Series A | \$124M round led by Dustin Moskovitz and Jaan Tallinn; \$550M valuation |
| **July 2021** | Additional Series A | \$580M raised |
| **January 2021** | Luke Muehlhauser joins Anthropic board | OP representation; represented early shareholders |
| **April 2022** | Series B | \$580M led by Sam Bankman-Fried/FTX; \$2B valuation |
| **2021** | Long-Term Benefit Trust established | Controls majority of board seats via special stock class |
| **May 2024** | Muehlhauser resigns from Anthropic board | To focus on Open Philanthropy work |
| **January 2025** | Holden Karnofsky joins Anthropic | Member of technical staff; responsible scaling policies |

### Context: Anthropic's Broader Funding

Anthropic has raised substantially more than OP's initial investment:

| Round | Date | Amount | Valuation | Lead Investors |
|-------|------|--------|-----------|---------------|
| Series A | May 2021 | \$124M | \$550M | Moskovitz, Tallinn |
| Series A (2) | July 2021 | \$580M | — | Various |
| Series B | April 2022 | \$580M | \$2B | FTX (Sam Bankman-Fried) |
| Series C | May 2023 | \$450M | \$4.1B | Spark Capital |
| Series D | Feb 2024 | \$750M | \$18B | Google, others |
| Series E | Mar 2025 | \$3.5B | \$61B | Various |
| Series F | Sep 2025 | \$13B | \$183B | Various |
| Latest | Nov 2025+ | \$10B+ | \$350B | Microsoft, Nvidia, others |

OP's direct equity investment represented early, safety-focused capital; subsequent rounds were dominated by major tech companies (Google, Amazon) and later Microsoft and Nvidia.

### Long-Term Benefit Trust

A key governance mechanism established in 2021, the Long-Term Benefit Trust controls the majority of Anthropic's board seats through a special class of non-transferable, non-dividend-yielding stock. The trust aims to ensure the company remains committed to beneficial AI development over the long term, independent of shareholder pressure for profit maximization.

### Board Representation

Luke Muehlhauser served on Anthropic's board from January 2021 to May 2024, representing early shareholders aligned with safety priorities. He resigned to focus on his governance work at Open Philanthropy—notably, not for safety-related concerns about Anthropic. In his [resignation explanation](https://lukemuehlhauser.com/why-i-resigned-from-the-anthropic-board/), he clarified his departure was not due to disagreements about Anthropic's direction.

### The Controversy

The Anthropic relationship generates significant debate within EA and longtermist communities:

| Position | Argument |
|----------|----------|
| **Supportive** | Safety-focused competitor to OpenAI is valuable; Anthropic publishes more safety research; RLHF Constitutional AI advances field |
| **Critical** | Investment accelerates overall AI capability development; billions in subsequent funding dwarfs safety-motivated early investment |
| **Nuanced** | Supporting safety-conscious labs may be net positive even with capability acceleration; counterfactual unclear |

Holden Karnofsky has written extensively about this tension, arguing that the field needs safety-conscious labs at the frontier even if their existence contributes to capability acceleration. The question of whether early safety-motivated investment "created" the company or merely participated in inevitable capital flows remains contested.

### Personal Connections

The OP-Anthropic relationship involves notable personal ties:
- **Holden Karnofsky** (OP co-founder) is married to **Daniela Amodei** (Anthropic President and co-founder)
- Karnofsky sat on OpenAI's board (2017-2021) before resigning to avoid conflict when Daniela helped found Anthropic
- In January 2025, Karnofsky joined Anthropic as technical staff working on responsible scaling policies

## Key People

### Leadership

| Person | Role | Background |
|--------|------|------------|
| **Alexander Berger** | Co-CEO | Former GiveWell research; global health expertise |
| **Holden Karnofsky** | Co-CEO Emeritus (until 2024) | GiveWell co-founder; now at Anthropic |
| **Cari Tuna** | President, Good Ventures | Former WSJ reporter; leads foundation operations |
| **Dustin Moskovitz** | Funder | Facebook co-founder; Asana founder; \$12B+ net worth |

### AI Safety Team Leadership

| Person | Role | Notable Work | Background |
|--------|------|--------------|------------|
| **Luke Muehlhauser** | Program Director, AI Governance & Policy | Leads \$100M+/year AI governance grantmaking | Former MIRI Executive Director (2011-2015); Anthropic board (2021-2024) |
| **Peter Favaloro** | Technical AI Safety Team Lead | 2025 \$40M RFP; technical research funding | Built out technical safety grantmaking team |
| **Ajeya Cotra** | Senior Program Officer | Bio Anchors report; AI timelines; technical safety grants | UC Berkeley EECS; founded EA Berkeley; joined 2016 |
| **Joseph Carlsmith** | Former Senior Advisor | Power-seeking AI risk analysis; consciousness research | Oxford PhD philosophy; now at Anthropic |

### Holden Karnofsky

Holden Karnofsky co-founded GiveWell in 2007 and led the creation of Open Philanthropy. His career trajectory reflects the evolution of effective altruism from evidence-based charity evaluation toward longtermist concerns:

| Period | Role | Focus |
|--------|------|-------|
| 2007-2017 | GiveWell Co-Executive Director | Evidence-based charity evaluation |
| 2014-2024 | Open Philanthropy Co-CEO | Cause prioritization; worldview diversification |
| 2017-2021 | OpenAI Board Member | Safety oversight; \$30M grant relationship |
| 2021-2024 | Co-CEO Emeritus | AI strategy; transition away from operations |
| 2024 | Left Open Philanthropy | Taking leave to focus on AI safety |
| 2025 | Joined Anthropic | Technical staff; responsible scaling policies |

Karnofsky's writing on [Cold Takes](https://www.cold-takes.com/) has been influential in AI safety discourse, including summaries of the Bio Anchors framework and arguments for hits-based giving. His resignation from OpenAI's board in 2021 was due to conflict of interest—his wife Daniela Amodei was co-founding Anthropic.

### Luke Muehlhauser

Luke Muehlhauser leads Open Philanthropy's AI Governance and Policy grantmaking with a target of giving away more than \$100 million per year:

| Period | Role | Focus |
|--------|------|-------|
| Pre-2011 | Blogger | Common Sense Atheism; skeptic community |
| 2011-2015 | MIRI Executive Director | Technical alignment research leadership |
| 2015-present | Open Philanthropy | Research Analyst → Senior Analyst → Program Director |
| 2021-2024 | Anthropic Board Member | Represented early shareholders; safety oversight |

Muehlhauser's path—from evangelical Christianity to skepticism to the LessWrong community to AI safety—exemplifies a common trajectory in the field. His investigations at OP have spanned social sciences, global catastrophic risks, animal consciousness, and forecasting.

### Ajeya Cotra

Ajeya Cotra's [Bio Anchors report](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/) is one of the most influential documents in AI safety. She developed the framework mainly between May 2019 and July 2020, posting it on LessWrong in September 2020.

| Attribute | Details |
|-----------|---------|
| **Education** | B.S. Electrical Engineering and Computer Science, UC Berkeley (2013-2016) |
| **EA Involvement** | Founded Effective Altruists of Berkeley; taught EA course at Berkeley |
| **Joined OP** | July 2016 as Research Analyst |
| **Current Role** | Senior Program Officer; previously led technical AI safety program |
| **Key Output** | Bio Anchors AI timelines report (2020); 2022 update |

**Bio Anchors Findings (2020):**
- 10% chance of transformative AI by 2031
- 50% chance by 2052
- ~80% chance by 2100

**2022 Update:**
- 15% by 2030
- 35% by 2036
- **50% by 2040** (shortened from 2052)
- 60% by 2050

The 2022 update shortened her median timeline by a full decade, reflecting AI progress (including GPT-3/GPT-4) and a shift from outside-view to inside-view models.

### Joseph Carlsmith

Joseph (Joe) Carlsmith was a senior advisor at Open Philanthropy focusing on existential risk from advanced AI before joining Anthropic to work on Claude's character and constitution:

| Attribute | Details |
|-----------|---------|
| **Education** | BA Philosophy, Yale; BPhil and DPhil Philosophy, Oxford |
| **Academic Work** | Helped Toby Ord write *The Precipice* (2017-18) |
| **OP Role** | Senior Advisor; AI existential risk analysis |
| **Current Role** | Anthropic; Claude character/constitution/spec |
| **Other Interests** | Meditation (1+ year on silent retreat); philosophical writing |

**Key Publication: ["Is Power-Seeking AI an Existential Risk?"](https://arxiv.org/abs/2206.13353)** (April 2021, arXiv June 2022)

This Open Philanthropy report examines the core argument for existential risk from misaligned AI, building the case around AI systems with:
- Advanced capabilities (outperforming humans on research, strategy, persuasion)
- Planning capabilities (making and executing plans toward objectives)
- Strategic awareness (understanding effects of gaining/maintaining power)

**Bottom line estimate**: >10% chance that misaligned AI leading to human disempowerment occurs by 2070.

### Nick Beckstead (Former)

Nick Beckstead led Open Philanthropy's longtermist grantmaking before departing for Anthropic. His role established the intellectual framework for OP's approach to existential risk funding.

## Grantmaking Philosophy

### Hits-Based Giving

Open Philanthropy explicitly embraces "hits-based giving"—a high-risk, high-reward approach borrowed from venture capital investing. The philosophy accepts that most grants may fail to have impact, betting that a few transformative successes will more than compensate.

From their [Hits-Based Giving](https://coefficientgiving.org/research/hits-based-giving/) documentation:

> "In many cases, we'll put substantial resources into a cause even though we think it's more likely than not that we'll fail to have any impact. (Potential risks from artificial intelligence is one such cause.)"

| Principle | Description |
|-----------|-------------|
| **High Risk Tolerance** | Willing to fund uncertain, speculative projects |
| **Expected Value Focus** | No inherent preference for low-risk vs. high-risk; maximizes portfolio EV |
| **Long Time Horizons** | Multi-decade outcome expectations; patient capital |
| **Unconventional Bets** | Fund work traditional philanthropy avoids |
| **Learning from History** | Case studies show philanthropic "hits" came from unconventional bets |

**Why Hits-Based Giving Works (According to OP):**
1. **Historical evidence**: Philanthropic case studies show transformative change often came from unconventional bets (Green Revolution, oral contraceptive development)
2. **Philanthropist comparative advantage**: Unlike governments (voter accountability) or businesses (shareholder pressure), philanthropists can support unproven ideas, back decades-long work, and pursue unpopular causes
3. **Portfolio dynamics**: If individual grants have positive expected value, funding many high-variance projects is optimal even if most fail

**Farm Animal Welfare Example**: When OP began funding in 2016, most animal philanthropy went to pet shelters. They backed The Humane League and Mercy for Animals on coordinated corporate campaigns. Within years, this won cage-free commitments from 2,700+ companies, improving 250+ million birds' lives annually.

### Worldview Diversification

Rather than betting everything on a single theory of impact, OP practices [worldview diversification](https://coefficientgiving.org/research/worldview-diversification/)—allocating resources across multiple plausible philosophical frameworks.

**The Four Worldviews** (as of 2025):

| Worldview | Focus | Theory of Change | Typical Causes |
|-----------|-------|------------------|----------------|
| **Global Health & Wellbeing** | Near-term human welfare | Evidence-based interventions | Malaria, deworming, GiveDirectly |
| **Farm Animal Welfare** | Animal suffering | Corporate campaigns, alternative proteins | Cage-free eggs, cultivated meat |
| **Biosecurity & Pandemic Preparedness** | Catastrophic biological risk | Policy, research, capacity building | Pandemic prevention, biodefense |
| **Potential Risks from Advanced AI** | Existential risk | Technical research, governance, talent | Alignment, evals, AI policy |

**Rationale for Diversification:**
- Moral philosophy offers no neutral way to decide between competing values
- Worldviews differ on whose welfare counts, over what timescales, and by what measure
- Betting everything on "best guess" worldview risks complete failure if wrong
- Diversification enables learning from track record across approaches

### Strategic Cause Selection

OP chooses focus areas through systematic cause prioritization using three criteria:

| Criterion | Definition | AI Safety Assessment |
|-----------|------------|---------------------|
| **Importance** | How many affected, how deeply? | Potentially all humans; existential |
| **Neglectedness** | Adequate existing attention/resources? | ~\$100M/year philanthropic vs. \$24B AI investment |
| **Tractability** | Can additional funding drive progress? | Growing evidence of useful research; talent-constrained |

### Grant Decision Process

| Stage | Description | Timeline |
|-------|-------------|----------|
| **Sourcing** | Staff identify opportunities; organizations apply; RFPs | Ongoing |
| **Investigation** | Deep research into organization/project | Weeks to months |
| **Internal Review** | Team discussion; devil's advocacy; debate | Variable |
| **Final Decision** | Senior staff approval | Days to weeks |
| **Monitoring** | Ongoing relationship; progress updates | Grant duration |

## Cause Area Breakdown

<Mermaid client:load chart={`
flowchart TD
    subgraph GV["Good Ventures / Coefficient Giving"]
        OP[Open Philanthropy]
    end

    subgraph Worldviews["Four Worldviews"]
        AI["AI Safety<br/>~12% historical<br/>~336M total"]
        BIO["Biosecurity<br/>Pandemic Preparedness"]
        FAW["Farm Animal Welfare<br/>Corporate campaigns"]
        GH["Global Health<br/>GiveWell-style"]
    end

    OP --> AI
    OP --> BIO
    OP --> FAW
    OP --> GH

    subgraph AI_Recipients["AI Safety Recipients"]
        ORGS[Research Orgs<br/>MIRI, Redwood, MATS]
        GOV[Governance<br/>GovAI, policy orgs]
        TALENT[Talent<br/>80K Hours, university programs]
    end

    AI --> ORGS
    AI --> GOV
    AI --> TALENT

    style GV fill:#e6f3ff
    style AI fill:#ffcccc
    style BIO fill:#ccffcc
    style FAW fill:#ffffcc
    style GH fill:#ccccff
`} />

### Longtermist Focus Areas

| Area | Annual Funding (Est.) | Key Activities | Sample Grantees |
|------|----------------------|----------------|-----------------|
| **AI Safety** | \$40-100M | Technical research, evals, governance | MIRI, Redwood, MATS, GovAI |
| **Biosecurity** | \$30-50M | Pandemic prevention, governance | Johns Hopkins CHS, Georgetown CSET |
| **Other X-Risk** | \$10-20M | Nuclear, space governance | Various |

### Near-term Focus Areas

| Area | Annual Funding (Est.) | Key Activities | Sample Grantees |
|------|----------------------|----------------|-----------------|
| **Global Health** | \$100M+ | GiveWell top charities, research | AMF, GiveDirectly, Malaria Consortium |
| **Farm Animal Welfare** | \$50M+ | Corporate campaigns, alternatives | Humane League, GFI, Mercy for Animals |
| **Criminal Justice** | \$30M+ | Reform advocacy | Various |

### Recent Funding Decisions

OP has recently stopped funding several categories:
- Many Republican-leaning think tanks
- "Post-alignment" causes including digital sentience
- Parts of the rationality community (LessWrong, Lightcone, SPARC, CFAR, MIRI in some categories)


## Research and Publications

Open Philanthropy produces foundational research that shapes AI safety field understanding, going beyond typical funder activities to serve as intellectual leader.

### Influential Reports

| Report | Author(s) | Year | Key Findings | Impact |
|--------|-----------|------|--------------|--------|
| **[Bio Anchors: Forecasting TAI](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/)** | Ajeya Cotra | 2020, updated 2022 | 50% probability of TAI by 2040 (updated from 2052) | Foundational AI timelines framework; informs OP/EA resource allocation |
| **[Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353)** | Joseph Carlsmith | 2021 | >10% chance of AI-caused human disempowerment by 2070 | Rigorous existential risk probability estimate |
| **AI Could Defeat All of Us** | Holden Karnofsky | 2022 | Accessible risk communication | Widely shared introduction to AI risk |
| **[Key Writings on AI Development](https://www.openphilanthropy.org/research/key-writings-on-ai-development-from-open-philanthropy-staff/)** | Various | Ongoing | Curated research hub | Central reference for OP AI thinking |
| **Worldview Diversification** | Various | 2017+ | Framework for multi-worldview allocation | Influenced EA philanthropic practice |
| **Consciousness Reports** | Joseph Carlsmith | 2020s | AI moral status analysis | Shapes thinking on digital minds |

### Bio Anchors Methodology

The Bio Anchors report attempts to answer: "When might AI systems automate the hardest and most impactful tasks humans do?" The methodology:

1. **Biological comparison**: Estimates compute needed for "human-level" training based on brain computation estimates
2. **Multiple anchors**: Uses several comparison points (evolution, brain, lifetime learning)
3. **Three drivers**: Models how algorithmic progress, compute costs, and available funding affect timelines
4. **Probabilistic output**: Produces probability distributions, not point estimates

**Original 2020 estimates**: 10% by 2031, 50% by 2052, ~80% by 2100

**2022 update**: 15% by 2030, 35% by 2036, **50% by 2040**, 60% by 2050—a full decade shortening of the median

The update reflected:
- AI progress exceeding expectations (GPT-3, GPT-4)
- Shift from outside-view to inside-view models
- Probability concentrated in narrower band (26 vs. 32 years for 10%→60%)

### Power-Seeking AI Analysis

Carlsmith's analysis provides the most rigorous publicly-available estimate of AI existential risk probability. The argument structure:

| Premise | Claim | Carlsmith's Assessment |
|---------|-------|----------------------|
| 1 | Advanced AI systems will be developed | Very likely by 2070 |
| 2 | Some will have planning capabilities | Likely given capabilities needed |
| 3 | Some will be given high-stakes tasks | Likely given economic incentives |
| 4 | Some will have strategic awareness | Plausible given planning capabilities |
| 5 | Some will develop power-seeking goals | Uncertain; depends on training |
| 6 | Some will pursue goals misaligned with humans | Uncertain; alignment difficulty |
| **Conclusion** | >10% x-risk from AI by 2070 | |

### Public Communication Channels

| Channel | Content | Frequency |
|---------|---------|-----------|
| **Blog** | Grant writeups, strategy updates, cause reports | Regular |
| **Cold Takes** | Holden Karnofsky's personal analysis (now inactive) | Archived |
| **Annual Updates** | [Progress reports and future plans](https://www.openphilanthropy.org/research/our-progress-in-2024-and-plans-for-2025/) | Yearly |
| **Grants Database** | Complete funding transparency | Continuous |
| **Podcast Appearances** | Staff on 80K Hours, other EA podcasts | Periodic |
| **EA Forum** | Engagement with community discussion | Ongoing |

## Comparison with Other AI Safety Funders

An estimated 80% of philanthropic AI safety funding flows through Open Philanthropy and the Survival and Flourishing Fund (SFF). Understanding their differences helps applicants and observers navigate the funding landscape.

### Funder Comparison

| Attribute | Open Philanthropy | Survival and Flourishing Fund | LTFF |
|-----------|------------------|------------------------------|------|
| **Primary Backer** | Dustin Moskovitz (\$12B+) | Jaan Tallinn (Skype co-founder) | EA community donors |
| **Total AI Safety Spend** | ~\$336M since 2017 | ~\$53M since 2019 | Smaller; individual-focused |
| **2023 AI Safety** | ~\$46M | ~\$30M | \$5-10M |
| **Median Grant Size** | ~\$257K | Variable | \$50-200K |
| **Average Grant Size** | ~\$1.67M | Smaller than OP | Smaller |
| **Decision Process** | Staff investigation → senior approval | S-process algorithm + committee | Grant committee |
| **Grant Style** | Multi-year organizational support | Shorter-term; more speculative | Individual researchers |
| **Speed** | Slower (months) | Twice yearly rounds | Rolling |
| **Focus** | Established orgs; large projects | New bets; smaller orgs | Individuals; early-career |

### Complementary Roles

| Niche | Funder Best Suited | Rationale |
|-------|-------------------|-----------|
| **Large organizational grants (\$1M+)** | Open Philanthropy | Scale, multi-year commitment capacity |
| **Small new organizations (\<\$250K)** | SFF, LTFF | OP capacity-constrained; may miss small orgs |
| **Individual researchers** | LTFF | Individual grants are LTFF specialty |
| **Speed-sensitive grants** | SFF, LTFF | OP process takes months |
| **Speculative/weird ideas** | SFF | Higher risk tolerance; S-process surfaces unusual bets |
| **University programs** | Open Philanthropy | Track record with academic institutions |
| **Policy/governance** | Open Philanthropy | Luke Muehlhauser's team; \$100M+/year target |

### Coordination and Tensions

| Dynamic | Description |
|---------|-------------|
| **Funding overlap** | Both fund CAIS, AI Impacts, some other orgs |
| **Adjustment tensions** | Reports of OP reducing grants when SFF also commits |
| **Field stability** | OP served as "funder of last resort" post-FTX collapse |
| **Blind spot correction** | Smaller funders can correct OP's worldview limitations |

### Funding Landscape Context

| Metric | Value | Implication |
|--------|-------|-------------|
| **Philanthropic AI safety (2023)** | ~\$100M | Dominated by OP + SFF |
| **Generative AI investment (2023)** | ~\$24B | 240x more than safety philanthropy |
| **Climate risk philanthropy (2023)** | \$9-15B | ~20x AI safety funding |
| **OP + Good Ventures share** | >50% | Concentrated funder dependency |

The concentration of AI safety funding in two organizations creates systemic risk—if either changes priorities, significant field disruption follows. Smaller donors can play valuable "angel investor" roles identifying promising opportunities OP may miss due to capacity constraints.

## Strengths and Limitations

### Organizational Strengths

| Strength | Evidence | Implication |
|----------|----------|-------------|
| **Scale** | \$4B+ total; \$336M+ to AI safety | Can fund large, multi-year organizational support |
| **Expertise** | Deep research team (Cotra, Muehlhauser, formerly Carlsmith) | Funding decisions informed by substantive analysis |
| **Patience** | Multi-year grants; decade-long time horizons | Can support speculative research without near-term pressure |
| **Transparency** | [Public grants database](https://www.openphilanthropy.org/grants/); detailed writeups | Field can learn from and critique decisions |
| **Field Leadership** | Sets research priorities through funding; produces foundational research | Intellectual influence beyond dollar value |
| **Stability** | Good Ventures balance sheet; served as "funder of last resort" post-FTX | Provides field continuity during funding shocks |
| **Hits-Based Philosophy** | Explicit tolerance for grant "failure" | Can fund high-variance opportunities |

### Organizational Limitations

| Limitation | Description | Mitigation |
|------------|-------------|------------|
| **Concentration Risk** | Single donor family (Moskovitz/Tuna) as primary source | Coefficient Giving rebrand signals multi-donor expansion |
| **Slow Process** | Large grants take months to approve | Small/urgent needs better served by SFF/LTFF |
| **Capacity Constraints** | ~100 staff can't evaluate all opportunities | May miss small, new organizations (\<\$250K) |
| **Worldview Influence** | Field may over-update on OP views; OP priorities become field priorities | Worldview diversification partially addresses |
| **Recent Funding Cuts** | Stopped funding some rationality orgs, Republican think tanks | Creates gaps other funders may not fill |
| **Anthropic Relationship** | Personal ties (Karnofsky-Amodei marriage) create perception issues | Disclosure and recusal policies in place |

### Strategic Considerations

| Issue | Assessment |
|-------|------------|
| **Funder dependency** | ~80% of philanthropic AI safety funding from OP + SFF; systemic risk |
| **Capability vs. safety** | Anthropic investment may accelerate capabilities; controversial |
| **Research vs. operations** | OP produces research that influences its own grantmaking; potential echo chamber |
| **Expected value** | Aims to equalize marginal returns across interventions; portfolio approach |

## Application Process

### How to Apply for Funding

| Path | Description | Best For |
|------|-------------|----------|
| **Unsolicited applications** | [General application](https://www.openphilanthropy.org/how-to-apply-for-funding/) via website | Established organizations with clear proposal |
| **RFPs** | Respond to specific Requests for Proposals | Researchers whose work fits RFP areas |
| **Staff outreach** | OP staff proactively identify opportunities | Known entities; referrals |
| **EA network** | Connections through EA Forum, conferences | Community-embedded applicants |

### RFP Process (Technical AI Safety Example)

1. **Read research areas**: Review [21 topic descriptions](https://www.openphilanthropy.org/tais-rfp-research-areas/)
2. **Submit Expression of Interest**: 300 words describing proposed work
3. **Review period**: OP aims to respond within 2 weeks
4. **Full proposal**: If invited, submit detailed proposal
5. **Investigation**: OP staff conduct due diligence
6. **Decision**: Senior staff approval/rejection
7. **Revise and resubmit**: Option for borderline applications

### What OP Looks For

| Criterion | Description |
|-----------|-------------|
| **Fit with priorities** | Alignment with worldview and cause area priorities |
| **Team quality** | Track record, relevant expertise, execution ability |
| **Theory of change** | Clear path from funding to impact |
| **Counterfactual value** | Would this happen without OP funding? |
| **Expected value** | Probability-weighted impact assessment |
| **Room for more funding** | Can organization productively absorb grant? |

### Tips for Applicants

- OP explicitly encourages EOI submissions even if uncertain about accepting a grant
- Researchers new to safety are welcome; many have successfully switched into the field
- Small grants (API credits, rapid experiments) are possible alongside large organizational funding
- If proposal doesn't fit Technical AI Safety RFP, consider AI Governance RFP or Capability Evaluations RFP

## Future Outlook

### 2025 Priorities

From OP's [2024 Progress and 2025 Plans](https://www.openphilanthropy.org/research/our-progress-in-2024-and-plans-for-2025/):

| Priority | Details |
|----------|---------|
| **Technical AI Safety** | Expects returns to grow as frontier AI enables empirical testing of risk predictions |
| **Team Buildout** | Peter Favaloro's technical safety team expanded; \$40M+ RFP |
| **AI Governance** | Luke Muehlhauser's team targeting \$100M+/year |
| **Multi-Donor Model** | Coefficient Giving rebrand reflects expansion beyond Good Ventures |
| **Ten-Year Reflection** | 2024 marked 10 years; refining approach based on lessons learned |

### Key Uncertainties

| Question | Implications |
|----------|--------------|
| **Anthropic trajectory** | Does safety-focused lab maintain safety focus as it scales? |
| **Funding landscape** | Will additional philanthropists enter AI safety? |
| **Technical progress** | Will safety research keep pace with capability progress? |
| **Worldview updates** | Could OP significantly shift AI safety priority up or down? |
| **Governance influence** | Will OP-funded policy research translate to actual regulation? |

## Sources and Citations

### Primary Sources
- [Coefficient Giving (formerly Open Philanthropy) Website](https://coefficientgiving.org/)
- [Open Philanthropy Grants Database](https://www.openphilanthropy.org/grants/)
- [Our Progress in 2024 and Plans for 2025](https://www.openphilanthropy.org/research/our-progress-in-2024-and-plans-for-2025/)
- [Worldview Diversification](https://coefficientgiving.org/research/worldview-diversification/)
- [Hits-Based Giving](https://coefficientgiving.org/research/hits-based-giving/)
- [Technical AI Safety RFP](https://www.openphilanthropy.org/request-for-proposals-technical-ai-safety-research/)
- [RFP Research Areas Guide](https://www.openphilanthropy.org/tais-rfp-research-areas/)
- [Coefficient Giving History](https://coefficientgiving.org/our-history/)

### Staff Research
- [Key Writings on AI Development from OP Staff](https://www.openphilanthropy.org/research/key-writings-on-ai-development-from-open-philanthropy-staff/)
- [Bio Anchors Summary](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/) (Holden Karnofsky / Cold Takes)
- [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353) (Joseph Carlsmith, arXiv)
- [Two-Year Update on Personal AI Timelines](https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines) (Ajeya Cotra)
- [Why I Resigned from the Anthropic Board](https://lukemuehlhauser.com/why-i-resigned-from-the-anthropic-board/) (Luke Muehlhauser)
- [Ajeya Cotra Profile](https://www.openphilanthropy.org/about/team/ajeya-cotra/)
- [Luke Muehlhauser Profile](https://www.openphilanthropy.org/about/team/luke-muehlhauser/)

### Funding Landscape Analysis
- [An Overview of the AI Safety Funding Situation](https://forum.effectivealtruism.org/posts/XdhwXppfqrpPL2YDX/an-overview-of-the-ai-safety-funding-situation) (EA Forum)
- [AI Safety and Security Need More Funders](https://coefficientgiving.org/research/ai-safety-and-security-need-more-funders/)
- [It Looks Like There Are Good Funding Opportunities in AI Safety Right Now](https://80000hours.org/2025/01/it-looks-like-there-are-some-good-funding-opportunities-in-ai-safety-right-now/) (80,000 Hours)

### Grant-Specific Sources
- [MIRI General Support 2017](https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2017/)
- [Redwood Research General Support 2021](https://www.openphilanthropy.org/grants/redwood-research-general-support/)
- [Redwood Research General Support 2023](https://www.openphilanthropy.org/grants/redwood-research-general-support-2023/)
- [MATS Research AI Safety Research Expenses](https://www.openphilanthropy.org/grants/mats-research-ai-safety-research-expenses/)

### Historical Context
- [The Relationship Between GiveWell and Coefficient Giving](https://www.givewell.org/about/gw-op-relationship)
- [Open Philanthropy Technical AI Safety RFP - \$40M Available](https://forum.effectivealtruism.org/posts/XtgDaunRKtCPzyCWg/open-philanthropy-technical-ai-safety-rfp-usd40m-available) (EA Forum)
- [Leaving Open Philanthropy, Going to Anthropic](https://forum.effectivealtruism.org/posts/EFF6wSRm9h7Xc6RMt/leaving-open-philanthropy-going-to-anthropic) (Joseph Carlsmith)
- [Taking a Leave of Absence from Open Philanthropy](https://forum.effectivealtruism.org/posts/aJwcgm2nqiZu6zq2S/taking-a-leave-of-absence-from-open-philanthropy-to-work-on) (Holden Karnofsky)

### News and External Coverage
- [Meet the Millennial Meta Cofounder and His Wife Who Are Giving Away \$20 Billion](https://fortune.com/2025/11/10/meet-the-millennial-meta-cofounder-and-his-wife-who-are-giving-away-20-billion/) (Fortune)
- [Cari Tuna: The 100 Most Influential People in AI 2024](https://time.com/collections/time100-ai-2024/7012763/cari-tuna/) (TIME)
- [Anthropic Hired President Daniela Amodei's Husband](https://fortune.com/2025/02/13/anthropic-hired-president-daniela-amodei-husband-ai-safety-responsible-scaling/) (Fortune)
- [Coefficient Giving Wikipedia](https://en.wikipedia.org/wiki/Coefficient_Giving)
- [Anthropic Wikipedia](https://en.wikipedia.org/wiki/Anthropic)

## External Links

- [Coefficient Giving Website](https://coefficientgiving.org/) (formerly Open Philanthropy)
- [Grants Database](https://www.openphilanthropy.org/grants/)
- [Blog / Research](https://www.openphilanthropy.org/research/)
- [How to Apply for Funding](https://www.openphilanthropy.org/how-to-apply-for-funding/)
- [Technical AI Safety RFP](https://www.openphilanthropy.org/request-for-proposals-technical-ai-safety-research/)
- [Good Ventures Foundation](https://www.goodventures.org/)
- [Cold Takes](https://www.cold-takes.com/) (Holden Karnofsky's blog, archived)

<Backlinks />
