---
title: OpenAI Foundation
description: Nonprofit organization holding 26% equity stake (~$130B) in OpenAI
  Group PBC, with governance control through board appointment rights and
  philanthropic commitments focused on health and AI resilience
importance: 85
lastEdited: "2026-02-03"
sidebar:
  order: 60
ratings:
  novelty: 8
  rigor: 9
  actionability: 6
  completeness: 9
quality: 80
llmSummary: The OpenAI Foundation is a nonprofit holding 26% equity (~$130B) in
  OpenAI Group PBC with governance control, created through 2025 restructuring
  to balance nonprofit mission with capital needs. The structure faces
  significant criticism for potential conflicts of interest and mission
  abandonment, with implications for AI safety funding given the foundation's
  unprecedented scale of resources.
metrics:
  wordCount: 3604
  citations: 47
  tables: 7
  diagrams: 0
---
import {EntityLink, Backlinks, KeyPeople, KeyQuestions, Section} from '@components/wiki';

## Quick Assessment

| Aspect | Description |
|--------|-------------|
| **Structure** | Nonprofit foundation holding 26% equity in for-profit OpenAI Group PBC |
| **Valuation** | \$130 billion stake at \$500B+ OpenAI valuation (October 2025) |
| **Governance Rights** | Appoints all board directors of OpenAI Group; can replace them at any time |
| **Initial Commitment** | \$25 billion focused on health/curing diseases and AI resilience |
| **Established** | October 28, 2025 (restructured from original 2015 nonprofit OpenAI, Inc.) |
| **Key People** | Bret Taylor (Board Chair), Sam Altman (CEO), nine other board members |
| **Comparison** | Similar to Anthropic's <EntityLink id="long-term-benefit-trust">Long-Term Benefit Trust</EntityLink> but with direct equity ownership vs. pledge-based control |

## Key Links

| Source | Link |
|--------|------|
| Official Website | [openai.com/foundation](https://openai.com/foundation/) |
| Structure Details | [openai.com/our-structure](https://openai.com/our-structure/) |
| Wikipedia | [en.wikipedia.org/wiki/OpenAI](https://en.wikipedia.org/wiki/OpenAI) |

## Overview

The **OpenAI Foundation** is a nonprofit organization that serves as the controlling entity over OpenAI Group PBC, the for-profit public benefit corporation developing advanced AI systems including ChatGPT and GPT-series models. Established through a major restructuring completed on October 28, 2025, the foundation holds a 26% equity stake in the for-profit entity valued at approximately \$130 billion, making it one of the world's most well-resourced philanthropic organizations.[^1][^2]

The foundation's structure represents an attempt to balance OpenAI's original nonprofit mission—"to ensure that artificial general intelligence benefits all of humanity"—with the capital requirements of developing cutting-edge AI systems. Through special voting and governance rights, the foundation appoints all members of OpenAI Group's board of directors and can remove them at any time, maintaining ultimate control despite holding a minority equity position compared to Microsoft's 27% stake.[^3][^4]

The foundation has committed \$25 billion initially across two priority areas: health and curing diseases (accelerating breakthroughs in diagnostics, treatments, and cures) and technical solutions for AI resilience (maximizing AI benefits and minimizing risks). In 2025, it launched the People-First AI Fund with an initial \$50 million commitment to support nonprofits and mission-focused organizations.[^1][^5]

## History and Restructuring

### Original Nonprofit Formation (2015-2019)

<EntityLink id="openai">OpenAI</EntityLink> was founded on December 11, 2015, as a nonprofit research organization in Delaware by <EntityLink id="sam-altman">Sam Altman</EntityLink>, <EntityLink id="elon-musk">Elon Musk</EntityLink>, Greg Brockman, <EntityLink id="ilya-sutskever">Ilya Sutskever</EntityLink>, and others. The founding team pledged over \$1 billion in total funding, with the explicit mission of advancing artificial general intelligence "unconstrained by a need to generate financial return."[^6][^7] The nonprofit structure was designed to avoid private gain and prioritize humanity-wide AGI benefits.

The idea emerged from private "founding dinners" in 2015 attended by key figures from Silicon Valley, amid concerns about AI risks and the accelerating pace of AI development following breakthroughs like AlexNet. Early donors included Peter Thiel, Reid Hoffman, Amazon Web Services, and Infosys.[^8]

### Capped-Profit Subsidiary (2019-2025)

By March 2019, OpenAI recognized that achieving its goals would require substantially more capital than initially anticipated—"on the order of \$10 billion" according to the organization's own projections.[^9] This led to the creation of OpenAI LP, a "capped-profit" for-profit subsidiary under nonprofit control, where investor returns were capped at 100 times their investment.

This structure allowed OpenAI to secure major investments, including an initial \$1 billion from Microsoft in 2019, while maintaining nominal nonprofit oversight. Microsoft secured broad rights to license and use OpenAI's intellectual property, except for technology related to artificial general intelligence (AGI).[^10][^11]

However, the hybrid structure proved increasingly difficult to maintain as OpenAI's valuation soared and the company required even more capital for infrastructure and computational resources. By 2024, OpenAI's reported valuation had reached approximately \$500 billion, making it effectively impossible for a for-profit entity to purchase the nonprofit arm's assets at fair market value, as would typically be required for a full conversion.[^12]

### Public Benefit Corporation Restructuring (2025)

On October 28, 2025, OpenAI completed a comprehensive restructuring that transformed the organization into two distinct entities:

1. **OpenAI Foundation** (nonprofit) - Holds 26% equity in the for-profit entity with special governance rights
2. **OpenAI Group PBC** (for-profit public benefit corporation) - Operates the business while maintaining mission alignment

The restructuring was approved by the California and Delaware Attorneys General after extensive negotiations. California AG Rob Bonta imposed 20 requirements for safety and security to remain under nonprofit control, while Delaware AG Kathy Jennings issued a non-objection statement contingent on fair valuation.[^13][^14]

Under the new structure, equity distribution is as follows:

- OpenAI Foundation: 26% (\$130 billion at October 2025 valuation)
- Microsoft: 27% (\$135 billion)
- Employees and early investors: 26%
- Recent investors (SoftBank, others): 15%
- Other investors: 6%[^15]

The foundation also secured a warrant allowing it to receive additional shares if OpenAI Group's valuation increases more than tenfold (to approximately \$5 trillion) after 15 years, positioning the foundation as "the single largest long-term beneficiary of OpenAI's success."[^16]

## Governance Structure

The OpenAI Foundation's governance power derives not from majority ownership but from **special voting and governance rights** that allow it to appoint all members of the OpenAI Group PBC board of directors and remove them at any time.[^17] This structure is intended to ensure that the for-profit entity remains aligned with the foundation's mission even as it pursues commercial objectives.

### Board Composition

The foundation is governed by a board of directors chaired by **Bret Taylor**, former co-CEO of Salesforce and co-creator of Google Maps. The board consists of nine members:[^18]

- **Bret Taylor** (Chair) - CEO of AI startup Sierra
- **Sam Altman** (CEO) - Co-founder of OpenAI
- **Adam D'Angelo** - Co-founder and CEO of Quora
- **Dr. Sue Desmond-Hellmann** - Former CEO of the Bill & Melinda Gates Foundation
- **Dr. Zico Kolter** - Computer scientist; chairs the Safety and Security Committee
- **Retired U.S. Army General Paul M. Nakasone** - Former Director of the NSA (2018-2024)
- **Adebayo Ogunlesi** - Managing partner at Global Infrastructure Partners
- **Nicole Seligman** - Attorney and former EVP of Sony Corporation
- **Larry Summers** - Economist and former Harvard president

All current foundation directors also serve on OpenAI Group's board, except Dr. Kolter, who serves as a non-voting observer.[^19] This dual-board structure has raised concerns about conflicts of interest, with critics arguing that board members may prioritize the for-profit entity's business interests over the foundation's public-serving mission.[^20]

### Safety and Security Oversight

The foundation established a Safety and Security Committee (SSC) chaired by Dr. Kolter. However, critics note that this committee consists of only four part-time volunteers with no dedicated staff, tasked with overseeing the development of potentially transformative AGI systems amid intense commercial pressures.[^21]

## Philanthropic Activities

### People-First AI Fund

In 2025, the OpenAI Foundation launched its first major philanthropic initiative: the People-First AI Fund with an initial \$50 million commitment. The fund issued an open call for applications from September 8 through October 8, 2025, receiving nearly 3,000 applications from U.S.-based 501(c)(3) nonprofits with annual operating budgets between \$500,000 and \$10 million.[^22][^23]

By December 2025, the foundation announced it would distribute:

- **\$40.5 million** in unrestricted grants to **208 nonprofits** across the United States
- **\$9.5 million** in board-directed grants for organizations advancing transformative AI work in health and other areas[^24]

The \$40.5 million initial disbursement represents a 440% increase over the foundation's previous year's grantmaking of \$7.5 million (when it was still operating as the original nonprofit OpenAI, Inc.).[^25]

Grant recipients span diverse sectors including AI literacy programs, rural healthcare initiatives, journalism organizations, youth development, veteran support, faith-based community groups, and Native-led media and STEM programs. Examples include:[^26]

- **Camai Community Health Center** (Alaska) - AI applications in primary care for remote communities
- **Tribal Education Departments National Assembly** - AI literacy programs emphasizing tribal sovereignty
- **FabNewport and Social Enterprise Greenhouse** (Rhode Island) - Community innovation
- **Martin Luther King Jr. Center for Nonviolent Social Change** (Georgia) - Civic engagement

### Long-Term Philanthropic Commitment

The foundation announced it will "initially focus on a \$25 billion commitment" across two primary areas:[^27]

1. **Health and Curing Diseases** - Accelerating breakthroughs in diagnostics, treatments, and cures; funding scientists; creating open-source datasets
2. **Technical Solutions to AI Resilience** - Maximizing AI benefits and minimizing risks through cybersecurity-like protections for AI systems

However, the foundation has not provided a timeline for when it will disburse this \$25 billion or whether it will structure itself as a private foundation subject to IRS rules requiring 5% annual distribution of assets. As of fiscal year 2023 (before the restructuring), the entity disbursed only \$2.6 million in grants.[^28]

If the OpenAI Foundation were required to follow the 5% rule on its \$130 billion corpus, it would need to disburse approximately \$6.5 billion annually—placing it in the same tier as the Gates Foundation, which averaged \$7 billion per year from 2019-2023.[^29]

### AI and Mental Health Research Grants

In addition to the People-First AI Fund, OpenAI's safety systems organization launched an AI and Mental Health Grant Program in 2025, awarding up to \$2 million in grants for interdisciplinary research on AI-mental health intersections. The program focuses on risks, benefits, cultural and language variations in distress expressions, lived experiences, and healthcare provider usage. Applications were accepted through December 19, 2025, with notifications by January 15, 2026.[^30]

## Comparison with Anthropic's Long-Term Benefit Trust

The OpenAI Foundation's structure invites comparison with <EntityLink id="anthropic">Anthropic</EntityLink>'s <EntityLink id="long-term-benefit-trust">Long-Term Benefit Trust</EntityLink>, both of which aim to preserve mission alignment as for-profit AI companies scale. However, the mechanisms differ significantly:

| Aspect | OpenAI Foundation | Anthropic Long-Term Benefit Trust |
|--------|-------------------|----------------------------------|
| **Control Mechanism** | Direct equity ownership (26%) + special voting rights | Pledge-based governance without equity ownership |
| **Board Appointment** | Foundation appoints all OpenAI Group directors | Trust holds majority of board seats via shareholder pledge |
| **Financial Interest** | \$130B equity stake creates financial incentive | No financial stake; pure governance role |
| **Legal Basis** | Corporate governance rights in PBC structure | Contractual shareholder agreement |
| **Permanence** | Equity ownership is legally binding | Dependent on continued shareholder commitment to pledge |
| **Conflicts of Interest** | Board members serve on both nonprofit and for-profit boards | Separate governance structures |

Anthropic's structure, established in 2023, relies on a shareholder pledge where investors agree to vote their shares according to the Trust's recommendations. The Trust is designed to hold a majority of board seats and maintain control over decisions related to AI safety and the company's public benefit mission.[^31] This approach avoids the potential conflicts of interest created by financial ownership but depends on the continued voluntary participation of shareholders.

OpenAI's approach, by contrast, gives the foundation direct financial returns from the for-profit entity's success, which critics argue creates incentives to prioritize growth and profitability over safety and public benefit.[^32] However, the foundation's special voting rights provide legally enforceable control that does not depend on voluntary commitments.

## Criticisms and Controversies

The OpenAI Foundation has faced substantial criticism from AI safety advocates, nonprofit watchdogs, and legal experts who question whether the restructuring genuinely preserves the original nonprofit mission or represents an abandonment of public commitments.

### Mission Abandonment and Asset Misuse

Critics argue the restructuring fundamentally betrays OpenAI's core purpose. As one former employee noted, "the entire philanthropic theory of change here was: we're going to put guardrails on profit motives so we can develop this tech safely."[^33] The shift to a for-profit-controlled structure, they argue, removes these guardrails.

Public Citizen, a consumer advocacy organization, released a strongly worded statement arguing that "the OpenAI Foundation will function as a corporate foundation, doing some good work but for the underlying purpose of advancing the interests of OpenAI For-Profit. The problem is, that's not how OpenAI Nonprofit was formed or what it is required to do—the for-profit was (dubiously) created to advance the mission of the nonprofit, not the reverse."[^34]

Tyler Johnston, executive director of AI watchdog The Midas Project, characterized the foundation's 26% stake as representing "humanity surrendering tens of hundreds of billions of dollars it was already entitled to," noting that the public will have far lower stake and control compared to the original structure where investors' returns were capped.[^35]

Legal scholar Luís Calderón Gómez from Cardozo School of Law explained that OpenAI could not simply abandon its nonprofit status because doing so would have required the for-profit entity to purchase the nonprofit's assets "for fair market value, something that it was unlikely to be able to do" given OpenAI's \$500 billion valuation. The restructuring, he suggested, was a creative workaround to this legal constraint.[^36]

### Conflicts of Interest and Governance Concerns

The dual-board structure, where eight of nine foundation board members also serve on OpenAI Group's board, has raised significant concerns about conflicts of interest. Fred Blackwell, CEO of the San Francisco Foundation, questioned whether board members would genuinely prioritize the foundation's public-serving mission over the for-profit entity's commercial interests: "Will the foundation be subject to the 5% rule, and will it spend beyond that requirement?"[^37]

Judith Bell, Chief Impact Officer at the San Francisco Foundation, emphasized that proper oversight requires "independent valuation and governance of nonprofit assets" and urged the foundation to "anchor AI development in human values" rather than market imperatives.[^38]

The foundation's initial philanthropic focus areas—health and AI resilience—have been criticized as potentially serving OpenAI's business interests. As Inside Philanthropy noted, OpenAI has a "financial interest in selling its technology to medical research entities," raising questions about whether health-focused grants constitute genuine public benefit or research and development for the commercial entity.[^39]

Similarly, the AI resilience program faces challenges managing conflicts of interest, as noted by Legal Advocates for Safe Science and Technology: OpenAI might be "biased against valuable technical AI safety work that could identify risks in OpenAI's models."[^40]

### "PR Stunts" and Regulatory Capture

The foundation's philanthropic initiatives have been dismissed by some critics as "calculated PR stunts" designed to placate regulators and the nonprofit sector. The \$50 million People-First AI Fund, while substantial, has been characterized as "obvious pandering" to California regulators, particularly given the timing of its announcement during the restructuring approval process.[^41]

Nick Moës, Executive Director of The Future Society, called October 28, 2025, a "dark day for philanthropy," arguing that "the public will actually benefit less from OpenAI's immense profits" under the new structure compared to the original nonprofit model.[^42]

### Safety Concerns and Rushed Development

Beyond structural concerns, critics have accused OpenAI of prioritizing market speed over safety. The organization disbanded its Superalignment and AGI Readiness teams—critical to its founding commitment to safe AI development—and "dramatically reduced safety testing times," releasing powerful models like DeepResearch and GPT-4.1 without promised safety reports.[^43]

These concerns gained public attention following incidents such as a teenager's death allegedly involving ChatGPT functioning as a "suicide coach," prompting calls for child safety regulations.[^44] The Safety and Security Committee's structure—four part-time volunteers with no staff overseeing the development of potentially transformative AGI systems—has been criticized as grossly inadequate.[^45]

### Legal Challenges

The restructuring faces ongoing legal challenges, most notably from Elon Musk, who filed a lawsuit arguing that his \$44 million investment was contingent on OpenAI remaining a nonprofit organization in perpetuity. Musk's lawsuit alleges that the restructuring violates founding agreements and represents a breach of the terms under which he and others provided initial funding.[^46] A trial is scheduled for fall 2025.

The EyesOnOpenAI coalition, led by the San Francisco Foundation, has continued to raise concerns about asset valuation and independence even after the California and Delaware Attorneys General approved the restructuring with conditions. In January 2025, the coalition sent an open letter to California AG Rob Bonta calling for action to protect OpenAI's charitable assets and ensure proper safeguards.[^47]

## Implications for AI Safety and Funding

The OpenAI Foundation's emergence as a potentially \$130+ billion philanthropic entity has significant implications for AI safety funding and governance.

### Scale of Resources

If the foundation operates as a traditional private foundation distributing 5% annually, it would represent by far the largest source of dedicated AI-related philanthropic funding, dwarfing current major funders like <EntityLink id="open-philanthropy">Open Philanthropy</EntityLink>, the <EntityLink id="sff">Survival and Flourishing Fund</EntityLink>, and the <EntityLink id="fli">Future of Life Institute</EntityLink>. Even the announced \$25 billion commitment, if deployed over a decade, would represent \$2.5 billion annually—an unprecedented scale for AI safety and related work.

However, the foundation's focus areas—health and AI resilience—may not directly translate to support for independent AI safety research, especially work that might identify risks in OpenAI's own systems. The potential for conflicts of interest raises questions about whether the foundation will fund critical, independent safety research or primarily support work that advances OpenAI's commercial objectives.

### Comparison with Other AI Safety Funding

Other major AI companies have taken different approaches to safety funding and governance:

- **Anthropic** established its <EntityLink id="long-term-benefit-trust">Long-Term Benefit Trust</EntityLink> with governance control but no financial stake, avoiding direct conflicts of interest
- **<EntityLink id="deepmind">DeepMind</EntityLink>** (now Google DeepMind) operates within Google's corporate structure with safety teams but no independent oversight entity
- **Various companies** contribute to external AI safety organizations but retain full corporate control over their own development

The OpenAI Foundation represents a middle path: substantial financial resources coupled with governance control, but embedded within a structure where the same individuals oversee both the nonprofit mission and for-profit operations.

### Precedent for Future AI Governance

The OpenAI Foundation's structure may set a precedent for how other AI companies balance mission preservation with capital requirements. If successful in maintaining genuine mission alignment while enabling necessary investment, it could provide a model for other organizations developing transformative AI systems. If, however, the foundation proves to prioritize commercial interests over public benefit, it may serve as a cautionary tale about the limits of nonprofit oversight in for-profit AI development.

The ultimate test will be whether the foundation demonstrates willingness to constrain OpenAI's commercial activities when they conflict with safety or public benefit considerations—for example, by blocking product releases deemed insufficiently safe or redirecting resources toward safety research that might slow commercialization.

## Key Uncertainties

Several fundamental questions about the OpenAI Foundation remain unresolved:

1. **Distribution Requirements**: Will the foundation be structured as a private foundation subject to the 5% annual distribution rule, or will it adopt a different structure with more flexible requirements?

2. **Timeline for Grantmaking**: When will the foundation begin deploying the announced \$25 billion commitment, and at what pace?

3. **Independence of Grantmaking**: Will the foundation support independent AI safety research that might identify risks in OpenAI's models, or will it primarily fund work aligned with OpenAI's commercial objectives?

4. **Governance in Practice**: Will the foundation's board exercise its authority to constrain OpenAI Group's activities when they conflict with safety or public benefit considerations, or will the dual-board structure lead to prioritization of commercial interests?

5. **Long-Term Control**: As OpenAI Group pursues a potential IPO and future funding rounds, will the foundation maintain effective control, or will dilution and investor pressure erode its governance authority?

6. **Legal Resolution**: How will the Elon Musk lawsuit and potential other legal challenges be resolved, and might they force changes to the foundation's structure or operations?

7. **Comparison with Alternatives**: Will the OpenAI Foundation's equity-based control model prove more or less effective at preserving mission alignment than Anthropic's pledge-based trust structure or other governance approaches?

These uncertainties reflect the unprecedented nature of the foundation's structure and the inherent tensions in attempting to maintain nonprofit mission alignment while operating as one of the world's most valuable technology companies.

## Sources

[^1]: [OpenAI Foundation - Official Website](https://openai.com/foundation/)
[^2]: [Built to Benefit Everyone - OpenAI Blog](https://openai.com/index/built-to-benefit-everyone/)
[^3]: [OpenAI's restructuring: how the company's structure works - NBC News](https://www.nbcnews.com/tech/tech-news/openai-restructuring-company-structure-chatgpt-invest-own-rcna240138)
[^4]: [Our Structure - OpenAI](https://openai.com/our-structure/)
[^5]: [People-First AI Fund Grantees - OpenAI Blog](https://openai.com/index/people-first-ai-fund-grantees/)
[^6]: [OpenAI - Britannica](https://www.britannica.com/money/OpenAI)
[^7]: [OpenAI: When and Why It Was Founded - Data Studios](https://www.datastudios.org/post/openai-when-and-why-it-was-founded-origins-mission-and-early-vision)
[^8]: [OpenAI - Wikipedia](https://en.wikipedia.org/wiki/OpenAI)
[^9]: [OpenAI Company Research - Contrary Research](https://research.contrary.com/company/openai)
[^10]: [OpenAI Timeline - Time Magazine](https://time.com/7328674/openai-chatgpt-sam-altman-elon-musk-timeline/)
[^11]: [A nonprofit on top, billions below: Inside OpenAI's new corporate balancing act - NBC News](https://www.nbcnews.com/tech/tech-news/openai-restructuring-company-structure-chatgpt-invest-own-rcna240138)
[^12]: [OpenAI Now Has a Foundation. We Have Some Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^13]: [OpenAI Restructure - San Francisco Foundation](https://sff.org/openai-restructure/)
[^14]: [80,000 Hours Podcast - OpenAI Restructuring Discussion](https://www.youtube.com/watch?v=EA0S66uW1kM)
[^15]: [OpenAI - Wikipedia](https://en.wikipedia.org/wiki/OpenAI)
[^16]: [OpenAI Structure - NBC News](https://www.nbcnews.com/tech/tech-news/openai-restructuring-company-structure-chatgpt-invest-own-rcna240138)
[^17]: [Our Structure - OpenAI](https://openai.com/our-structure/)
[^18]: [OpenAI Leadership - Wikipedia](https://en.wikipedia.org/wiki/OpenAI)
[^19]: [Our Structure - OpenAI](https://openai.com/our-structure/)
[^20]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^21]: [OpenAI Restructuring Discussion - 80,000 Hours](https://www.youtube.com/watch?v=EA0S66uW1kM)
[^22]: [People-First AI Fund Grantees - OpenAI](https://openai.com/index/people-first-ai-fund-grantees/)
[^23]: [How to Apply for OpenAI's \$50M People-First AI Fund - The Class Consulting Group](https://www.theclassconsultinggroup.org/post/how-to-apply-for-openai-s-50m-people-first-ai-fund-2025-guide)
[^24]: [People-First AI Fund Grantees - OpenAI](https://openai.com/index/people-first-ai-fund-grantees/)
[^25]: [A Look Under the Hood of the OpenAI Foundation's People-First AI Fund - Inside Philanthropy](https://www.insidephilanthropy.com/home/a-look-under-the-hood-of-the-openai-foundations-people-first-ai-fund)
[^26]: [People-First AI Fund Grantees - OpenAI](https://openai.com/index/people-first-ai-fund-grantees/)
[^27]: [OpenAI Foundation - Official Website](https://openai.com/foundation/)
[^28]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^29]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^30]: [AI and Mental Health Research Grants - OpenAI](https://openai.com/index/ai-mental-health-research-grants/)
[^31]: Anthropic Long-Term Benefit Trust structure (general knowledge; specific citation unavailable in provided research)
[^32]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^33]: [Inside OpenAI's Controversial Plan to Abandon Its Nonprofit - EA Forum](https://forum.effectivealtruism.org/posts/tbrF6M9mtsMiqc75q/inside-openai-s-controversial-plan-to-abandon-its-nonprofit)
[^34]: [NBC News on OpenAI Restructuring](https://www.nbcnews.com/tech/tech-news/openai-restructuring-company-structure-chatgpt-invest-own-rcna240138)
[^35]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^36]: [NBC News on OpenAI Restructuring](https://www.nbcnews.com/tech/tech-news/openai-restructuring-company-structure-chatgpt-invest-own-rcna240138)
[^37]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^38]: [OpenAI Restructure - San Francisco Foundation](https://sff.org/openai-restructure/)
[^39]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^40]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^41]: [Inside OpenAI's Controversial Plan - EA Forum](https://forum.effectivealtruism.org/posts/tbrF6M9mtsMiqc75q/inside-openai-s-controversial-plan-to-abandon-its-nonprofit)
[^42]: [OpenAI Foundation Questions - Inside Philanthropy](https://www.insidephilanthropy.com/home/openai-now-has-a-foundation-we-have-some-questions)
[^43]: [Inside OpenAI's Controversial Plan - EA Forum/LessWrong](https://www.lesswrong.com/posts/SHbEEFHqTuoveZcLr/inside-openai-s-controversial-plan-to-abandon-its-nonprofit)
[^44]: [5 Policy Questions Prompted by OpenAI's Restructuring - Tech Policy Press](https://techpolicy.press/5-policy-questions-prompted-by-openais-restructuring)
[^45]: [OpenAI Restructuring Discussion - 80,000 Hours](https://www.youtube.com/watch?v=EA0S66uW1kM)
[^46]: [Inside OpenAI's Controversial Plan - EA Forum](https://forum.effectivealtruism.org/posts/tbrF6M9mtsMiqc75q/inside-openai-s-controversial-plan-to-abandon-its-nonprofit)
[^47]: [Coalition Requests Attorney General Action - San Francisco Foundation](https://sff.org/coalition-requests-attorney-general-action-to-protect-openais-charitable-assets/)

<Backlinks />
