insights:
  - id: "401"
    insight: "Timeline disagreement is fundamental: median estimates for transformative AI range from 2027 to 2060+ among
      informed experts, reflecting deep uncertainty about scaling, algorithms, and bottlenecks."
    source: /knowledge-base/metrics/expert-opinion
    tags:
      - timelines
      - forecasting
      - disagreement
    type: disagreement
    surprising: 2
    important: 4.5
    actionable: 3
    neglected: 2
    compact: 4.3
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "402"
    insight: "Interpretability value is contested: some researchers view mechanistic interpretability as the path to
      alignment; others see it as too slow to matter before advanced AI."
    source: /knowledge-base/cruxes/solutions
    tags:
      - interpretability
      - research-priorities
      - crux
    type: disagreement
    surprising: 2.5
    important: 4
    actionable: 4
    neglected: 2.5
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "403"
    insight: "Open source safety tradeoff: open-sourcing models democratizes safety research but also democratizes misuse -
      experts genuinely disagree on net impact."
    source: /ai-transition-model/factors/misalignment-potential/ai-governance
    tags:
      - open-source
      - misuse
      - governance
    type: disagreement
    surprising: 2.2
    important: 4
    actionable: 3.5
    neglected: 2.5
    compact: 4.4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "404"
    insight: "Warning shot probability: some expect clear dangerous capabilities before catastrophe; others expect deceptive
      systems or rapid takeoff without warning."
    source: /knowledge-base/cruxes/accident-risks
    tags:
      - warning-shot
      - deception
      - takeoff
    type: disagreement
    surprising: 2.5
    important: 4.5
    actionable: 3
    neglected: 3
    compact: 4.2
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "706"
    insight: Mesa-optimization remains empirically unobserved in current systems, though theoretical arguments for its
      emergence are contested.
    source: /knowledge-base/cruxes/accident-risks
    tags:
      - mesa-optimization
      - empirical
      - theoretical
    type: disagreement
    surprising: 2.5
    important: 4.2
    actionable: 3.5
    neglected: 2.5
    compact: 4.2
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "826"
    insight: ML researchers median p(doom) is 5% vs AI safety researchers 20-30% - the gap may partly reflect exposure to
      safety arguments rather than objective assessment.
    source: /knowledge-base/metrics/expert-opinion
    tags:
      - expert-opinion
      - disagreement
      - methodology
    type: disagreement
    surprising: 2.5
    important: 3.5
    actionable: 3.5
    neglected: 2.5
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "827"
    insight: 60-75% of experts believe AI verification will permanently lag generation capabilities - provenance-based
      authentication may be the only viable path forward.
    source: /knowledge-base/cruxes/solutions
    tags:
      - detection
      - authentication
      - cruxes
      - methodology
    type: disagreement
    surprising: 2.8
    important: 4
    actionable: 4.5
    neglected: 3
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: solution-cruxes-1
    insight: Only 25-40% of experts believe AI-based verification can match generation capability; 60-75% expect
      verification to lag indefinitely, suggesting verification R&D may yield limited returns without alternative
      approaches like provenance.
    source: /knowledge-base/cruxes/solutions/
    tags:
      - verification-gap
      - expert-disagreement
      - arms-race
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 3.5
    compact: 4.5
    added: 2025-01-22
    lastVerified: 2026-01-23
  - id: risk-portfolio-4
    insight: "Expert surveys show massive disagreement on AI existential risk: AI Impacts survey (738 ML researchers) found
      5-10% median x-risk, while Conjecture survey (22 safety researchers) found 80% median. True uncertainty likely
      spans 2-50%."
    source: /knowledge-base/models/analysis-models/ai-risk-portfolio-analysis/
    tags:
      - expert-disagreement
      - x-risk
      - uncertainty
    type: disagreement
    surprising: 4.5
    important: 5
    actionable: 3
    neglected: 3.5
    compact: 4
    added: 2025-01-22
    lastVerified: 2026-01-23
  - id: power-seeking-conditions-9
    insight: Current AI safety interventions may fundamentally misunderstand power-seeking risks, with expert opinions
      diverging from 30% to 90% emergence probability, indicating critical uncertainty in our understanding.
    source: /knowledge-base/models/risk-models/power-seeking-conditions/
    tags:
      - expert-consensus
      - uncertainty
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 3.5
    neglected: 4
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: deceptive-alignment-25
    insight: Expert probability estimates for deceptive AI alignment range dramatically from 5% to 90%, indicating profound
      uncertainty about this critical risk mechanism.
    source: /knowledge-base/risks/accident/deceptive-alignment/
    tags:
      - expert-estimates
      - risk-uncertainty
    type: disagreement
    surprising: 4.5
    important: 5
    actionable: 3.5
    neglected: 4
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: case-against-xrisk-35
    insight: The 50x+ gap between expert risk estimates (LeCun ~0% vs Yampolskiy 99%) reflects fundamental disagreement
      about technical assumptions rather than just parameter uncertainty, indicating the field lacks consensus on core
      questions.
    source: /knowledge-base/debates/formal-arguments/case-against-xrisk/
    tags:
      - expert-disagreement
      - uncertainty
      - methodology
    type: disagreement
    surprising: 3.5
    important: 3.5
    actionable: 4
    neglected: 4
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: self-improvement-52
    insight: The feasibility of software-only intelligence explosion is highly sensitive to compute-labor substitutability,
      with recent analysis finding conflicting evidence ranging from strong substitutes (enabling RSI without compute
      bottlenecks) to strong complements (keeping compute as binding constraint).
    source: /knowledge-base/capabilities/self-improvement/
    tags:
      - intelligence-explosion
      - compute-constraints
      - economic-modeling
    type: disagreement
    surprising: 3.5
    important: 4.5
    actionable: 4
    neglected: 4
    compact: 3.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: capability-threshold-model-59
    insight: The AI safety industry is fundamentally unprepared for existential risks, with all major companies claiming AGI
      achievement within the decade yet none scoring above D-grade in existential safety planning according to
      systematic assessment.
    source: /knowledge-base/models/framework-models/capability-threshold-model/
    tags:
      - industry-preparedness
      - existential-risk
      - governance
      - safety-planning
    type: disagreement
    surprising: 3.5
    important: 5
    actionable: 4
    neglected: 3
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: research-agendas-5
    insight: The field estimates only 40-60% probability that current AI safety approaches will scale to superhuman AI, yet
      most research funding concentrates on these near-term methods rather than foundational alternatives.
    source: /knowledge-base/responses/alignment/research-agendas/
    tags:
      - scaling-uncertainty
      - resource-allocation
      - timeline-mismatch
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 4
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: power-seeking-26
    insight: Turner's formal mathematical proofs demonstrate that power-seeking emerges from optimization fundamentals
      across most reward functions in MDPs, but Turner himself cautions against over-interpreting these results for
      practical AI systems.
    source: /knowledge-base/risks/accident/power-seeking/
    tags:
      - theory-practice-gap
      - instrumental-convergence
      - formal-results
    type: disagreement
    surprising: 3.5
    important: 4
    actionable: 3
    neglected: 4
    compact: 3.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: agi-timeline-46
    insight: There is a striking 20+ year disagreement between industry lab leaders claiming AGI by 2026-2031 and broader
      expert consensus of 2045, suggesting either significant overconfidence among those closest to development or
      insider information not reflected in academic surveys.
    source: /knowledge-base/forecasting/agi-timeline/
    tags:
      - expert-disagreement
      - industry-timelines
      - forecasting-bias
    type: disagreement
    surprising: 4.5
    important: 4.5
    actionable: 3.5
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: emergent-capabilities-36
    insight: Stanford research suggests 92% of reported emergent abilities occur under just two specific metrics (Multiple
      Choice Grade and Exact String Match), with 25 of 29 alternative metrics showing smooth rather than emergent
      improvements.
    source: /knowledge-base/risks/accident/emergent-capabilities/
    tags:
      - measurement-artifacts
      - emergence-debate
      - evaluation-metrics
    type: disagreement
    surprising: 4
    important: 3.5
    actionable: 4
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: why-alignment-hard-107
    insight: Expert estimates of AI alignment failure probability span from 5% (median ML researcher) to 95%+ (Eliezer
      Yudkowsky), with Paul Christiano at 10-20% and MIRI researchers averaging 66-98%, indicating massive uncertainty
      about fundamental technical questions.
    source: /knowledge-base/debates/formal-arguments/why-alignment-hard/
    tags:
      - expert-opinion
      - probability-estimates
      - uncertainty
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 3.5
    neglected: 2.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: critical-uncertainties-1
    insight: "Expert disagreement on AI extinction risk is extreme: 41-51% of AI researchers assign >10% probability to
      human extinction from AI, while remaining researchers assign much lower probabilities, with this disagreement
      stemming primarily from just 8-12 key uncertainties."
    source: /knowledge-base/models/analysis-models/critical-uncertainties/
    tags:
      - expert-surveys
      - extinction-risk
      - uncertainty
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: misuse-risks-31
    insight: The RAND biological uplift study found no statistically significant difference in bioweapon attack plan
      viability with or without LLM access, contradicting widespread assumptions about AI bio-risk while other evidence
      (OpenAI o3 at 94th percentile virology, 13/57 bio-tools rated 'Red') suggests concerning capabilities.
    source: /knowledge-base/cruxes/misuse-risks/
    tags:
      - bioweapons
      - capabilities
      - risk-assessment
    type: disagreement
    surprising: 4
    important: 4
    actionable: 3.5
    neglected: 4
    compact: 3.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: goal-misgeneralization-44
    insight: Goal misgeneralization may become either more severe or self-correcting as AI capabilities scale, with current
      empirical evidence being mixed on which direction scaling takes us.
    source: /knowledge-base/responses/safety-approaches/goal-misgeneralization/
    tags:
      - scaling
      - empirical-uncertainty
      - capabilities
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 3
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: provably-safe-51
    insight: The capability tax from formal safety constraints may render provably safe AI systems practically unusable,
      creating a fundamental tension between mathematical safety guarantees and system effectiveness.
    source: /knowledge-base/responses/safety-approaches/provably-safe/
    tags:
      - capability-tax
      - practicality
      - trade-offs
    type: disagreement
    surprising: 3.5
    important: 4
    actionable: 4.5
    neglected: 3
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: bioweapons-59
    insight: Open-source AI models present a fundamental governance challenge for biological risks, as China's DeepSeek
      model was reported by Anthropic's CEO as 'the worst of basically any model we'd ever tested' for biosecurity with
      'absolutely no blocks whatsoever.'
    source: /knowledge-base/risks/misuse/bioweapons/
    tags:
      - open-source-models
      - international-governance
      - safety-gaps
    type: disagreement
    surprising: 3.5
    important: 4.5
    actionable: 4
    neglected: 4
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: why-alignment-easy-94
    insight: Leading alignment researchers like Paul Christiano and Jan Leike express 70-85% confidence in solving alignment
      before transformative AI, contrasting sharply with MIRI's 5-15% estimates, indicating significant expert
      disagreement on tractability.
    source: /knowledge-base/debates/formal-arguments/why-alignment-easy/
    tags:
      - expert-opinion
      - probability-estimates
      - researcher-views
    type: disagreement
    surprising: 3.5
    important: 4
    actionable: 3
    neglected: 3.5
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: anthropic-core-views-17
    insight: The RSP framework has been adopted by OpenAI and DeepMind, but critics argue the October 2024 update reduced
      accountability by shifting from precise capability thresholds to more qualitative descriptions of safety
      requirements.
    source: /knowledge-base/responses/alignment/anthropic-core-views/
    tags:
      - responsible-scaling
      - policy-influence
      - transparency
    type: disagreement
    surprising: 3.5
    important: 4
    actionable: 4.5
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: colorado-ai-act-68
    insight: The Trump administration has specifically targeted Colorado's AI Act with a DOJ litigation taskforce, creating
      substantial uncertainty about whether state-level AI regulation can survive federal preemption challenges.
    source: /knowledge-base/responses/governance/legislation/colorado-ai-act/
    tags:
      - federal-preemption
      - political-risk
      - regulatory-uncertainty
    type: disagreement
    surprising: 4.5
    important: 4.5
    actionable: 3.5
    neglected: 4
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: flash-dynamics-threshold-12
    insight: Threshold 5 (recursive acceleration) represents an existential risk where AI systems improve themselves faster
      than humans can track or govern, but is currently assessed as 'limited risk' with 10% probability of recursive
      takeoff scenario by 2030-2035.
    source: /knowledge-base/models/threshold-models/flash-dynamics-threshold/
    tags:
      - recursive-improvement
      - existential-risk
      - ai-development
    type: disagreement
    surprising: 4
    important: 5
    actionable: 3
    neglected: 2
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: corporate-37
    insight: External audit acceptance varies significantly between companies, with Anthropic showing high acceptance while
      OpenAI shows limited acceptance, revealing substantial differences in accountability approaches despite similar
      market positions.
    source: /knowledge-base/responses/corporate/
    tags:
      - transparency
      - external-oversight
      - accountability
    type: disagreement
    surprising: 3.5
    important: 4
    actionable: 4.5
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: anthropic-96
    insight: Anthropic leadership estimates 10-25% probability of AI catastrophic risk while actively building frontier
      systems, creating an apparent contradiction that they resolve through 'frontier safety' reasoning.
    source: /knowledge-base/organizations/labs/anthropic/
    tags:
      - risk-estimates
      - frontier-safety
      - philosophical-tensions
    type: disagreement
    surprising: 4
    important: 4
    actionable: 3
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: misaligned-catastrophe-120
    insight: Expert probability estimates for AI-caused extinction by 2100 vary dramatically from 0% to 99%, with ML
      researchers giving a median of 5% but mean of 14.4%, suggesting heavy-tailed risk distributions that standard risk
      assessment may underweight.
    source: /knowledge-base/future-projections/misaligned-catastrophe/
    tags:
      - expert-forecasting
      - probability-estimates
      - risk-assessment
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 3.5
    neglected: 4
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: structural-risks-25
    insight: Structural risks as a distinct category from accident/misuse risks remain contested (40-55% view as genuinely
      distinct), representing a fundamental disagreement that determines whether governance interventions or technical
      safety should be prioritized.
    source: /knowledge-base/cruxes/structural-risks/
    tags:
      - foundational-questions
      - risk-categorization
      - prioritization
    type: disagreement
    surprising: 3.5
    important: 4
    actionable: 3.5
    neglected: 4
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: expert-opinion-80
    insight: Expert opinions on AI extinction risk show extraordinary disagreement with individual estimates ranging from
      0.01% to 99% despite a median of 5-10%, indicating fundamental uncertainty rather than emerging consensus among
      domain experts.
    source: /knowledge-base/metrics/expert-opinion/
    tags:
      - expert-opinion
      - existential-risk
      - forecasting
      - disagreement
    type: disagreement
    surprising: 4.5
    important: 4.5
    actionable: 3.5
    neglected: 2
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: cirl-87
    insight: The robustness of CIRL's alignment guarantees to capability scaling remains an open crux, with uncertainty
      about whether sufficiently advanced systems could game the uncertainty mechanism itself.
    source: /knowledge-base/responses/safety-approaches/cirl/
    tags:
      - capability-robustness
      - deception
      - scaling
      - uncertainty
    type: disagreement
    surprising: 3.5
    important: 4.5
    actionable: 3.5
    neglected: 3
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: lab-incentives-model-120
    insight: Racing dynamics intensification is a key crux that could elevate lab incentive work from mid-tier to high
      importance, while technical safety tractability affects whether incentive alignment even matters.
    source: /knowledge-base/models/dynamics-models/lab-incentives-model/
    tags:
      - strategic-cruxes
      - racing-dynamics
      - prioritization
    type: disagreement
    surprising: 3
    important: 4
    actionable: 4
    neglected: 3
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: uk-aisi-127
    insight: The February 2025 rebrand from 'AI Safety Institute' to 'AI Security Institute' represents a significant
      narrowing of focus away from broader societal harms toward national security threats, drawing criticism from the
      AI safety community.
    source: /knowledge-base/organizations/government/uk-aisi/
    tags:
      - scope-change
      - controversy
      - national-security
    type: disagreement
    surprising: 4
    important: 3.5
    actionable: 3
    neglected: 3.5
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: china-ai-regulations-55
    insight: China's regulatory approach prioritizing 'socialist values' alignment and social stability over individual
      rights creates fundamental incompatibilities with Western AI governance frameworks, posing significant barriers to
      international coordination on existential AI risks despite shared expert concerns about AGI dangers.
    source: /knowledge-base/responses/governance/legislation/china-ai-regulations/
    tags:
      - international-coordination
      - values-alignment
      - ai-governance
    type: disagreement
    surprising: 3.5
    important: 5
    actionable: 4
    neglected: 3
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: pause-debate-85
    insight: Several major AI researchers hold directly opposing views on existential risk itself—Yann LeCun believes the
      risk 'isn't real' while Eliezer Yudkowsky advocates 'shut it all down'—suggesting the pause debate reflects deeper
      disagreements about fundamental threat models rather than just policy preferences.
    source: /knowledge-base/debates/pause-debate/
    tags:
      - expert-disagreement
      - risk-assessment
      - pause-debate
    type: disagreement
    surprising: 4
    important: 4
    actionable: 3.5
    neglected: 3.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: structural-89
    insight: There is a massive 72% to 8% public preference for slowing versus speeding AI development, creating a large
      democratic deficit as AI development is primarily shaped by optimistic technologists rather than risk-concerned
      publics.
    source: /knowledge-base/metrics/structural/
    tags:
      - elite-public-gap
      - democratic-legitimacy
      - public-opinion
    type: disagreement
    surprising: 4
    important: 4.5
    actionable: 4.5
    neglected: 4
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
