insights:
  - id: "301"
    insight: "Lock-in risks may dominate takeover risks: AI systems could entrench values/power structures for very long
      periods without any dramatic 'takeover' event."
    source: /ai-transition-model/scenarios/long-term-lockin
    tags:
      - lock-in
      - long-term
      - governance
    type: neglected
    surprising: 3.5
    important: 4.5
    actionable: 3.5
    neglected: 4.5
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "302"
    insight: Human oversight quality degrades as AI capability increases - the very systems most in need of oversight become
      hardest to oversee.
    source: /ai-transition-model/factors/misalignment-potential
    tags:
      - oversight
      - scalability
      - governance
    type: neglected
    surprising: 2.8
    important: 4.3
    actionable: 3.8
    neglected: 4
    compact: 4.5
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "303"
    insight: "Multi-agent AI dynamics are understudied: interactions between multiple AI systems could produce emergent
      risks not present in single-agent scenarios."
    source: /knowledge-base/cruxes/structural-risks
    tags:
      - multi-agent
      - emergence
      - coordination
    type: neglected
    surprising: 3.2
    important: 3.8
    actionable: 3.5
    neglected: 4.8
    compact: 4.2
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "304"
    insight: Non-Western perspectives on AI governance are systematically underrepresented in safety discourse, creating
      potential blind spots and reducing policy legitimacy.
    source: /ai-transition-model/factors/civilizational-competence/governance
    tags:
      - governance
      - diversity
      - epistemics
    type: neglected
    surprising: 2.5
    important: 3.5
    actionable: 3.8
    neglected: 4.5
    compact: 4.3
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "305"
    insight: AI's effect on human skill atrophy is poorly studied - widespread AI assistance may erode capabilities needed
      for oversight and recovery from AI failures.
    source: /knowledge-base/risks/structural/expertise-atrophy
    tags:
      - skills
      - human-capital
      - resilience
    type: neglected
    surprising: 3.5
    important: 3.8
    actionable: 3.5
    neglected: 4.8
    compact: 4.4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "715"
    insight: "AI safety discourse may have epistemic monoculture: small community with shared assumptions could have
      systematic blind spots."
    source: /ai-transition-model/factors/civilizational-competence/epistemics
    tags:
      - epistemics
      - community
      - diversity
    type: neglected
    surprising: 3
    important: 3.5
    actionable: 3.8
    neglected: 4
    compact: 4.2
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "719"
    insight: "Power concentration from AI may matter more than direct AI risk: transformative AI controlled by few could
      reshape governance without 'takeover'."
    source: /ai-transition-model/scenarios/long-term-lockin/political-power
    tags:
      - power-concentration
      - governance
      - structural-risk
    type: neglected
    surprising: 3
    important: 4.3
    actionable: 3.2
    neglected: 4
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "822"
    insight: Epistemic collapse from AI may be largely irreversible - learned helplessness listed as 'generational' in
      reversibility, yet receives far less attention than technical alignment.
    source: /knowledge-base/risks/structural/epistemic-risks
    tags:
      - epistemic
      - neglected
      - irreversibility
    type: neglected
    surprising: 3.5
    important: 4.2
    actionable: 3.5
    neglected: 4.5
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "823"
    insight: Flash dynamics - AI systems interacting faster than human reaction time - may create qualitatively new systemic
      risks, yet this receives minimal research attention.
    source: /knowledge-base/cruxes/structural-risks
    tags:
      - flash-dynamics
      - speed
      - systemic
      - neglected
    type: neglected
    surprising: 3.2
    important: 3.8
    actionable: 3.5
    neglected: 4.5
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "824"
    insight: Values crystallization risk - AI could lock in current moral frameworks before humanity develops sufficient
      wisdom - is discussed theoretically but has no active research program.
    source: /ai-transition-model/scenarios/long-term-lockin/values
    tags:
      - lock-in
      - values
      - neglected
      - longtermism
    type: neglected
    surprising: 2.8
    important: 4
    actionable: 3
    neglected: 4.5
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: "825"
    insight: Expertise atrophy from AI assistance is well-documented in aviation but understudied in critical domains like
      medicine, law, and security - 39% of skills projected obsolete by 2030.
    source: /knowledge-base/risks/structural/expertise-atrophy
    tags:
      - expertise
      - automation
      - human-factors
      - neglected
    type: neglected
    surprising: 2.5
    important: 3.8
    actionable: 4
    neglected: 4
    compact: 4
    added: 2025-01-21
    lastVerified: 2026-01-23
  - id: sa-004
    insight: International coordination on AI safety is rated PRIORITIZE but 'severely underdeveloped' at $10-30M/yr -
      critical governance infrastructure with insufficient investment.
    source: /knowledge-base/responses/safety-approaches/table
    tableRef: safety-approaches#international-coordination
    tags:
      - governance
      - international
      - neglected
      - safety-approaches
    type: neglected
    surprising: 2.8
    important: 4.5
    actionable: 4.2
    neglected: 4.5
    compact: 4.3
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: sa-005
    insight: Compute governance is 'one of few levers to affect timeline' and rated PRIORITIZE, yet receives only $5-20M/yr
      - policy research vastly underfunded relative to technical work.
    source: /knowledge-base/responses/safety-approaches/table
    tableRef: safety-approaches#compute-governance
    tags:
      - compute
      - governance
      - policy
      - safety-approaches
    type: neglected
    surprising: 3
    important: 4.3
    actionable: 4.5
    neglected: 4.5
    compact: 4.2
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: sa-010
    insight: Safety cases methodology ($5-15M/yr) offers 'promising framework; severely underdeveloped for AI' with
      PRIORITIZE recommendation - a mature approach from other industries needs adaptation.
    source: /knowledge-base/responses/safety-approaches/table
    tableRef: safety-approaches#safety-cases
    tags:
      - safety-cases
      - methodology
      - governance
      - safety-approaches
    type: neglected
    surprising: 3.2
    important: 4
    actionable: 4.5
    neglected: 4.2
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: sa-012
    insight: Cooperative AI research shows NEUTRAL capability uplift with SOME safety benefit - a rare approach that doesn't
      accelerate capabilities while improving multi-agent safety.
    source: /knowledge-base/responses/safety-approaches/table
    tableRef: safety-approaches#cooperative-ai
    tags:
      - cooperative-ai
      - differential-progress
      - multi-agent
      - safety-approaches
    type: neglected
    surprising: 3.5
    important: 3.8
    actionable: 4
    neglected: 4.2
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: risk-portfolio-2
    insight: "Governance/policy research is significantly underfunded: currently receives $18M (14% of funding) but optimal
      allocation for medium-timeline scenarios is 20-25%, creating a $7-17M annual funding gap."
    source: /knowledge-base/models/analysis-models/ai-risk-portfolio-analysis/
    tags:
      - funding-gap
      - governance
      - neglected-area
    type: neglected
    surprising: 3.5
    important: 5
    actionable: 5
    neglected: 4.5
    compact: 4
    added: 2025-01-22
    lastVerified: 2026-01-23
  - id: risk-portfolio-3
    insight: Agent safety is severely underfunded at $8.2M (6% of funding) versus the optimal 10-15% allocation,
      representing a $7-12M annual gap—a high-value investment opportunity with substantial room for marginal
      contribution.
    source: /knowledge-base/models/analysis-models/ai-risk-portfolio-analysis/
    tags:
      - agent-safety
      - funding-gap
      - high-neglectedness
    type: neglected
    surprising: 4
    important: 4.5
    actionable: 5
    neglected: 5
    compact: 4.5
    added: 2025-01-22
    lastVerified: 2026-01-23
  - id: ai-control-3
    insight: Current annual investment in AI control research is $6.5-11M with 25-40 FTEs, but estimated requirement is
      $38-68M—a funding gap of $19-33M that could significantly accelerate development.
    source: /knowledge-base/responses/alignment/ai-control/
    tags:
      - funding-gap
      - ai-control
      - resource-constraints
    type: neglected
    surprising: 3.5
    important: 4
    actionable: 4.5
    neglected: 4.5
    compact: 4.5
    added: 2025-01-22
    lastVerified: 2026-01-23
  - id: eliciting-latent-knowledge-26
    insight: The ELK problem has received only $5-15M/year in research investment despite being potentially critical for
      superintelligence safety, suggesting it may be significantly under-resourced relative to its importance.
    source: /knowledge-base/responses/safety-approaches/eliciting-latent-knowledge/
    tags:
      - elk
      - funding
      - research-priorities
    type: neglected
    surprising: 3.5
    important: 4
    actionable: 4.5
    neglected: 4
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: goal-misgeneralization-probability-81
    insight: The evaluation-to-deployment shift represents the highest risk scenario (Type 4 extreme shift) with 27.7% base
      misgeneralization probability, yet this critical transition receives insufficient attention in current safety
      practices.
    source: /knowledge-base/models/risk-models/goal-misgeneralization-probability/
    tags:
      - evaluation-deployment
      - distribution-shift
      - safety-practices
    type: neglected
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 4.5
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: international-regimes-48
    insight: Only 7 of 193 UN member states participate in the seven most prominent AI governance initiatives, while 118
      countries (mostly in the Global South) are entirely absent from AI governance discussions as of late 2024.
    source: /knowledge-base/responses/governance/compute-governance/international-regimes/
    tags:
      - global-governance
      - participation-gaps
      - developing-countries
    type: neglected
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 4.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: weak-to-strong-83
    insight: Despite being potentially critical for AI safety, weak-to-strong generalization receives only $10-50M annual
      investment and remains in experimental stages, suggesting significant underinvestment relative to its importance.
    source: /knowledge-base/responses/safety-approaches/weak-to-strong/
    tags:
      - research-funding
      - prioritization
      - resource-allocation
    type: neglected
    surprising: 3
    important: 4
    actionable: 4.5
    neglected: 4.5
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: capability-unlearning-42
    insight: Capability unlearning receives only $5-20M annually in research investment despite being one of the few
      approaches that could directly address open-weight model risks where behavioral controls are ineffective.
    source: /knowledge-base/responses/safety-approaches/capability-unlearning/
    tags:
      - funding
      - open-weight
      - research-investment
    type: neglected
    surprising: 3.5
    important: 4
    actionable: 4.5
    neglected: 4.5
    compact: 4.5
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: formal-verification-54
    insight: Formal verification receives only $5-20M/year in research investment despite potentially providing mathematical
      guarantees against deceptive alignment, representing a high neglectedness-to-impact ratio.
    source: /knowledge-base/responses/safety-approaches/formal-verification/
    tags:
      - funding
      - deception-robustness
      - research-priorities
    type: neglected
    surprising: 4
    important: 4
    actionable: 4.5
    neglected: 4.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
  - id: debate-115
    insight: AI Safety via Debate receives only $5-20M/year in investment despite being one of the few alignment approaches
      specifically designed to scale to superintelligent systems, unlike RLHF which fundamentally breaks at superhuman
      capabilities.
    source: /knowledge-base/responses/safety-approaches/debate/
    tags:
      - investment
      - scalability
      - superintelligence
    type: neglected
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 4.5
    compact: 4
    added: 2026-01-23
    lastVerified: 2026-01-23
