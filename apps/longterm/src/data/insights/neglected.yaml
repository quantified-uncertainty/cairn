insights:
  - id: "303"
    insight: >-
      Multi-agent AI dynamics are understudied: interactions between multiple AI systems could produce emergent risks
      not present in single-agent scenarios.
    source: /knowledge-base/cruxes/structural-risks
    tags:
      - multi-agent
      - emergence
      - coordination
    type: neglected
    surprising: 3.2
    important: 3.8
    actionable: 3.5
    neglected: 4.8
    compact: 4.2
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "304"
    insight: >-
      Non-Western perspectives on AI governance are systematically underrepresented in safety discourse, creating
      potential blind spots and reducing policy legitimacy.
    source: /ai-transition-model/factors/civilizational-competence/governance
    tags:
      - governance
      - diversity
      - epistemics
    type: neglected
    surprising: 2.5
    important: 3.5
    actionable: 3.8
    neglected: 4.5
    compact: 4.3
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "715"
    insight: >-
      AI safety discourse may have epistemic monoculture: small community with shared assumptions could have systematic
      blind spots.
    source: /ai-transition-model/factors/civilizational-competence/epistemics
    tags:
      - epistemics
      - community
      - diversity
    type: neglected
    surprising: 3
    important: 3.5
    actionable: 3.8
    neglected: 4
    compact: 4.2
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "719"
    insight: >-
      Power concentration from AI may matter more than direct AI risk: transformative AI controlled by few could reshape
      governance without 'takeover'.
    source: /ai-transition-model/scenarios/long-term-lockin/political-power
    tags:
      - power-concentration
      - governance
      - structural-risk
    type: neglected
    surprising: 3
    important: 4.3
    actionable: 3.2
    neglected: 4
    compact: 4
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "823"
    insight: >-
      Flash dynamics - AI systems interacting faster than human reaction time - may create qualitatively new systemic
      risks, yet this receives minimal research attention.
    source: /knowledge-base/cruxes/structural-risks
    tags:
      - flash-dynamics
      - speed
      - systemic
      - neglected
    type: neglected
    surprising: 3.2
    important: 3.8
    actionable: 3.5
    neglected: 4.5
    compact: 4
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: "824"
    insight: >-
      Values crystallization risk - AI could lock in current moral frameworks before humanity develops sufficient wisdom
      - is discussed theoretically but has no active research program.
    source: /ai-transition-model/scenarios/long-term-lockin/values
    tags:
      - lock-in
      - values
      - neglected
      - longtermism
    type: neglected
    surprising: 2.8
    important: 4
    actionable: 3
    neglected: 4.5
    compact: 4
    added: 2025-01-21T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: risk-portfolio-2
    insight: >-
      Governance/policy research is significantly underfunded: currently receives $18M (14% of funding) but optimal
      allocation for medium-timeline scenarios is 20-25%, creating a $7-17M annual funding gap.
    source: /knowledge-base/models/analysis-models/ai-risk-portfolio-analysis/
    tags:
      - funding-gap
      - governance
      - neglected-area
    type: neglected
    surprising: 3.5
    important: 5
    actionable: 5
    neglected: 4.5
    compact: 4
    added: 2025-01-22T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: risk-portfolio-3
    insight: >-
      Agent safety is severely underfunded at $8.2M (6% of funding) versus the optimal 10-15% allocation, representing a
      $7-12M annual gapâ€”a high-value investment opportunity with substantial room for marginal contribution.
    source: /knowledge-base/models/analysis-models/ai-risk-portfolio-analysis/
    tags:
      - agent-safety
      - funding-gap
      - high-neglectedness
    type: neglected
    surprising: 4
    important: 4.5
    actionable: 5
    neglected: 5
    compact: 4.5
    added: 2025-01-22T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: goal-misgeneralization-probability-81
    insight: >-
      The evaluation-to-deployment shift represents the highest risk scenario (Type 4 extreme shift) with 27.7% base
      misgeneralization probability, yet this critical transition receives insufficient attention in current safety
      practices.
    source: /knowledge-base/models/risk-models/goal-misgeneralization-probability/
    tags:
      - evaluation-deployment
      - distribution-shift
      - safety-practices
    type: neglected
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 4.5
    compact: 4.5
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
  - id: international-regimes-48
    insight: >-
      Only 7 of 193 UN member states participate in the seven most prominent AI governance initiatives, while 118
      countries (mostly in the Global South) are entirely absent from AI governance discussions as of late 2024.
    source: /knowledge-base/responses/governance/compute-governance/international-regimes/
    tags:
      - global-governance
      - participation-gaps
      - developing-countries
    type: neglected
    surprising: 4
    important: 4.5
    actionable: 4
    neglected: 4.5
    compact: 4
    added: 2026-01-23T00:00:00.000Z
    lastVerified: 2026-01-23T00:00:00.000Z
