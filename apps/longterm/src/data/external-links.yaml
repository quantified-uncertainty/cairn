# External Links Mapping
# Maps page entity IDs to their corresponding pages on external platforms
#
# Supported platforms:
#   - wikipedia: Wikipedia article URL
#   - lesswrong: LessWrong tag/wiki URL (format: https://www.lesswrong.com/tag/{slug})
#   - alignmentForum: Alignment Forum wiki URL (usually same as LessWrong)
#   - eaForum: EA Forum topic URL (format: https://forum.effectivealtruism.org/topics/{slug})
#
# To add a new mapping, add an entry with:
#   - pageId: The entity ID used in this wiki (matches the MDX filename without extension)
#   - links: Object containing platform -> URL mappings

# Core AI Safety Concepts
- pageId: situational-awareness
  links:
    lesswrong: https://www.lesswrong.com/tag/situational-awareness
    alignmentForum: https://www.alignmentforum.org/tag/situational-awareness

- pageId: deceptive-alignment
  links:
    lesswrong: https://www.lesswrong.com/tag/deceptive-alignment
    alignmentForum: https://www.alignmentforum.org/tag/deceptive-alignment

- pageId: mesa-optimization
  links:
    wikipedia: https://en.wikipedia.org/wiki/AI_alignment#Mesa-optimization
    lesswrong: https://www.lesswrong.com/tag/mesa-optimization

- pageId: inner-alignment
  links:
    lesswrong: https://www.lesswrong.com/tag/inner-alignment

- pageId: outer-alignment
  links:
    lesswrong: https://www.lesswrong.com/tag/outer-alignment

- pageId: corrigibility
  links:
    lesswrong: https://www.lesswrong.com/tag/corrigibility
    alignmentForum: https://www.alignmentforum.org/tag/corrigibility

- pageId: instrumental-convergence
  links:
    wikipedia: https://en.wikipedia.org/wiki/Instrumental_convergence
    lesswrong: https://www.lesswrong.com/tag/instrumental-convergence

- pageId: orthogonality-thesis
  links:
    wikipedia: https://en.wikipedia.org/wiki/Orthogonality_thesis
    lesswrong: https://www.lesswrong.com/tag/orthogonality-thesis

# Capabilities
- pageId: language-models
  links:
    wikipedia: https://en.wikipedia.org/wiki/Large_language_model
    lesswrong: https://www.lesswrong.com/tag/language-models-llms
    eaForum: https://forum.effectivealtruism.org/topics/large-language-models

- pageId: agentic-ai
  links:
    lesswrong: https://www.lesswrong.com/tag/agentic-ai
    eaForum: https://forum.effectivealtruism.org/topics/agentic-ai

- pageId: reasoning
  links:
    lesswrong: https://www.lesswrong.com/tag/reasoning

- pageId: tool-use
  links:
    lesswrong: https://www.lesswrong.com/tag/tool-use

- pageId: self-improvement
  links:
    lesswrong: https://www.lesswrong.com/tag/recursive-self-improvement

- pageId: scientific-research
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-for-science

# Risk Categories
- pageId: existential-risk
  links:
    wikipedia: https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence
    lesswrong: https://www.lesswrong.com/tag/existential-risk
    eaForum: https://forum.effectivealtruism.org/topics/existential-risk

- pageId: bioweapons
  links:
    wikipedia: https://en.wikipedia.org/wiki/Biological_warfare
    eaForum: https://forum.effectivealtruism.org/topics/global-catastrophic-biological-risk

- pageId: cyberweapons
  links:
    wikipedia: https://en.wikipedia.org/wiki/Cyberwarfare
    lesswrong: https://www.lesswrong.com/tag/computer-security-and-cryptography

- pageId: deepfakes
  links:
    wikipedia: https://en.wikipedia.org/wiki/Deepfake

- pageId: autonomous-weapons
  links:
    wikipedia: https://en.wikipedia.org/wiki/Lethal_autonomous_weapon

- pageId: racing-dynamics
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-arms-race
    eaForum: https://forum.effectivealtruism.org/topics/racing-to-the-precipice

- pageId: lock-in
  links:
    lesswrong: https://www.lesswrong.com/tag/value-lock-in
    eaForum: https://forum.effectivealtruism.org/topics/lock-in

- pageId: power-concentration
  links:
    lesswrong: https://www.lesswrong.com/tag/instrumental-convergence
    eaForum: https://forum.effectivealtruism.org/topics/concentration-of-power

# Safety Research Areas
- pageId: interpretability
  links:
    lesswrong: https://www.lesswrong.com/tag/interpretability-ml-and-ai
    eaForum: https://forum.effectivealtruism.org/topics/ai-interpretability

- pageId: scalable-oversight
  links:
    lesswrong: https://www.lesswrong.com/tag/scalable-oversight

- pageId: eliciting-latent-knowledge
  links:
    lesswrong: https://www.lesswrong.com/tag/eliciting-latent-knowledge

- pageId: ai-control
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-control

- pageId: agent-foundations
  links:
    lesswrong: https://www.lesswrong.com/tag/agent-foundations

- pageId: rlhf
  links:
    lesswrong: https://www.lesswrong.com/tag/rlhf

- pageId: constitutional-ai
  links:
    lesswrong: https://www.lesswrong.com/tag/constitutional-ai

- pageId: debate
  links:
    lesswrong: https://www.lesswrong.com/tag/debate-ai-safety-technique-1

# Governance & Policy
- pageId: ai-governance
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-governance
    eaForum: https://forum.effectivealtruism.org/topics/ai-governance

- pageId: compute-governance
  links:
    lesswrong: https://www.lesswrong.com/tag/compute-governance
    eaForum: https://forum.effectivealtruism.org/topics/compute-governance

- pageId: regulation
  links:
    lesswrong: https://www.lesswrong.com/tag/regulation-and-ai-risk
    eaForum: https://forum.effectivealtruism.org/topics/policy

- pageId: international-coordination
  links:
    eaForum: https://forum.effectivealtruism.org/topics/international-relations

# Timelines & Forecasting
- pageId: ai-timelines
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-timelines
    eaForum: https://forum.effectivealtruism.org/topics/ai-forecasting

- pageId: ai-takeoff
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-takeoff
    eaForum: https://forum.effectivealtruism.org/topics/ai-takeoff

- pageId: transformative-ai
  links:
    lesswrong: https://www.lesswrong.com/tag/transformative-ai
    eaForum: https://forum.effectivealtruism.org/topics/transformative-artificial-intelligence

- pageId: agi
  links:
    wikipedia: https://en.wikipedia.org/wiki/Artificial_general_intelligence
    lesswrong: https://www.lesswrong.com/tag/general-intelligence

- pageId: superintelligence
  links:
    wikipedia: https://en.wikipedia.org/wiki/Superintelligence
    lesswrong: https://www.lesswrong.com/tag/superintelligence

# Key Concepts
- pageId: goodharts-law
  links:
    wikipedia: https://en.wikipedia.org/wiki/Goodhart%27s_law
    lesswrong: https://www.lesswrong.com/tag/goodhart-s-law

- pageId: specification-gaming
  links:
    lesswrong: https://www.lesswrong.com/tag/specification-gaming

- pageId: reward-hacking
  links:
    lesswrong: https://www.lesswrong.com/tag/reward-hacking

- pageId: goal-misgeneralization
  links:
    lesswrong: https://www.lesswrong.com/tag/goal-misgeneralization

- pageId: scheming
  links:
    lesswrong: https://www.lesswrong.com/tag/scheming

- pageId: sycophancy
  links:
    lesswrong: https://www.lesswrong.com/tag/sycophancy

# Organizations (People pages would have Wikipedia links for notable researchers)
- pageId: openai
  links:
    wikipedia: https://en.wikipedia.org/wiki/OpenAI
    lesswrong: https://www.lesswrong.com/tag/openai

- pageId: anthropic
  links:
    wikipedia: https://en.wikipedia.org/wiki/Anthropic
    lesswrong: https://www.lesswrong.com/tag/anthropic-org

- pageId: deepmind
  links:
    wikipedia: https://en.wikipedia.org/wiki/DeepMind
    lesswrong: https://www.lesswrong.com/tag/google-deepmind

- pageId: miri
  links:
    wikipedia: https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute
    lesswrong: https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri

# People (Notable researchers with Wikipedia pages)
- pageId: eliezer-yudkowsky
  links:
    wikipedia: https://en.wikipedia.org/wiki/Eliezer_Yudkowsky
    lesswrong: https://www.lesswrong.com/tag/eliezer-yudkowsky

- pageId: nick-bostrom
  links:
    wikipedia: https://en.wikipedia.org/wiki/Nick_Bostrom
    lesswrong: https://www.lesswrong.com/tag/nick-bostrom

- pageId: stuart-russell
  links:
    wikipedia: https://en.wikipedia.org/wiki/Stuart_J._Russell
    lesswrong: https://www.lesswrong.com/tag/stuart-russell

# Philosophical Concepts
- pageId: longtermism
  links:
    wikipedia: https://en.wikipedia.org/wiki/Longtermism
    lesswrong: https://www.lesswrong.com/tag/longtermism
    eaForum: https://forum.effectivealtruism.org/topics/longtermism

- pageId: effective-altruism
  links:
    wikipedia: https://en.wikipedia.org/wiki/Effective_altruism
    lesswrong: https://www.lesswrong.com/tag/effective-altruism
    eaForum: https://forum.effectivealtruism.org/topics/building-effective-altruism

- pageId: decision-theory
  links:
    wikipedia: https://en.wikipedia.org/wiki/Decision_theory
    lesswrong: https://www.lesswrong.com/tag/decision-theory
    eaForum: https://forum.effectivealtruism.org/topics/decision-theory

# AI Safety Specific
- pageId: ai-safety
  links:
    wikipedia: https://en.wikipedia.org/wiki/AI_safety
    lesswrong: https://www.lesswrong.com/tag/ai
    eaForum: https://forum.effectivealtruism.org/topics/ai-safety

- pageId: alignment
  links:
    wikipedia: https://en.wikipedia.org/wiki/AI_alignment
    lesswrong: https://www.lesswrong.com/tag/ai
    eaForum: https://forum.effectivealtruism.org/topics/ai-alignment

# Evaluation & Benchmarks
- pageId: ai-evaluations
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-evaluations
    eaForum: https://forum.effectivealtruism.org/topics/ai-evaluations-and-standards

- pageId: dangerous-capability-evaluations
  links:
    lesswrong: https://www.lesswrong.com/tag/dangerous-capability-evaluations

# Field Building
- pageId: field-building
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-alignment-fieldbuilding
    eaForum: https://forum.effectivealtruism.org/topics/building-the-field-of-ai-safety

# Economic & Structural
- pageId: economic-disruption
  links:
    lesswrong: https://www.lesswrong.com/tag/economic-consequences-of-agi

- pageId: automation
  links:
    wikipedia: https://en.wikipedia.org/wiki/Automation
    lesswrong: https://www.lesswrong.com/tag/automation

# Miscellaneous Concepts
- pageId: scaling-laws
  links:
    lesswrong: https://www.lesswrong.com/tag/scaling-laws

- pageId: emergent-capabilities
  links:
    lesswrong: https://www.lesswrong.com/tag/emergent-behavior-emergence

- pageId: chain-of-thought
  links:
    lesswrong: https://www.lesswrong.com/tag/chain-of-thought-alignment

- pageId: sparse-autoencoders
  links:
    lesswrong: https://www.lesswrong.com/tag/sparse-autoencoders-saes

# Models/Frameworks
- pageId: sharp-left-turn
  links:
    lesswrong: https://www.lesswrong.com/tag/sharp-left-turn

- pageId: slow-takeoff
  links:
    lesswrong: https://www.lesswrong.com/tag/slow-takeoff

- pageId: fast-takeoff
  links:
    lesswrong: https://www.lesswrong.com/tag/fast-takeoff

# Threat Models
- pageId: treacherous-turn
  links:
    lesswrong: https://www.lesswrong.com/tag/treacherous-turn

- pageId: ai-boxing
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-boxing-containment

- pageId: oracle-ai
  links:
    lesswrong: https://www.lesswrong.com/tag/oracle-ai

- pageId: tool-ai
  links:
    lesswrong: https://www.lesswrong.com/tag/tool-ai

# Training & Methods
- pageId: reinforcement-learning
  links:
    wikipedia: https://en.wikipedia.org/wiki/Reinforcement_learning
    lesswrong: https://www.lesswrong.com/tag/reinforcement-learning

- pageId: value-learning
  links:
    lesswrong: https://www.lesswrong.com/tag/value-learning

- pageId: human-values
  links:
    lesswrong: https://www.lesswrong.com/tag/human-values

- pageId: utility-functions
  links:
    lesswrong: https://www.lesswrong.com/tag/utility-functions

# Safety Mechanisms
- pageId: sandboxing
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-boxing-containment

- pageId: model-auditing
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-evaluations

- pageId: red-teaming
  links:
    lesswrong: https://www.lesswrong.com/tag/red-teaming

# Risk Assessment
- pageId: catastrophic-risk
  links:
    lesswrong: https://www.lesswrong.com/tag/global-catastrophic-risk
    eaForum: https://forum.effectivealtruism.org/topics/global-catastrophic-risk

- pageId: s-risk
  links:
    lesswrong: https://www.lesswrong.com/tag/s-risk
    eaForum: https://forum.effectivealtruism.org/topics/s-risk

# Worldviews
- pageId: doomer
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-doomers

- pageId: optimistic
  links:
    lesswrong: https://www.lesswrong.com/tag/ai-optimism
