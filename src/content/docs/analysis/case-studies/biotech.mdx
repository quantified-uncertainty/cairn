---
title: "Biotechnology & Gain-of-Function Research"
description: Lessons from biotech governance, Asilomar Conference, and dual-use research debates
sidebar:
  order: 3
---

import { InfoBox } from '../../../../components/wiki';

<InfoBox
  type="case-study"
  title="Biotechnology & Gain-of-Function Research"
  customFields={[
    { label: "Timeline", value: "1970s-present" },
    { label: "Key Event", value: "Asilomar Conference (1975)" },
    { label: "Governance Established", value: "NIH Guidelines, BWC, local oversight" },
    { label: "Recent Crisis", value: "COVID-19 lab leak debates (2019-present)" },
    { label: "Current Status", value: "Ongoing dual-use research concerns" },
  ]}
/>

## Overview

Biotechnology and gain-of-function research provide crucial lessons about scientific self-governance, dual-use technology risks, and the challenges of regulating beneficial research that poses catastrophic risks. The biotech case study is particularly relevant to AI because both involve:

- Dual-use technologies (beneficial applications coupled with catastrophic risk potential)
- Rapid scientific advancement
- International diffusion of capabilities
- Debate over research restrictions vs. progress
- Challenges in distinguishing "safe" from "dangerous" research

## Development Timeline

### Discovery of Recombinant DNA (1973)

**1973**: Cohen and Boyer develop recombinant DNA techniques - ability to splice genes between organisms.

**Immediate Recognition of Risks**: Scientists quickly recognized potential for creating dangerous organisms.

**Key Insight**: Unlike many technologies, biotech risks were recognized almost immediately by the scientific community.

### Asilomar Conference (1975)

**February 1975**: 140 scientists, lawyers, and journalists met to discuss recombinant DNA risks.

**Unprecedented Step**: Scientists voluntarily called for moratorium on certain experiments.

**Outcomes**:
- Temporary moratorium on high-risk experiments
- Classification of experiments by risk level
- Containment requirements for different risk categories
- Commitment to ongoing oversight

**Historical Significance**: Often cited as successful scientific self-governance.

### NIH Guidelines Era (1976-1990s)

**1976**: NIH Recombinant DNA Guidelines established.

**Key Features**:
- Risk-based categorization of experiments
- Containment requirements (BSL-1 through BSL-4)
- Institutional Biosafety Committees (IBCs) at research institutions
- Mandatory compliance for NIH-funded research

**Effectiveness**:
- **Success**: Prevented accidents at major research institutions
- **Limitation**: Only covered NIH-funded research in US
- **Evolution**: Guidelines relaxed over time as technology seemed safer

### Commercialization and Diffusion (1980s-2000s)

**1980**: US Supreme Court allows patenting of organisms (Diamond v. Chakrabarty).

**1982**: First recombinant DNA drug (human insulin) approved.

**Consequence**: Massive commercial interest; biotech moves beyond academic labs.

**Pattern**: As commercial value increased, enthusiasm for restrictions decreased.

### Dual-Use Research of Concern (2000s-2010s)

**2001**: Anthrax attacks increase awareness of bioterrorism.

**2005**: H5N1 flu virus genome sequenced and published.

**2011-2012**: Controversial gain-of-function experiments on H5N1 flu:
- Fouchier and Kawaoka labs created highly transmissible H5N1 variants
- Debate over publication (eventually published with some details)
- Voluntary moratorium declared, then lifted

**2014-2017**: US moratorium on gain-of-function research after lab accidents.

**2017**: Moratorium lifted with new oversight framework (P3CO - Potential Pandemic Pathogen Care and Oversight).

## Warning Signs: What Was Heeded, What Wasn't

### Warnings That Led to Action

**Asilomar Concerns (1975)**
- Scientists recognized risks early
- **Response**: Guidelines, containment requirements, oversight committees
- **Success**: Established precedent for self-governance

**Publication of Dangerous Information**
- H5N1 gain-of-function experiments raised alarm
- **Response**: NSABB (National Science Advisory Board for Biosecurity) review process
- **Partial Success**: Some information redacted, though eventually published

**Lab Accidents**
- CDC anthrax exposure (2014), H5N1 contamination, smallpox vials found
- **Response**: US moratorium on gain-of-function research (2014-2017)
- **Limitation**: Moratorium temporary; oversight still limited

### Warnings Inadequately Addressed

**Diffusion of Dangerous Capabilities**
- CRISPR and gene synthesis make dangerous research more accessible
- **Response**: Limited; technology continues to proliferate
- **Gap**: International coordination minimal

**Engineered Pandemic Potential**
- Multiple reports warn of bioengineered pandemic risk
- **Response**: Some oversight, but research continues
- **Gap**: Economic and scientific incentives favor research over restriction

**Lab Leak Risks**
- Studies show lab accidents occur regularly
- **Response**: Safety improvements at major labs, but
- **Gap**: Proliferation to less-secure labs worldwide continues

**COVID-19 Lab Leak Debate**
- Possibility that pandemic originated from lab
- **Response**: Highly politicized; limited investigation
- **Gap**: Governance failures if lab leak occurred

## Governance Responses

### Mechanisms That Worked (Partially)

**1. Asilomar Conference and Scientific Self-Governance**

**Success**:
- Demonstrated scientists can proactively address risks
- Created template for risk-based research categorization
- Established containment and oversight norms

**Limitations**:
- Voluntary nature limited enforcement
- Guidelines relaxed over time
- Only covered academic research
- National rather than international

**AI Parallel**: AI researchers could similarly self-organize, but must maintain discipline over time.

**2. Institutional Biosafety Committees (IBCs)**

**Success**:
- Local oversight at research institutions
- Scientists reviewing peers' research
- Prevented many accidents at major institutions

**Limitations**:
- Variable quality across institutions
- Conflicts of interest (institution benefits from research)
- Only covers certain types of research
- No authority to stop research, just modify protocols

**AI Parallel**: AI labs could have internal safety committees, but same limitations apply.

**3. Biological Weapons Convention (BWC, 1975)**

**Success**:
- 185 nations committed to no biological weapons
- Norm against bioweapons established

**Limitations**:
- No verification mechanism (unlike nuclear treaties)
- Dual-use research makes compliance ambiguous
- Some nations likely violated treaty

**AI Parallel**: International AI treaty possible but verification challenges even greater.

**4. National Science Advisory Board for Biosecurity (NSABB)**

**Success**:
- Reviews dual-use research for publication risks
- Influenced redaction of some dangerous details

**Limitations**:
- Advisory only, no enforcement power
- US-only; international journals and research continue
- Science eventually published anyway

**AI Parallel**: AI capabilities can be published or kept secret; similar tradeoffs.

### Governance Failures and Gaps

**1. Proliferation of Dangerous Capabilities**

**Problem**: Gene synthesis, CRISPR, other tools now widely available.

**Failure**: No effective international control on diffusion.

**Consequence**: More actors can conduct dangerous research.

**AI Parallel**: AI capabilities proliferating even faster; same control challenges.

**2. Inadequate International Coordination**

**Problem**: Biotech research occurs worldwide with varying oversight.

**Failure**: No international equivalent of IAEA for biotech.

**Consequence**: Regulatory arbitrage; dangerous research moves to less-regulated jurisdictions.

**AI Parallel**: AI development even more internationalized; coordination critical but difficult.

**3. Commercial Incentives Overwhelm Safety**

**Problem**: Biotech industry worth hundreds of billions.

**Failure**: Economic incentives favored relaxing restrictions.

**Consequence**: Safety culture weakened over time from Asilomar era.

**AI Parallel**: Trillion-dollar AI market creates similar pressure to prioritize progress over safety.

**4. Publication Dilemma Unresolved**

**Problem**: Publishing dangerous information aids both beneficial research and potential misuse.

**Failure**: No clear policy on what should/shouldn't be published.

**Consequence**: Dangerous information (gain-of-function methods, synthesis techniques) widely available.

**AI Parallel**: AI capabilities and methods similarly dual-use; publication norms unclear.

**5. Lab Safety Failures Continue**

**Problem**: Even well-funded, regulated labs have accidents.

**Examples**: CDC anthrax exposure, H5N1 contamination, smallpox vials found in storage.

**Failure**: Perfect safety impossible even with strong oversight.

**AI Parallel**: Even well-intentioned AI labs may have accidents or leaks.

## COVID-19 and Lab Leak Debates

The COVID-19 pandemic, and debates over its potential lab origin, provide crucial lessons:

### What Happened

**2019-2020**: SARS-CoV-2 emerges in Wuhan, China; becomes global pandemic.

**Lab Leak Hypothesis**: Wuhan Institute of Virology conducted coronavirus research, including gain-of-function experiments, near outbreak origin.

**Evidence**:
- **For Natural Origin**: Virus structure, genetic similarity to wildlife coronaviruses, precedent of natural spillovers
- **For Lab Leak**: Proximity to WIV, prior lab accidents in China, WIV conducted risky research, transparency failures
- **Uncertainty**: Origin remains contested; insufficient investigation

### Governance Failures Revealed

**1. Transparency Failures**
- Chinese authorities limited investigation
- Lab records not made available
- International community lacked leverage

**2. Inadequate Oversight of Risky Research**
- If lab leak, governance failed to prevent
- Gain-of-function research oversight gaps
- International coordination minimal

**3. Politicization of Scientific Questions**
- Lab leak hypothesis became politically charged
- Scientists dismissed possibility prematurely
- Polarization hindered objective investigation

**4. No Accountability Mechanisms**
- Even if lab leak, no international body to investigate or enforce consequences
- BWC lacks verification provisions

### Lessons for AI

**1. Accidents Can Have Global Consequences**
- Single lab, single incident, global catastrophe
- **AI Parallel**: AI accident could similarly propagate globally

**2. Transparency Critical But Difficult to Ensure**
- Without transparency, impossible to learn from accidents
- **AI Parallel**: AI labs should commit to incident reporting, transparency

**3. International Governance Gaps Are Dangerous**
- No international authority could investigate or enforce
- **AI Parallel**: Need international AI safety authority with actual power

**4. Science Can Be Politicized in Crisis**
- Objective inquiry suffered due to political dynamics
- **AI Parallel**: AI safety debates should be depoliticized before crisis

**5. Prevention Better Than Response**
- If lab leak, governance failed to prevent
- **AI Parallel**: Proactive AI safety essential; reactive response insufficient

## Lessons for AI Governance

### What Worked: Transferable Strategies

**1. Scientific Self-Governance Is Possible (With Caveats)**
- Asilomar showed scientists can recognize and address risks
- **Application**: AI researchers should proactively organize
- **Caveat**: Discipline must be maintained as commercial incentives grow
- **Example**: AI safety conferences, model release norms

**2. Risk-Based Categorization**
- Experiments categorized by risk level (BSL-1 through BSL-4)
- **Application**: AI capabilities could be similarly categorized
- **Example**: Distinguish general-purpose AI from dual-use or dangerous capabilities

**3. Containment and Access Restrictions**
- High-risk experiments in secure facilities with limited access
- **Application**: Advanced AI models with restricted access, monitoring
- **Example**: Staged release, red-teaming before deployment

**4. Institutional Oversight Committees**
- IBCs review risky research at local level
- **Application**: AI labs could have internal safety review boards
- **Caveat**: Same conflicts of interest; need external oversight too

**5. Publication Norms Can Evolve**
- Debate over H5N1 publication led to more careful consideration
- **Application**: AI community developing norms around capability publication
- **Example**: Responsible disclosure of vulnerabilities

**6. International Norms Have Value Even Without Verification**
- BWC established norm against bioweapons despite no enforcement
- **Application**: AI safety norms worth establishing even if imperfectly enforced
- **Example**: Partnership on AI, OECD AI Principles

### What Didn't Work: Warnings for AI

**1. Voluntary Governance Erodes Under Commercial Pressure**
- Asilomar spirit faded as biotech industry grew
- **Warning**: AI companies face trillion-dollar incentives to prioritize speed over safety
- **Implication**: Voluntary governance insufficient; regulation necessary

**2. Dual-Use Makes Restriction Extremely Hard**
- Can't ban beneficial biotech research due to catastrophic risk
- **Warning**: AI even more dual-use; almost every capability has beneficial applications
- **Implication**: Need approaches beyond simple bans

**3. Capabilities Proliferate Despite Oversight**
- Gene synthesis, CRISPR now widely accessible
- **Warning**: AI capabilities proliferating faster; control even harder
- **Implication**: Focus on safe proliferation, not just prevention

**4. Local Oversight Has Conflicts of Interest**
- IBCs funded by institutions that benefit from research
- **Warning**: AI lab internal safety teams face same pressures
- **Implication**: External, independent oversight essential

**5. International Coordination Minimal**
- No IAEA-equivalent for biotech
- **Warning**: AI development even more international; coordination gap dangerous
- **Implication**: International AI safety authority urgently needed

**6. Lab Accidents Happen Despite Best Efforts**
- Even CDC, NIH have had accidents
- **Warning**: AI accidents likely even at well-intentioned labs
- **Implication**: Assume accidents will occur; need robust incident response

**7. Crisis Response Often Inadequate**
- COVID-19 origin investigation limited by lack of international authority
- **Warning**: After AI catastrophe, may be unable to investigate or prevent recurrence
- **Implication**: Build accountability mechanisms before crisis

### Unique AI Challenges Beyond Biotech

**1. Faster Proliferation**
- Biotech requires labs, materials, expertise
- AI requires compute, data, code (easier to copy and diffuse)
- **Challenge**: AI governance must act even faster

**2. Autonomous Behavior**
- Biotech creations (viruses, etc.) follow biological rules
- AI systems may have autonomous goals and strategies
- **Challenge**: Risk model more complex for AI

**3. Digital vs. Physical Containment**
- Biotech contained by physical barriers (BSL labs)
- AI contained by... access controls? Monitoring? Less clear
- **Challenge**: Containment strategies less proven for AI

**4. Recursive Self-Improvement**
- Biotech doesn't improve itself
- AI could potentially self-improve rapidly
- **Challenge**: Unprecedented risk dynamic

**5. Broader Applications**
- Biotech primarily research and medicine
- AI applies to nearly every domain
- **Challenge**: Harder to regulate comprehensively

## Recommendations for AI Based on Biotech Case

### Immediate Actions

1. **AI Safety Conference (New Asilomar)**
   - Convene leading AI researchers to agree on safety norms
   - Establish risk categorization framework
   - Commit to containment measures for high-risk systems

2. **Internal Safety Review Boards**
   - Every AI lab should have independent safety committee
   - Review risky capabilities before deployment
   - Authority to delay or modify releases

3. **Responsible Publication Norms**
   - AI community should develop norms for publishing dangerous capabilities
   - Balance openness with safety
   - Consider staged release for high-risk models

4. **Incident Reporting Systems**
   - Establish confidential AI incident reporting
   - Learn from failures without punitive focus
   - Share lessons across organizations

### Medium-Term Institutional Development

5. **External Oversight Bodies**
   - Government or international bodies to audit AI safety
   - Avoid conflicts of interest of internal review
   - Model on NSABB but with more authority

6. **Risk-Based Access Controls**
   - High-risk AI capabilities in secure environments
   - Monitor access and use
   - Analogue to BSL-3/BSL-4 labs

7. **International Coordination**
   - Create international AI safety organization
   - Unlike biotech, establish verification mechanisms
   - Learn from BWC failure to include enforcement

8. **Safety Culture Maintenance**
   - Actively counter commercial pressure to cut safety corners
   - Celebrate safety successes, not just capability advances
   - Maintain Asilomar spirit as AI industry grows

### Long-Term Governance Goals

9. **International AI Safety Treaty**
   - Binding commitments on dangerous capabilities
   - Unlike BWC, include verification and enforcement
   - Balance innovation with catastrophic risk prevention

10. **Mandatory Safety Standards**
    - Don't rely solely on voluntary measures
    - Regulations for high-risk AI systems
    - Analogue to biosafety levels, but legally required

11. **Proliferation Management**
    - Accept that AI capabilities will proliferate
    - Focus on safe proliferation (safety features baked in)
    - Monitor and audit, even if can't fully prevent

12. **Accountability for Catastrophic Failures**
    - Unlike biotech, establish clear accountability for AI catastrophes
    - International investigation authority
    - Learn lessons even (especially) when politically uncomfortable

## Conclusion

The biotechnology case study provides both inspiration and warning for AI governance. Asilomar demonstrated that scientific communities can proactively address catastrophic risks, risk-based frameworks can guide research decisions, and containment measures can prevent many accidents.

However, the biotech case also reveals critical failures: voluntary governance eroded under commercial pressure, international coordination remained minimal, dual-use technologies proliferated despite risks, and lab accidents continued even at well-regulated institutions. Most sobering, if COVID-19 did originate from a lab, governance catastrophically failed to prevent a pandemic.

**The central lessons for AI**:

1. **Organize proactively**: AI researchers should emulate Asilomar's early self-governance, but with stronger institutions
2. **External oversight is essential**: Internal safety review faces conflicts of interest
3. **International coordination is urgent**: Unlike biotech, AI needs strong international governance before catastrophe
4. **Plan for proliferation**: Can't prevent AI capabilities from spreading; must ensure safe proliferation
5. **Maintain safety culture**: Commercial pressure will increase; discipline must be sustained
6. **Build accountability now**: Establish mechanisms to investigate and learn from AI incidents before crisis

Unlike biotech, AI development is happening faster, with more commercial pressure, and potentially more catastrophic risks. We must learn from biotech's successes and failures, but act more decisively and rapidly to build governance that can keep pace with AI advancement.

