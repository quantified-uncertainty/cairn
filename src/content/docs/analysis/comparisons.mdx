---
title: Comparison Tables
description: Side-by-side comparisons of AI labs, safety approaches, and key positions
sidebar:
  order: 3
tableOfContents: false
---

import { ComparisonTable, DataDisagreementMap } from '../../../components/wiki';


This page provides side-by-side comparisons to help understand the landscape of AI safety organizations, approaches, and expert positions.

:::note[Data-Driven]
Expert position maps on this page pull data automatically from our structured data layer.
:::

## Frontier Labs Comparison

<ComparisonTable
  client:load
  title="Frontier AI Labs"
  columns={["Safety Focus", "Approach", "Timeline View", "P(doom) Range", "Key Safety Work"]}
  rows={[
    {
      name: "Anthropic",
      link: "/organizations/anthropic",
      values: {
        "Safety Focus": { value: "Very High", badge: "high" },
        "Approach": "Constitutional AI, RSP, Interpretability",
        "Timeline View": "Short (2026-2030)",
        "P(doom) Range": "10-25%",
        "Key Safety Work": "Constitutional AI, Scaling Monosemanticity, Sleeper Agents"
      }
    },
    {
      name: "OpenAI",
      link: "/organizations/openai",
      values: {
        "Safety Focus": { value: "Medium", badge: "medium" },
        "Approach": "Iterative deployment, RLHF, Superalignment (dissolved)",
        "Timeline View": "Short (2027-2030)",
        "P(doom) Range": "5-20%",
        "Key Safety Work": "GPT safety tuning, Weak-to-strong generalization"
      }
    },
    {
      name: "Google DeepMind",
      link: "/organizations/deepmind",
      values: {
        "Safety Focus": { value: "Medium", badge: "medium" },
        "Approach": "Scalable oversight, Debate, Formal verification",
        "Timeline View": "Medium (2028-2035)",
        "P(doom) Range": "5-15%",
        "Key Safety Work": "Interpretability, Debate, Specification gaming"
      }
    },
    {
      name: "Meta AI",
      values: {
        "Safety Focus": { value: "Lower", badge: "low" },
        "Approach": "Open source, RLHF",
        "Timeline View": "Varied",
        "P(doom) Range": "under 5%",
        "Key Safety Work": "Llama safety tuning, Red teaming"
      }
    },
    {
      name: "xAI",
      values: {
        "Safety Focus": { value: "Lower", badge: "low" },
        "Approach": "Maximum truth-seeking",
        "Timeline View": "Short",
        "P(doom) Range": "Unknown",
        "Key Safety Work": "Limited public safety research"
      }
    }
  ]}
/>

## Safety Research Organizations

<ComparisonTable
  client:load
  title="Safety Research Organizations"
  columns={["Focus Area", "Approach", "Size", "Key Outputs"]}
  rows={[
    {
      name: "MIRI",
      link: "/organizations/miri",
      values: {
        "Focus Area": "Agent foundations, theoretical alignment",
        "Approach": "Fundamental research, deconfusion",
        "Size": "~10-15 researchers",
        "Key Outputs": "Decision theory, embedded agency, Alignment Forum"
      }
    },
    {
      name: "ARC",
      link: "/organizations/arc",
      values: {
        "Focus Area": "Alignment research, evaluations",
        "Approach": "Empirical + theoretical",
        "Size": "~5-10 researchers",
        "Key Outputs": "Eliciting latent knowledge, ARC evals"
      }
    },
    {
      name: "Redwood Research",
      link: "/organizations/redwood",
      values: {
        "Focus Area": "AI control, adversarial robustness",
        "Approach": "Empirical safety research",
        "Size": "~20 staff",
        "Key Outputs": "AI control protocols, causal scrubbing"
      }
    },
    {
      name: "CHAI (Berkeley)",
      link: "/organizations/chai",
      values: {
        "Focus Area": "Human-compatible AI",
        "Approach": "Academic research",
        "Size": "~20-30 researchers",
        "Key Outputs": "Inverse reward design, CIRL"
      }
    },
    {
      name: "CAIS",
      link: "/organizations/cais",
      values: {
        "Focus Area": "Catastrophic AI risk reduction",
        "Approach": "Field-building, research",
        "Size": "~15-20 staff",
        "Key Outputs": "MMLU benchmark, AI safety statement"
      }
    }
  ]}
/>

## Technical Safety Approaches

<ComparisonTable
  client:load
  title="Technical Safety Approaches Compared"
  columns={["Tractability", "Scalability", "When Useful", "Key Limitation"]}
  highlightColumn="Scalability"
  rows={[
    {
      name: "Mechanistic Interpretability",
      link: "/safety-approaches/technical/interpretability",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "Unknown", badge: "medium" },
        "When Useful": "Detecting deception, verifying alignment",
        "Key Limitation": "May not scale to frontier models"
      }
    },
    {
      name: "RLHF",
      link: "/safety-approaches/technical/rlhf",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "Medium", badge: "medium" },
        "When Useful": "Making models helpful and harmless",
        "Key Limitation": "Reward hacking, doesn't solve inner alignment"
      }
    },
    {
      name: "Constitutional AI",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "Medium", badge: "medium" },
        "When Useful": "Self-supervised safety training",
        "Key Limitation": "Constitution may be incomplete or gamed"
      }
    },
    {
      name: "AI Control",
      link: "/safety-approaches/technical/ai-control",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "High", badge: "high" },
        "When Useful": "Near-term deployment of untrusted systems",
        "Key Limitation": "Doesn't solve alignment, just contains it"
      }
    },
    {
      name: "Agent Foundations",
      link: "/safety-approaches/technical/agent-foundations",
      values: {
        "Tractability": { value: "Low", badge: "low" },
        "Scalability": { value: "Unknown", badge: "low" },
        "When Useful": "Fundamental understanding of agency",
        "Key Limitation": "Progress is slow, unclear if needed"
      }
    },
    {
      name: "Scalable Oversight",
      link: "/safety-approaches/technical/scalable-oversight",
      values: {
        "Tractability": { value: "Medium", badge: "medium" },
        "Scalability": { value: "High", badge: "high" },
        "When Useful": "Supervising superhuman AI",
        "Key Limitation": "May inherit evaluator biases"
      }
    }
  ]}
/>

## Governance Approaches

<ComparisonTable
  client:load
  title="AI Governance Approaches"
  columns={["Feasibility", "Speed", "Scope", "Main Challenge"]}
  rows={[
    {
      name: "Compute Governance",
      link: "/policies/compute-governance",
      values: {
        "Feasibility": { value: "High", badge: "high" },
        "Speed": "Can implement now",
        "Scope": "Hardware/infrastructure",
        "Main Challenge": "Circumvention, international coordination"
      }
    },
    {
      name: "International Treaties",
      link: "/safety-approaches/governance/international",
      values: {
        "Feasibility": { value: "Low", badge: "low" },
        "Speed": "Years to negotiate",
        "Scope": "Global",
        "Main Challenge": "US-China tensions, verification"
      }
    },
    {
      name: "National Regulation",
      values: {
        "Feasibility": { value: "Medium", badge: "medium" },
        "Speed": "1-3 years",
        "Scope": "Single country",
        "Main Challenge": "Regulatory capture, race to bottom"
      }
    },
    {
      name: "Lab Self-Governance (RSPs)",
      values: {
        "Feasibility": { value: "High", badge: "high" },
        "Speed": "Can implement now",
        "Scope": "Participating labs only",
        "Main Challenge": "Voluntary, competitive pressure"
      }
    },
    {
      name: "Licensing/Certification",
      values: {
        "Feasibility": { value: "Medium", badge: "medium" },
        "Speed": "1-3 years",
        "Scope": "Frontier development",
        "Main Challenge": "Defining requirements, enforcement"
      }
    }
  ]}
/>

## Expert Positions on Key Cruxes

### Timeline Estimates

<DataDisagreementMap
  client:load
  topic="timelines"
  description="Expert estimates for transformative AI timelines"
  spectrum={{ low: "2060+", high: "2025-2030" }}
/>

### P(doom) Estimates

<DataDisagreementMap
  client:load
  topic="p-doom"
  description="Expert estimates of existential risk from AI"
  spectrum={{ low: "under 1%", high: ">50%" }}
/>

### Alignment Tractability

<DataDisagreementMap
  client:load
  topic="how-hard-is-alignment-"
  description="Expert views on whether current approaches can solve alignment"
  spectrum={{ low: "Extremely hard", high: "Tractable" }}
/>

## Worldview Comparison

<ComparisonTable
  client:load
  title="AI Risk Worldviews"
  columns={["Timelines", "P(doom)", "Primary Concern", "Recommended Action"]}
  rows={[
    {
      name: "AI Doomer",
      link: "/understanding-ai-risk/worldviews/doomer",
      values: {
        "Timelines": "Short (5-15 years)",
        "P(doom)": "30-90%",
        "Primary Concern": "Misalignment, deceptive AI",
        "Recommended Action": "Slow down, solve alignment first"
      }
    },
    {
      name: "Concerned Researcher",
      values: {
        "Timelines": "Medium (10-30 years)",
        "P(doom)": "5-20%",
        "Primary Concern": "Multiple risks, uncertainty",
        "Recommended Action": "Technical safety + governance"
      }
    },
    {
      name: "Governance-Focused",
      link: "/understanding-ai-risk/worldviews/governance-focused",
      values: {
        "Timelines": "Varied",
        "P(doom)": "5-15%",
        "Primary Concern": "Misuse, concentration, racing",
        "Recommended Action": "International coordination, regulation"
      }
    },
    {
      name: "Long-Termist Optimist",
      link: "/understanding-ai-risk/worldviews/optimistic",
      values: {
        "Timelines": "Long (30+ years)",
        "P(doom)": "under 5%",
        "Primary Concern": "Missing AI's benefits",
        "Recommended Action": "Accelerate responsibly"
      }
    }
  ]}
/>

