---
title: "Will We Fail to Coordinate?"
description: "This analysis examines whether competitive pressures will prevent safe AI development. Evidence suggests 40-60% probability of adequate coordination, with key variables including US-China relations, regulatory pace, and industry self-governance through mechanisms like the Frontier Model Forum."
sidebar:
  order: 7
importance: 82.5
quality: 78
lastEdited: "2025-12-28"
llmSummary: "Analyzes coordination failure as a critical AI risk pathway, finding 40-60% probability of adequate coordination based on US-China relations, regulatory pace, and industry self-governance. Provides quantitative comparison of historical coordination attempts (Montreal Protocol 99% success vs. BWC failure) and current AI governance developments through 2024-2025."
---
import {InfoBox, EstimateBox, DisagreementMap, KeyQuestions, Mermaid, R} from '../../../../components/wiki';

<InfoBox
  type="crux"
  title="Coordination Failure"
  customFields={[
    { label: "Claim", value: "Competitive pressures will prevent safe AI development" },
    { label: "Key Uncertainty", value: "Can labs, nations, and institutions coordinate?" },
    { label: "Related To", value: "Governance, Racing Dynamics, Timelines" },
  ]}
/>

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Current coordination level** | Moderate but fragmented | 11 countries in International Network of AI Safety Institutes; 16 companies signed Seoul Frontier AI Safety Commitments (May 2024) |
| **US-China cooperation** | Minimal but not absent | First intergovernmental AI dialogue held May 2024; November 2024 agreement on human control of nuclear weapons decisions |
| **Regulatory pace** | Lagging but accelerating | EU AI Act entered force August 2024; US policy fragmented between administrations |
| **Industry self-governance** | Promising but voluntary | Frontier Model Forum members committed over \$10 million to AI Safety Fund; all members published safety frameworks by February 2025 |
| **Historical precedent** | Mixed success | Montreal Protocol achieved 99% phase-out of ozone-depleting substances with universal ratification; nuclear non-proliferation partial (9 nuclear states) |
| **Expert consensus** | Wide disagreement | Estimates range from less than 20% to greater than 80% probability of successful coordination |
| **Key bottleneck** | Verification and trust | AI capabilities harder to verify than nuclear/chemical weapons; dual-use nature complicates monitoring |

**The core question**: Even if alignment is technically possible, will competitive pressures between labs and nations lead to racing, corner-cutting on safety, and ultimately catastrophe?

This is the "social" dimension of AI risk. Even with perfect technical solutions, coordination failures could prevent their adoption.

## The Coordination Failures

<Mermaid client:load chart={`
flowchart TD
    RACE[Racing Dynamics] --> LAB[Lab Competition]
    RACE --> NATION[Nation Competition]
    LAB --> CORNER[Safety Corner-Cutting]
    NATION --> CORNER
    CORNER --> DEPLOY[Premature Deployment]

    PROLIF[Proliferation] --> OPEN[Open Source Release]
    PROLIF --> STATE[State Actor Access]
    OPEN --> SPREAD[Capability Spread]
    STATE --> SPREAD

    GOV[Governance Failure] --> LAG[Regulatory Lag]
    GOV --> FRAG[Jurisdictional Fragmentation]
    LAG --> WEAK[Weak Oversight]
    FRAG --> WEAK

    DEPLOY --> RISK[Increased Risk]
    SPREAD --> RISK
    WEAK --> RISK

    style RACE fill:#ffcccc
    style PROLIF fill:#ffcccc
    style GOV fill:#ffcccc
    style RISK fill:#ff9999
    style CORNER fill:#ffddcc
    style SPREAD fill:#ffddcc
    style WEAK fill:#ffddcc
`} />

<EstimateBox
  client:load
  variable="Coordination Failure Modes"
  description="Ways coordination could fail in AI development"
  unit=""
  estimates={[
    { source: "Lab Racing", value: "High Risk", notes: "Companies cut safety corners to ship first" },
    { source: "Nation Racing", value: "High Risk", notes: "Countries deprioritize safety for strategic advantage" },
    { source: "Proliferation", value: "Medium Risk", notes: "Dangerous capabilities spread to bad actors" },
    { source: "Regulatory Failure", value: "High Risk", notes: "Governance can't keep up with technology" },
    { source: "Value Disagreement", value: "Medium Risk", notes: "No consensus on what 'safe' means" },
  ]}
/>

### The Tragedy of the Commons

**The structure**:
- Safety is a public good: everyone benefits if AI is safe
- But safety is costly: slower development, fewer features
- Each actor benefits by cutting corners while others are safe
- If everyone cuts corners, everyone loses

**Classic prisoner's dilemma**: Cooperation is collectively optimal, but defection is individually rational.

### First-Mover Advantage

**The argument**: Massive incentives to be first.

**Benefits of being first**:
- Capture market before competitors
- Accumulate more data â†’ better models
- Set industry standards
- Achieve "moat" through scale
- Potentially achieve decisive strategic advantage

**Implication**: Safety measures that slow you down feel costly, even if they're globally optimal.

## Arguments for Coordination Failure

### 1. History of Coordination Failures

**The evidence**: Humans have repeatedly failed at similar problems.

**Examples**:
| Domain | Coordination Attempt | Quantitative Result | Key Lesson for AI |
|--------|---------------------|---------------------|-------------------|
| Nuclear weapons | <R id="c9650d862aaac40d">Non-Proliferation Treaty</R> (1968) | 9 nuclear states vs. 5 in 1968; 191 parties but India, Pakistan, Israel, North Korea outside | Verification matters; holdouts undermine regime |
| Climate change | <R id="03af8f147205d972">Paris Agreement</R> (2015) | 195 parties; emissions rose 1.2% in 2024; 2.7C trajectory vs. 1.5C target | Voluntary commitments insufficient without enforcement |
| Ozone layer | <R id="23c5f6c21fca78b6">Montreal Protocol</R> (1987) | 198 parties (first universal treaty); 99% ODS phase-out; ozone recovery by 2040 projected | Success possible with clear threat, verifiable actions, economic alternatives |
| Financial regulation | Basel III (2010-2023) | Implementation varies by jurisdiction; 2023 SVB failure despite rules | Regulatory arbitrage between jurisdictions |
| Bioweapons | <R id="06679c8d368585ee">BWC</R> (1972) | 187 parties; no verification regime; 4 staff members; budget smaller than average McDonald's | Weak institutions fail; verification remains critical gap |

**Key insight**: Coordination is possible but difficult and often incomplete. The <R id="81b14196c8af1e9d">Montreal Protocol's success</R> (99% reduction with universal participation) shows what's achievable when threats are clear, actions are verifiable, and economic alternatives exist. AI coordination lacks all three conditions.

### 2. US-China Competition

**The dynamic**: The two leading AI powers are geopolitical rivals.

**Current state of US-China AI relations** (as of late 2024):

| Development | Date | Significance |
|-------------|------|--------------|
| <R id="6efc39d4b532521d">First intergovernmental AI dialogue</R> | May 2024 | Geneva meeting; no concrete deliverables but opened channel |
| <R id="ab22aa0df9b1be7b">Nuclear weapons AI agreement</R> | November 2024 | Biden-Xi APEC summit: humans should control nuclear decisions |
| <R id="25ca111eea083021">UN AI resolutions</R> | March-June 2024 | Both countries supported each other's AI governance resolutions |
| <R id="7629a035e7e22ee1">Paris AI Summit divergence</R> | February 2025 | US and UK declined to sign inclusive AI declaration; China proposed rival Shanghai-based governance body |

**The problem**:
- Neither wants to fall behind (China produced an estimated 200,000-300,000 AI chips in 2025 vs. Nvidia's 4-5 million)
- Each fears the other will defect on safety
- Limited trust for verification (AI capabilities harder to verify than nuclear arsenals)
- Different values and governance systems
- Dual-use makes civilian/military separation hard

**The parallel**: Cold War arms race, but with AI instead of nukes.

**Key question**: Can rivals cooperate on existential risk? The <R id="ab22aa0df9b1be7b">RAND analysis</R> argues the November 2024 nuclear AI agreement "could serve as a foundation for future discussions about potential areas of mutual restraint."

### 3. Regulatory Lag

**The argument**: Government can't keep up with AI progress.

**Current regulatory landscape**:

| Jurisdiction | Framework | Status | Key Provisions |
|--------------|-----------|--------|----------------|
| <R id="1ad6dc89cded8b0c">EU AI Act</R> | Comprehensive risk-based regulation | Entered force August 2024; full application August 2026 | Prohibited uses (Feb 2025); GPAI rules (Aug 2025); high-risk systems (Aug 2026); fines up to EUR 35M or 7% global revenue |
| US (Biden) | Executive Order 14110 | October 2023 | Safety requirements for frontier models; NIST standards |
| US (Trump) | AI Action Plan | July 2025 | Shift toward deregulation; links federal funding to less restrictive state laws |
| UK | Principles-based approach | Ongoing | Sector-specific regulation; no comprehensive AI law |
| China | Multiple regulations | 2023-2025 | Algorithmic recommendations, deep synthesis, generative AI rules |

**Reasons for lag**:
- Technical complexity overwhelms regulators
- Revolving door with industry
- Lobbying against regulation
- Jurisdictional fragmentation (27 EU member states now <R id="367b0430008c3a97">developing national implementation plans</R>)
- Speed of AI progress

**Evidence**: The EU AI Act, the world's first comprehensive AI law, took 3+ years to negotiate and will take until 2027 for full implementation. GPT-4 was released during negotiations. By full implementation, AI capabilities may have advanced 2-3 generations.

### 4. Open Source and Proliferation

**The dilemma**: Open source democratizes AI but spreads dangerous capabilities.

**The case for open source**:
- Prevents monopoly
- Enables independent safety research
- Allows broader scrutiny
- Reduces concentration of power

**The case against**:
- Can't take back released capabilities
- Bad actors get access
- Can't control downstream use
- Safety features can be removed

**The meta-coordination problem**: Even if major labs agree on safety, open source efforts may not.

### 5. Value Disagreement

**The challenge**: No consensus on what "safe" or "aligned" means.

**Disagreements**:
- Whose values should AI reflect?
- How much autonomy should AI have?
- What risks are acceptable?
- Who decides deployment?

**Implication**: Even with will to coordinate, coordination on *what* is hard.

<DisagreementMap
  client:load
  topic="Can We Coordinate on AI Safety?"
  description="Expert views on the tractability of AI coordination"
  spectrum={{ low: "Coordination will fail", high: "Coordination achievable" }}
  positions={[
    { actor: "Pessimists", position: "Will fail", estimate: "Less than 20% succeed", confidence: "medium" },
    { actor: "MIRI", position: "Very difficult", estimate: "20-30% succeed", confidence: "medium" },
    { actor: "Policy researchers", position: "Challenging but possible", estimate: "40-60% succeed", confidence: "medium" },
    { actor: "Governance optimists", position: "Achievable with effort", estimate: "60-80% succeed", confidence: "medium" },
    { actor: "Tech optimists", position: "Will figure it out", estimate: "80%+ succeed", confidence: "low" },
  ]}
/>

## Arguments for Successful Coordination

### 1. Mutual Destruction Motivates Cooperation

**The argument**: Everyone loses from catastrophe, so everyone has incentive to cooperate.

**The logic**:
- AI catastrophe is bad for all actors
- All parties know this
- Therefore, cooperation is mutually beneficial
- Self-interest aligns with coordination

**Historical parallel**: Cold War nuclear deterrence (we didn't have nuclear war).

**Counter**: This requires everyone to understand the risk. Disagreement about risk level undermines coordination.

### 2. Small Number of Key Actors

**The argument**: Only a few frontier labs to coordinate.

**Current frontier labs**: Anthropic, OpenAI, Google DeepMind, (Meta AI, xAI, others emerging)

**Advantages of small N**:
- Easier to negotiate
- Easier to monitor
- Easier to build trust
- Can make binding agreements

**Historical parallel**: Nuclear non-proliferation works better with fewer players.

**Counter**: More actors entering. Open source creates many actors.

### 3. Visible Progress on Cooperation

**The evidence**: Coordination efforts are forming.

**Current efforts** (as of late 2024):

| Initiative | Type | Participants | Key Achievements |
|------------|------|--------------|------------------|
| <R id="a65ad4f1a30f1737">International Network of AI Safety Institutes</R> | International | 11 countries + EU | Launched November 2024; Joint Evaluation Protocol; Global AI Incident Database |
| <R id="4c0cce743341851e">Bletchley Declaration</R> | International | 28 countries (Nov 2023) | First global AI safety agreement; established summit series |
| <R id="73d81d3ead01bc0e">Seoul Declaration</R> | International | 27 countries + EU (May 2024) | Expanded to "safe, innovative, and inclusive AI"; China notably did not sign |
| <R id="51e8802a5aef29f6">Frontier Model Forum</R> | Industry | Anthropic, Google, Microsoft, OpenAI | Over \$10M AI Safety Fund; all members published safety frameworks by Feb 2025 |
| <R id="4487a62bbc1c45d6">Seoul Frontier AI Safety Commitments</R> | Industry | 16 companies | Commitment to publish safety frameworks; intolerable risk thresholds; external reporting |
| <R id="acc5ad4063972046">EU AI Act</R> | Regulation | 27 EU members | World's first comprehensive AI law; Amazon, Google, Microsoft, OpenAI, Anthropic signed Code of Practice |

**Key question**: Are these efforts sufficient? The <R id="0572f91896f52377">CSIS analysis</R> notes the network "lacks formal mechanisms for coordinating with frontier AI developers" and needs clearer priorities.

### 4. Lab Incentives Favor Safety

**The argument**: Labs genuinely want safe AI, not just fast AI.

**Reasons**:
- Reputation matters
- Liability concerns
- Founders care about impact
- Safety is a selling point
- Don't want to cause catastrophe

**Evidence**: Major labs all have safety teams, publish safety research.

**Counter**: Competitive pressure may override internal incentives.

### 5. Historical Precedents of Success

**Examples of successful coordination**:

| Domain | Mechanism | Quantitative Outcome | AI-Relevant Features |
|--------|-----------|---------------------|---------------------|
| Ozone layer | <R id="f0c9caf8e366215e">Montreal Protocol</R> (1987) | 99% ODS phase-out; 198 parties; \$3B Multilateral Fund; 0.5C warming avoided by 2100 | Clear threat; verifiable; economic alternatives existed; universal participation |
| Smallpox | WHO campaign (1967-1980) | 100% eradication; last case 1977 | Ring vaccination strategy; clear endpoint; no animal reservoir |
| Nuclear taboo | Norm + deterrence | No use in warfare since 1945 (80 years) | Existential stakes; small number of actors; verification (IAEA has 2,500+ staff) |
| Internet governance | IETF, ICANN, RIRs | 5.5B users; 99.9%+ uptime for core infrastructure | Technical community self-governance; rough consensus model |
| Aviation safety | ICAO + national agencies | 0.07 fatal accidents per million flights (2023) | Clear liability; reputation matters; shared standards |

**Key insight**: Coordination *can* work, especially when:
- Risks are clear (ozone: measurable depletion; nuclear: existential threat)
- Costs of cooperation are manageable (CFC alternatives existed)
- Verification is possible (IAEA inspections; atmospheric monitoring)
- Small number of key actors (5-9 nuclear powers; major CFC producers)

**Challenge for AI**: AI development lacks clear thresholds (what capability is "dangerous"?), alternatives to racing are unclear, and verification of AI capabilities is fundamentally harder than physical measurements.

## Current Coordination Efforts

<KeyQuestions
  client:load
  questions={[
    {
      question: "Can major AI labs effectively coordinate on safety?",
      positions: [
        {
          position: "Yes - alignment of interests enables cooperation",
          confidence: "medium",
          reasoning: "Labs share interest in avoiding catastrophe. Small number of frontier actors. Already seeing coordination (Frontier Model Forum, RSPs). Reputation incentives matter.",
          implications: "Industry self-regulation can work; focus on making it robust"
        },
        {
          position: "Partially - but competitive pressure will dominate",
          confidence: "medium",
          reasoning: "Labs may cooperate on some things but compete on capability. Voluntary commitments are weak. New entrants may not cooperate. Economic incentives ultimately dominate.",
          implications: "Need regulation to backstop industry efforts"
        },
        {
          position: "No - racing dynamics will prevent meaningful cooperation",
          confidence: "medium",
          reasoning: "First-mover advantage too strong. Pressure from investors, boards, nations. Safety theater rather than real safety. Will defect at critical moments.",
          implications: "Need hard constraints (regulation, compute governance)"
        }
      ]
    },
    {
      question: "Can US and China cooperate on AI safety?",
      positions: [
        {
          position: "Yes - mutual interest in survival trumps rivalry",
          confidence: "low",
          reasoning: "Both countries lose from AI catastrophe. Precedent of arms control during Cold War. Track-2 diplomacy shows interest. Narrow cooperation possible even amid competition.",
          implications: "Invest in diplomatic channels; seek narrow agreements"
        },
        {
          position: "Unlikely - strategic competition prevents trust",
          confidence: "medium",
          reasoning: "Verification nearly impossible for AI. Each side fears other will cheat. AI is key military technology. Domestic politics in both countries oppose cooperation.",
          implications: "Focus on defensive measures and domestic safety"
        },
        {
          position: "Uncertain - depends on how competition evolves",
          confidence: "low",
          reasoning: "Current trajectory is negative, but could change. Crisis might create coordination opportunity. Depends heavily on political leadership.",
          implications: "Maintain channels for cooperation; prepare for both scenarios"
        }
      ]
    },
    {
      question: "Will governance keep pace with AI development?",
      positions: [
        {
          position: "No - technology moves too fast",
          confidence: "medium",
          reasoning: "AI progress is exponential. Regulation takes years. Technical complexity exceeds regulator capacity. Lobbying slows regulation.",
          implications: "Need proactive governance, not reactive. Industry self-regulation critical."
        },
        {
          position: "Maybe - with institutional investment",
          confidence: "medium",
          reasoning: "Can build technical capacity in government. AI Safety Institutes are a start. Adaptive regulation possible. Some domains (finance) show it can work.",
          implications: "Invest heavily in government capacity. Create agile regulatory frameworks."
        }
      ]
    }
  ]}
/>

## The Unilateral Action Problem

**The question**: Even if *you* act safely, will others?

| Unilateral Action Matters | Unilateral Action Is Futile |
|---------------------------|----------------------------|
| Set norms and standards | Others won't follow |
| Buy time for coordination | Just delays the inevitable |
| Demonstrate feasibility | Cedes advantage to less careful |
| First-mover on safety creates pressure | Creates competitive disadvantage |
| Safety leader can later help others | Unsafe leader wins the race |

**The coordination dilemma**: If you unilaterally slow down for safety, you might:
- Demonstrate that safe development is possible (positive)
- Fall behind competitors who don't care (negative)
- Create pressure for others to follow (positive)
- Become irrelevant as less safe actors dominate (negative)

**Strategic insight**: The value of unilateral safety depends on whether others can be brought along.

## Mechanisms for Coordination

### Compute Governance

**The idea**: Control AI development by controlling the hardware.

**Current US export control regime**:

| Action | Date | Target | Effect |
|--------|------|--------|--------|
| Initial controls | October 2022 | Advanced AI chips to China | Blocked Nvidia A100/H100 exports |
| <R id="b7658186b450082b">Expanded controls</R> | October 2024 | High-bandwidth memory (HBM) | Added strategic memory components |
| <R id="8e077efb75c0d69a">AI Diffusion Framework</R> | January 2025 | Model weights; compute clusters | Tiered country access; controls on advanced model weights |
| Framework rescinded | 2025 | N/A | Trump administration reversed Biden-era Diffusion Rule |
| <R id="fe41a8475bafc188">Conditional chip sales</R> | August 2025 | Nvidia H20, AMD MI308 to China | 15% revenue share to US government |

**Production gap**: Nvidia projected 4-5 million AI chips in 2025 (double 2024); Huawei estimated 200,000-300,000 completed chips due to HBM shortage.

**Advantages**: Hardware is physical, visible, verifiable. Concentrated production (TSMC produces greater than 90% of advanced chips).

**Challenges**:
- Circumvention through efficiency improvements and alternative architectures
- China's <R id="a3e39f7b4281936a">retaliatory export bans</R> on critical minerals (gallium, germanium, antimony)
- <R id="ccaecd7ab4d9e399">Outbound Investment Security Program</R> (January 2025) restricts US investment in Chinese AI companies

### Responsible Scaling Policies (RSPs)

**The idea**: Labs commit to specific safety requirements at capability levels.

**How it works**:
1. Define capability thresholds (ASL levels at Anthropic)
2. Specify safety requirements for each level
3. Don't deploy until requirements met
4. External audit and verification

**Current status** (per <R id="7e3b7146e1266c71">METR analysis</R>):

| Company | Framework | Key Elements |
|---------|-----------|--------------|
| Anthropic | <R id="394ea6d17701b621">RSP</R> (Sept 2023) | ASL 1-4 levels; capability-triggered evaluations; biosecurity, cyber, autonomy testing |
| OpenAI | <R id="f92eef86f39c6038">Preparedness Framework</R> (Dec 2023) | Tracked categories (bio, cyber, autonomy, persuasion); Medium/High/Critical thresholds |
| Google DeepMind | <R id="d8c3d29798412b9f">Frontier Safety Framework</R> (May 2024) | Critical capability levels; evaluations before training and deployment |
| xAI | Safety framework | Published February 2025 per Seoul Commitment |

**Common elements** (per <R id="30b9f5e826260d9d">METR Common Elements analysis</R>):
- Capability thresholds triggering enhanced safety measures
- Model weight security requirements
- Deployment mitigations for dangerous capabilities
- External reporting and (in some cases) government notification

**Limitations**: Voluntary; relies on honest assessment; <R id="4487a62bbc1c45d6">16 companies committed</R> at Seoul but frameworks vary in rigor; may not cover all risks.

### International Agreements

**The idea**: Nations agree on AI development rules.

**Possible forms**:
- Test ban (no systems above X capability)
- Development limits (no autonomous weapons)
- Mutual inspection
- Shared safety standards

**Challenges**: Verification, enforcement, US-China relations.

### Domestic Regulation

**The idea**: Governments regulate AI within their borders.

**Examples**:
- EU AI Act (risk-based regulation)
- US executive order (safety requirements)
- UK approach (principles-based)

**Limitations**: Varies by jurisdiction, enforcement is hard, risk of regulatory capture.

## What Would Change Your Mind?

### Evidence for coordination failure:
- Major lab breaks from safety commitments
- US-China relations deteriorate further
- Open source capabilities reach dangerous levels
- Regulation fails to pass or is captured
- Racing dynamic intensifies

### Evidence for successful coordination:
- Binding international agreement on AI safety
- Effective compute governance implemented
- Labs successfully enforce RSPs
- Meaningful US-China cooperation on AI
- Governance keeps pace with capability growth

## Implications for Action

### If Coordination Will Fail

**Strategy**:
- Focus on technical solutions that don't require coordination
- Build defensive capabilities
- Prepare for worst case
- Reduce dependency on AI cooperation

**Priority**: Technical alignment, containment, resilience

### If Coordination Is Possible

**Strategy**:
- Invest heavily in diplomatic and governance efforts
- Build institutions for cooperation
- Create verification mechanisms
- Foster trust between actors

**Priority**: Policy, governance, institution-building

### If Uncertain

**Strategy**:
- Pursue both technical and governance paths
- Build coordination capacity while developing technical solutions
- Seek cheap coordination wins
- Maintain optionality

**Priority**: Both technical and governance investment

## The Interaction with Other Cruxes

Coordination interacts with other cruxes:

- **[Timelines](/understanding-ai-risk/core-argument/timelines/)**: Short timelines make coordination harder.
- **[Takeoff Speed](/understanding-ai-risk/core-argument/takeoff/)**: Fast takeoff reduces time for coordination.
- **[Alignment Difficulty](/understanding-ai-risk/core-argument/alignment-difficulty/)**: If alignment is easy, less coordination needed.
- **[Warning Signs](/understanding-ai-risk/core-argument/warning-signs/)**: Warning shots might enable coordination.

---

## Sources

### International Governance

- <R id="a65ad4f1a30f1737">International Network of AI Safety Institutes</R> - NIST fact sheet on the November 2024 launch
- <R id="0572f91896f52377">AI Safety Institute International Network: Next Steps</R> - CSIS analysis of network challenges
- <R id="b163447fdc804872">International AI Safety Report 2025</R> - Collaborative scientific assessment
- <R id="73d81d3ead01bc0e">Seoul Declaration</R> - May 2024 summit outcomes
- <R id="4487a62bbc1c45d6">Frontier AI Safety Commitments</R> - Industry commitments from Seoul Summit

### US-China Relations

- <R id="ab22aa0df9b1be7b">Potential for U.S.-China Cooperation on Reducing AI Risks</R> - RAND analysis of cooperation opportunities
- <R id="25ca111eea083021">Roadmap for US-China AI Dialogue</R> - Brookings policy recommendations
- <R id="331246d11298126e">Challenges and Opportunities for US-China Collaboration</R> - Sandia National Labs analysis
- <R id="7629a035e7e22ee1">Reading Between the Lines of Dueling AI Action Plans</R> - Atlantic Council comparison

### Regulatory Frameworks

- <R id="1ad6dc89cded8b0c">EU AI Act</R> - Comprehensive EU AI Act resource
- <R id="2edad72f4d6f1804">EU AI Act Implementation Timeline</R>772906) - European Parliament timeline
- <R id="8e077efb75c0d69a">AI Diffusion Framework</R> - US Federal Register rule
- <R id="b7658186b450082b">US Export Controls on Advanced Computing</R> - Holland & Knight analysis

### Industry Self-Governance

- <R id="51e8802a5aef29f6">Frontier Model Forum Progress Update</R> - 2024 achievements
- <R id="30b9f5e826260d9d">Common Elements of Frontier AI Safety Policies</R> - METR analysis of safety frameworks
- <R id="7e3b7146e1266c71">Frontier AI Safety Policies Comparison</R> - METR framework comparison

### Historical Coordination

- <R id="81b14196c8af1e9d">Montreal Protocol Success</R> - UN assessment of ozone layer recovery
- <R id="f0c9caf8e366215e">About Montreal Protocol</R> - UNEP overview
- <R id="28cf9e30851a7bc2">AI Race Dynamics</R> - AI Safety textbook chapter on racing dynamics
