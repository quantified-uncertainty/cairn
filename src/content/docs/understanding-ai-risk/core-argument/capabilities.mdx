---
title: "Will AI Be Transformatively Powerful?"
description: "This analysis examines whether AI systems will become capable enough to pose existential risks. Evidence suggests a greater than 50% chance of human-level AI by 2030-2035, with o3 already scoring 87.5% on ARC-AGI (surpassing human performance of 85%), while expert surveys estimate 5-14% probability of AI-caused extinction."
sidebar:
  order: 2
importance: 85
quality: 82
lastEdited: "2025-12-28"
llmSummary: "Comprehensive analysis examining AI capability trajectories toward transformative power, documenting o3's 87.5% ARC-AGI score (exceeding 85% human baseline) and forecaster estimates of 50% probability of AGI by 2028-2031. Includes systematic evidence from benchmarks, scaling laws, recursive self-improvement mechanisms, and expert surveys estimating 5-14% median extinction risk."
---
import {InfoBox, EstimateBox, DisagreementMap, KeyQuestions, Mermaid, R} from '../../../../components/wiki';

<InfoBox
  type="crux"
  title="Transformative AI Capabilities"
  customFields={[
    { label: "Claim", value: "AI will become powerful enough to pose existential risk" },
    { label: "Key Uncertainty", value: "When and how transformative?" },
    { label: "Related To", value: "Timelines, Takeoff, Alignment Difficulty" },
  ]}
/>

### Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Current capability level** | Approaching human-level on specific benchmarks | <R id="457fa3b0b79d8812">o3 scores 87.5% on ARC-AGI</R> vs. 85% human threshold |
| **Timeline to AGI** | 50% probability by 2028-2031 | <R id="f2394e3212f072f5">Metaculus forecasters</R>: median 2027; AI Frontiers: 50% by 2028 |
| **Scaling trajectory** | Shifting from pretraining to post-training | <R id="04f151d760c5b129">Ilya Sutskever</R>: "Now we're back in the age of wonder and discovery" |
| **Expert extinction risk estimate** | 5-14% median probability | <R id="9f9f0a463013941f">2023 AI researcher survey</R>: 14.4% mean, 5% median |
| **Economic transformation potential** | 7% GDP increase; 40% jobs exposed | <R id="41b8c4c3bf70b8d0">Goldman Sachs</R>; <R id="d70245053c0a284b">IMF</R> |
| **Self-improvement capability** | Emerging, timeline 2-4 years | <R id="42900576efb2f3c1">Eric Schmidt</R>: recursive self-improvement within 2-4 years |

**The core question**: Will AI systems become capable enough to potentially end or permanently curtail human civilization—not just cause harm, but be truly transformative?

This is the first link in the chain of reasoning about AI existential risk. If AI never becomes sufficiently powerful, the existential risk is zero regardless of other factors.

## What "Transformative" Means

<Mermaid client:load chart={`
flowchart TD
    NARROW[Narrow AI<br/>Current systems] --> AGI[Human-Level AGI<br/>Matches most cognitive tasks]
    AGI --> SUPER[Superhuman AI<br/>Exceeds humans broadly]
    SUPER --> ASI[Superintelligence<br/>Vastly exceeds humans]

    AGI --> |"Recursive improvement"| SUPER
    SUPER --> |"Intelligence explosion"| ASI

    NARROW --> |"2025-2030?"| AGI

    AGI -.-> RISK1[Economic disruption<br/>Power concentration]
    SUPER -.-> RISK2[Existential risk<br/>if misaligned]
    ASI -.-> RISK3[Near-certain catastrophe<br/>if misaligned]

    style NARROW fill:#ccffcc
    style AGI fill:#ffffcc
    style SUPER fill:#ffddcc
    style ASI fill:#ffcccc
    style RISK1 fill:#fff
    style RISK2 fill:#fee
    style RISK3 fill:#fcc
`} />

<EstimateBox
  client:load
  variable="Capability Levels"
  description="Different levels of AI capability and their existential implications"
  unit=""
  estimates={[
    { source: "Narrow AI", value: "Current", notes: "Better than humans at specific tasks. No direct existential risk." },
    { source: "Human-Level AGI", value: "Maybe transformative", notes: "Matches humans across most cognitive tasks. Risk depends on deployment and goals." },
    { source: "Superhuman AI", value: "Transformative", notes: "Exceeds humans at nearly everything. Likely existential risk if misaligned." },
    { source: "Superintelligence", value: "Highly transformative", notes: "Vastly exceeds humans. Almost certainly existential risk if misaligned." },
  ]}
/>

### Key Capability Thresholds

**Economically Transformative**: AI that can automate most human cognitive work, potentially leading to:
- Massive economic disruption
- Rapid technological advancement
- Concentration of power in AI developers

**Strategically Transformative**: AI that can outperform humans at:
- Scientific research and discovery
- Military strategy and operations
- Political manipulation and persuasion
- Long-term planning and execution

**Existentially Transformative**: AI that could, if misaligned:
- Take actions that prevent human course-correction
- Acquire resources and capabilities faster than humans can respond
- Fundamentally reshape the world against human interests

## Arguments for Transformative Capabilities

### 1. No Known Ceiling on AI Capabilities

**The argument**: There's no theoretical reason AI capabilities should stop improving at any particular level.

**Supporting evidence**:
- Scaling laws show consistent improvement with more compute and data
- New architectures continue to unlock capabilities
- Human intelligence emerged from optimization—AI is another form of optimization
- Physical laws don't prevent information processing beyond human levels

**Recent benchmark progress demonstrates rapid capability gains:**

| Benchmark | GPT-4 (2023) | o1 (2024) | o3 (2024) | Human Baseline |
|-----------|--------------|-----------|-----------|----------------|
| <R id="a27f2ad202a2b5a7">ARC-AGI</R> | 5% | 18% | **87.5%** | 85% |
| <R id="056c40c4515292c5">AIME 2024</R> (math competition) | ~30% | 56.6% | **96.7%** | ~90% |
| <R id="056c40c4515292c5">Frontier Math</R> (novel problems) | ~2% | ~2% | **25.2%** | Varies |
| <R id="3c8e4281a140e1cd">GPQA Diamond</R> (PhD-level Q&A) | ~50% | ~70% | **87.7%** | 70% (PhDs) |
| <R id="3c8e4281a140e1cd">SWE-bench Verified</R> (coding) | ~20% | ~50% | **71.7%** | — |

*It took four years for AI models to progress from 0% to 5% on ARC-AGI. Then, in months, o3 reached 87.5%.*

**The key insight**: As <R id="fa86a823d9f92b0c">Nick Bostrom</R> notes, "The brain is just atoms arranged in a particular way. Nothing says that arrangement is optimal for intelligence."

### 2. Intelligence Is the Source of Human Power

**The argument**: Humans dominate Earth not due to physical strength but cognitive capabilities. Superior cognition → superior outcomes.

**Implications**:
- What took humans thousands of years, AI might do in months
- AI could discover technologies we can't imagine
- The cognitive advantage compounds: smarter → better tools → even smarter

**Historical parallel**: Humans vs. other great apes. Small cognitive differences led to radically different outcomes.

### 3. Recursive Self-Improvement

**The argument**: AI capable of improving AI could lead to rapid capability gains.

**Mechanism**:
1. AI helps design better AI architectures
2. Better AI is even better at AI design
3. Feedback loop continues
4. Capabilities grow exponentially

**Current developments:**

- <R id="42900576efb2f3c1">Former Google CEO Eric Schmidt</R> stated at Harvard that the AI industry is "rapidly approaching recursive self-improvement" and this could happen within **2-4 years**
- The <R id="069db046b5cfeba7">Darwin Godel Machine</R> (2024) demonstrated an AI that rewrites its own code to improve performance on programming tasks
- <R id="14bfb02e6a6554c3">Meta AI's Self-Rewarding Language Models</R> research explores models capable of providing their own training feedback
- Influential essays like <R id="148d0bf3dde0b4a8">"Situational Awareness"</R> and "AI-2027" project superintelligence emergence through recursive self-improvement by 2027-2030

**Uncertainty**: Whether this produces smooth acceleration or sudden jumps (see Takeoff Speed).

### 4. Operational Advantages

Even at human-level cognition, AI has advantages:

| Advantage | Implication |
|-----------|-------------|
| **Speed** | Thinks millions of times faster than humans |
| **Parallelism** | Can run many copies simultaneously |
| **Communication** | Perfect, instant communication between copies |
| **Memory** | Perfect recall, can be backed up |
| **No downtime** | Doesn't need sleep, breaks, or motivation |
| **Scalability** | More compute = more capability |

**Implication**: Human-level AI is already superhuman in important ways.

### 5. Specific Dangerous Capabilities

Capabilities that could enable existential risk:

**Persuasion at Scale**: Ability to manipulate humans individually and collectively
- <R id="80486926b4324a65">Deepfake attempts increased 3,000% in 2023</R>; deepfake videos grew 550% since 2019
- Research shows AI-generated personalized political messages can be <R id="38c3ea9ededca662">more persuasive than human-generated content</R>
- The <R id="742a2119cf8d25da">World Economic Forum's 2024 Global Risks Report</R> ranked AI-powered misinformation as the biggest short-term global risk
- In India's 2024 elections, over <R id="80486926b4324a65">15 million people were reached</R> via 5,800 WhatsApp groups circulating AI deepfakes

**Cyber Offense**: Ability to hack, sabotage, or control digital infrastructure
- AI already assists with vulnerability discovery
- Critical infrastructure increasingly networked
- Speed advantage is decisive in cyber conflict

**Scientific Research**: Ability to make scientific and technological breakthroughs
- Drug discovery, materials science, weapons development
- Could design technologies humans can't understand or counter
- Biological weapons, novel attack vectors

**Strategic Planning**: Long-horizon reasoning and execution
- Humans struggle with long-term planning
- AI could optimize for years while appearing harmless
- Consistent goals across millions of copies

**Self-Replication**: Ability to acquire resources and copy itself
- Could spread beyond human ability to track or control
- Economic resources → more compute → more copies
- Resilient to shutdown attempts

## Arguments Against Transformative Capabilities

### 1. Intelligence May Have Diminishing Returns

**The argument**: Beyond a certain point, more intelligence doesn't help much.

**Supporting examples**:
- Physics problems solvable with sufficient intelligence are already solved
- Many problems are limited by data, not analysis
- Some problems are computationally intractable regardless of intelligence

**Evidence for scaling plateau**: <R id="04f151d760c5b129">Ethan Mollick</R> notes that "for over a year now, frontier models appear to have reached their ceiling. The scaling laws that powered the exponential progress of LLMs like GPT-4 have started to show diminishing returns." <R id="c0d70df6062a3e77">Nobel laureate Daron Acemoglu</R> offers more conservative estimates, projecting AI-driven productivity gains of only 0.7% over 10 years.

**Counter-argument**: Diminishing returns at the margin doesn't mean intelligence stops mattering—it means the marginal gains decrease, not that gains stop. The shift from pretraining to <R id="056c40c4515292c5">post-training via reinforcement learning</R> (as seen in o1/o3) may represent a new scaling paradigm.

### 2. Physical Constraints

**The argument**: Intelligence is constrained by physical reality, not just cognition.

**Examples**:
- Can't violate laws of physics
- Resources are finite
- Some processes have minimum time requirements
- Can't know everything (information is limited)

**Counter-argument**: Within physical constraints, immense optimization space remains. Humans are nowhere near optimal use of physics.

### 3. AI May Remain Tool-Like

**The argument**: AI might never become agent-like or autonomous.

**Supporting reasoning**:
- Current AI has no inherent goals
- Economic value is in tools, not agents
- We might never build agentic AI
- AI without goals can't threaten humans

**Counter-argument**: Economic pressures favor autonomy. Agentic systems are being built (AutoGPT, AI agents). The tool/agent boundary is unclear.

### 4. "Superintelligence" Is Incoherent

**The argument**: The concept of general superhuman intelligence may not make sense.

**Reasoning**:
- Intelligence is multidimensional—there's no single axis
- Different tasks require different capabilities
- "Better at everything" may be meaningless
- Human-like cognition might be a local optimum

**Counter-argument**: Even if multidimensional, being better on most dimensions still confers huge advantages. And we've seen capabilities improvements on many dimensions simultaneously.

### 5. Current AI Has Fundamental Limitations

**The argument**: LLMs and current approaches can't reach transformative capabilities.

**Claimed limitations**:
- No true understanding or reasoning
- Brittleness and hallucination
- Can't learn from experience continuously
- No genuine creativity

**Counter-argument**: These limitations apply to current systems. New architectures emerge regularly. Past limitations (can't beat humans at Go, can't generate coherent text) have been overcome.

## The Ceiling Question

### AGI Timeline Predictions

| Source | Prediction | Date of Estimate |
|--------|------------|------------------|
| <R id="f2394e3212f072f5">Metaculus forecasters</R> | 50% by 2031; 25% by 2027 | Dec 2024 |
| <R id="509bbd7d887827b2">AI Frontiers (Khoja & Hiscott)</R> | 50% by 2028; 80% by 2030 | 2024 |
| <R id="b768ab4c3f4ed9dd">Dario Amodei (Anthropic)</R> | "Powerful AI" by 2026 | 2024 |
| <R id="b768ab4c3f4ed9dd">Jensen Huang (Nvidia)</R> | Human-level by 2029 | March 2024 |
| <R id="b768ab4c3f4ed9dd">Sam Altman (OpenAI)</R> | Possibly 2025 (gradual impact) | 2024 |
| <R id="2f2cf65315f48c6b">Andrej Karpathy</R> | ~10 years away | 2024 |

*In four years, the mean Metaculus estimate for AGI has fallen from 50 years to 5 years.*

<DisagreementMap
  client:load
  topic="Where Do AI Capabilities Top Out?"
  description="Expert views on the ultimate limits of AI capability"
  spectrum={{ low: "Narrow tasks only", high: "Vastly superhuman" }}
  positions={[
    { actor: "Deep learning skeptics", position: "Current paradigm hits wall", estimate: "~Human level at best", confidence: "medium" },
    { actor: "Moderate AI researchers", position: "Human level achievable", estimate: "Matches humans", confidence: "medium" },
    { actor: "Scaling advocates", position: "Human level is not a ceiling", estimate: "Significantly superhuman", confidence: "medium" },
    { actor: "MIRI/Yudkowsky", position: "No meaningful ceiling", estimate: "Vastly superhuman possible", confidence: "high" },
  ]}
/>

### The "Foom" Question

**Fast takeoff view**: Once AI matches human-level, it quickly becomes vastly superhuman through recursive self-improvement.

**Slow takeoff view**: Improvements continue at similar rates; no sudden jump. Human-level to superhuman takes decades.

(See the [Takeoff Speed](/understanding-ai-risk/core-argument/takeoff/) page for more detail.)

## Why This Is a Crux

Your view on transformative capabilities affects:

**If you believe transformative AI is unlikely:**
- Focus on near-term harms (bias, disinformation, unemployment)
- Technical alignment research is less urgent
- Governance for current systems is priority

**If you believe transformative AI is likely:**
- Existential risk is a real concern
- Alignment research is critical
- Need to consider long-term implications

**If you're uncertain:**
- Hedging strategies make sense
- Work on both near and long-term issues
- Update based on progress

## Quantifying the Question

<KeyQuestions
  client:load
  questions={[
    {
      question: "Will AI match humans at most cognitive tasks within 30 years?",
      positions: [
        {
          position: "Very likely (>80%)",
          confidence: "medium",
          reasoning: "Current trajectory, scaling laws, massive investment",
          implications: "Need to prepare for transformative AI soon"
        },
        {
          position: "Likely (50-80%)",
          confidence: "medium",
          reasoning: "Significant obstacles may emerge; current methods may plateau",
          implications: "Should invest in alignment while maintaining uncertainty"
        },
        {
          position: "Uncertain (under 50%)",
          confidence: "low",
          reasoning: "Too many unknowns; AI history is full of winters",
          implications: "Focus on nearer-term issues; maintain optionality"
        }
      ]
    },
    {
      question: "If AI reaches human-level, will it quickly exceed human capabilities?",
      positions: [
        {
          position: "Yes, rapid acceleration",
          confidence: "medium",
          reasoning: "Recursive self-improvement, operational advantages",
          implications: "Short window between human-level and superhuman"
        },
        {
          position: "Gradual improvement only",
          confidence: "medium",
          reasoning: "Diminishing returns, physical constraints, complexity",
          implications: "More time to adapt and align"
        }
      ]
    }
  ]}
/>

## Current Evidence

### Supporting Transformative Capabilities

**Scaling Laws**: Capabilities have consistently improved with scale. Each generation of models exceeds previous. <R id="cb1cd9e4d736df7f">Our World in Data</R> documents how compute used in AI training has increased by a factor of 10 billion since 2010.

**Novel Capabilities Emerge**: Larger models develop capabilities not present in smaller versions (in-context learning, chain-of-thought reasoning). The <R id="457fa3b0b79d8812">o3 model's 87.5% on ARC-AGI</R> represents a qualitative leap—this benchmark was specifically designed to resist current AI approaches.

**Rapid Progress**: Chess → Go → Protein folding → Language → Coding. Each seemed impossibly hard until it wasn't. The <R id="7226d362130b23f8">performance gap between US and Chinese models</R> narrowed from 9.26% in January 2024 to 1.70% by February 2025.

**Investment**: Hundreds of billions being invested. <R id="9ce35082bc3ab2d4">OpenAI's compute costs</R> for GPT-4 training exceeded $100 million; next-generation models may cost $1 billion+.

### Against Transformative Capabilities (Near-Term)

**Plateaus Happen**: <R id="7226d362130b23f8">Multiple sources</R> report pretraining-focused scaling (GPT-4.5, Grok 4) "disappointed" in 2025—it proved roughly 30x more efficient to invest in post-training (reinforcement learning) instead.

**Fundamental Issues Remain**: Hallucination, reliability, reasoning remain problems. <R id="04f151d760c5b129">Yann LeCun and others</R> argue that LLMs will not achieve AGI and "progress will require new breakthroughs."

**Deployment Challenges**: Getting AI to work reliably in the real world is hard. <R id="6a08d4237e7b3507">o3's high-compute mode costs exceed $1,000 per query</R>—172x the cost of low-compute mode.

**Previous Hype Cycles**: AI has had multiple boom-bust cycles. Current one may end similarly.

## Implications for Action

### If Transformative AI Is Coming Soon

**Urgency is high**:
- Alignment research needs to succeed before capabilities outpace it
- Governance and coordination needed now
- Prepare for major economic and social disruption
- Consider slowing capability development

### If Transformative AI Is Distant or Unlikely

**Less urgent, different focus**:
- Current AI harms (bias, privacy, jobs) are priority
- Can develop governance frameworks gradually
- Technical alignment research is less time-sensitive
- Focus on getting current AI right

### If Uncertain

**Hedging makes sense**:
- Invest in both near and long-term issues
- Build institutional capacity for rapid response
- Monitor progress closely
- Maintain optionality

---

## Sources & Further Reading

### Benchmark Performance & Capabilities
- <R id="457fa3b0b79d8812">OpenAI o3 Breakthrough High Score on ARC-AGI-Pub</R> - ARC Prize analysis of o3's 87.5% score
- <R id="a27f2ad202a2b5a7">ARC-AGI-1 Leaderboard</R> - Current benchmark standings
- <R id="056c40c4515292c5">Scaling Laws for LLMs: From GPT-3 to o3</R> - Deep dive on scaling trajectory
- <R id="3c8e4281a140e1cd">OpenAI o3: The 2024 Finale of AI</R> - Nathan Lambert's analysis

### Timelines & Expert Forecasts
- <R id="f2394e3212f072f5">Shrinking AGI timelines: a review of expert forecasts</R> - 80,000 Hours comprehensive review
- <R id="2f2cf65315f48c6b">When Will AGI/Singularity Happen? 8,590 Predictions Analyzed</R> - AIMultiple aggregate analysis
- <R id="509bbd7d887827b2">AGI's Last Bottlenecks</R> - AI Frontiers timeline estimates

### Scaling & Industry Trends
- <R id="04f151d760c5b129">Scaling: The State of Play in AI</R> - Ethan Mollick on scaling plateaus
- <R id="7226d362130b23f8">A brief history of LLM Scaling Laws</R> - 2025 scaling landscape
- <R id="9ce35082bc3ab2d4">AI Model Scaling Isn't Over</R> - AI Business on new directions
- <R id="cb1cd9e4d736df7f">Scaling up AI</R> - Our World in Data on compute trends

### Economic Impact
- <R id="41b8c4c3bf70b8d0">The Potentially Large Effects of AI on Economic Growth</R> - Goldman Sachs research
- <R id="d70245053c0a284b">AI Will Transform the Global Economy</R> - IMF analysis
- <R id="c0d70df6062a3e77">What do we know about the economics of AI?</R> - MIT/Acemoglu research

### Existential Risk & Safety
- <R id="9f9f0a463013941f">Existential risk from artificial intelligence</R> - Wikipedia overview with survey data
- <R id="d9fb00b6393b6112">Problem profiles: Risks from power-seeking AI</R> - 80,000 Hours risk analysis
- <R id="fa86a823d9f92b0c">Superintelligence, Ten Years On</R> - Bostrom retrospective

### Self-Improvement & Advanced Capabilities
- <R id="42900576efb2f3c1">Recursive self-improvement</R> - Wikipedia overview
- <R id="069db046b5cfeba7">The Darwin Godel Machine</R> - Sakana AI's self-improving system
- <R id="14bfb02e6a6554c3">The Unavoidable Problem of Self-Improvement in AI</R> - Future of Life Institute interview

### Disinformation & Manipulation
- <R id="80486926b4324a65">AI-driven disinformation: policy recommendations</R> - PMC research on deepfakes
- <R id="38c3ea9ededca662">Don't Panic (Yet): AI and Elections</R> - Knight Institute analysis
- <R id="742a2119cf8d25da">AI supercharging misinformation concerns</R> - Harvard Kennedy School
