---
title: AI Safety Wiki
description: A comprehensive guide to AI existential risk and safety
template: splash
hero:
  tagline: Understanding AI risk, alignment, and the path to safe artificial intelligence
  actions:
    - text: Start Here
      link: /understanding-ai-risk/core-argument/
      icon: right-arrow
    - text: FAQ
      link: /resources/faq/
      icon: information
      variant: minimal
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## What Is This?

This wiki maps the **landscape of AI existential risk**â€”the arguments, disagreements, organizations, and interventions that matter for ensuring advanced AI goes well for humanity.

Whether you're new to AI safety or a researcher looking for a comprehensive reference, this site aims to be your guide.

---

## Core Content

<CardGrid>
  <Card title="ðŸŽ¯ Understanding AI Risk" icon="warning">
    The [core argument](/understanding-ai-risk/core-argument/) for why AI might pose existential risk, broken into key claims and cruxes.

    - [Will AI be transformatively powerful?](/understanding-ai-risk/core-argument/capabilities/)
    - [How hard is alignment?](/understanding-ai-risk/core-argument/alignment-difficulty/)
    - [Will we get warning signs?](/understanding-ai-risk/core-argument/warning-signs/)
  </Card>

  <Card title="âš–ï¸ Key Debates" icon="comment-alt">
    Structured arguments on contested questions:

    - [Is AI x-risk real?](/debates/is-ai-xrisk-real/)
    - [Pause vs. accelerate?](/debates/pause-vs-accelerate/)
    - [Working at AI labs?](/debates/working-at-labs/)
  </Card>

  <Card title="ðŸ¢ Organizations" icon="seti:folder">
    Profiles of key players:

    - [Frontier labs](/organizations/) (Anthropic, OpenAI, DeepMind)
    - [Safety research orgs](/organizations/) (MIRI, ARC, Redwood)
    - [Funders and governance](/resources/funding/)
  </Card>

  <Card title="ðŸ›¡ï¸ Safety Approaches" icon="approve-check">
    Technical and governance solutions:

    - [Technical approaches](/safety-approaches/technical/) (interpretability, RLHF, AI control)
    - [Governance](/safety-approaches/governance/) (compute governance, international coordination)
    - [Research agendas compared](/analysis/research-agendas/)
  </Card>
</CardGrid>

---

## Analysis & Visualization

<CardGrid>
  <Card title="ðŸ“Š Timeline" icon="seti:clock">
    [Interactive AI timeline](/analysis/ai-timeline/) from the Dartmouth conference to near-term predictions.
  </Card>

  <Card title="ðŸ—ºï¸ Risk Map" icon="seti:pipeline">
    [Visual dependency graph](/analysis/risk-map/) showing how risk factors connect.
  </Card>

  <Card title="ðŸ“ˆ Comparisons" icon="list-format">
    [Side-by-side tables](/analysis/comparisons/) comparing labs, approaches, and expert positions.
  </Card>

  <Card title="ðŸ”® Scenarios" icon="puzzle">
    [Future scenarios](/scenarios/) for how AI development might unfold.
  </Card>
</CardGrid>

---

## Learning Paths

**New to AI safety?** Start with:
1. [FAQ: Common questions](/resources/faq/)
2. [Core argument overview](/understanding-ai-risk/core-argument/)
3. [Glossary of terms](/resources/glossary/)

**Want to contribute?** Explore:
1. [Intervention analysis](/analysis/interventions/) - What can you do?
2. [Funding landscape](/resources/funding/) - How to get funded
3. [Organizations](/organizations/) - Who's working on this

**Looking for depth?** Try:
1. [Key debates](/debates/) - Strongest arguments on each side
2. [Research agendas](/analysis/research-agendas/) - Compare approaches
3. [Historical case studies](/analysis/case-studies/) - Lessons from other risks

---

## Key Numbers

| Question | Estimates |
|----------|-----------|
| **P(transformative AI by 2040)** | 40-80% (varies by source) |
| **P(doom) estimates** | 5-90% (wide disagreement) |
| **AI safety researchers** | ~300-1000 FTE |
| **Annual safety funding** | ~$100-500M |
| **Frontier lab safety spend** | ~$50-200M combined |

See [estimates dashboard](/analysis/estimates-dashboard/) for detailed breakdowns.

---

## Featured People

Key voices in AI safety:

- **[Dario Amodei](/people/dario-amodei/)** - Anthropic CEO, "10-25% doom"
- **[Eliezer Yudkowsky](/people/eliezer-yudkowsky/)** - MIRI, most pessimistic public voice
- **[Paul Christiano](/people/paul-christiano/)** - ARC founder, influential alignment researcher
- **[Geoffrey Hinton](/people/geoffrey-hinton/)** - "Godfather of AI", recent safety advocate
- **[Stuart Russell](/people/stuart-russell/)** - UC Berkeley, *Human Compatible* author

[See all researchers â†’](/people/)

---

## This Wiki Is...

âœ… **Comprehensive** â€” Covers technical, governance, and strategic perspectives

âœ… **Structured** â€” Organized by cruxes, not just topics

âœ… **Balanced** â€” Presents strongest arguments on all sides

âœ… **Interactive** â€” Timeline, risk maps, argument maps

âœ… **Practical** â€” Career and funding guidance

---

## Contributing

This is an open project. Key areas where contributions would be valuable:
- Adding researcher profiles
- Updating organization pages
- Improving argument maps
- Adding sources and citations

---

<CardGrid>
  <Card title="Explore by Topic" icon="magnifier">
    Browse the sidebar to explore specific topics, risks, and organizations.
  </Card>

  <Card title="Start Learning" icon="open-book">
    [Begin with the core argument â†’](/understanding-ai-risk/core-argument/)
  </Card>
</CardGrid>
