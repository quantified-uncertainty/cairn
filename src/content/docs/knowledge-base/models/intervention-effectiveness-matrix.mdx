---
title: Intervention Effectiveness Matrix
description: Mapping AI safety interventions to the risks they mitigate, with effectiveness estimates and gap analysis
sidebar:
  order: 35
quality: 4
lastEdited: "2025-12-27"
ratings:
  novelty: 3       # Matrix format is practical but not conceptually novel
  rigor: 3         # Effectiveness ratings now include evidence basis and confidence levels
  actionability: 5 # Directly actionable - shows which interventions address which risks
  completeness: 4  # Good coverage of interventions with gap prioritization
---

import { Aside } from '@astrojs/starlight/components';
import { DataInfoBox, Backlinks, KeyQuestions, Mermaid } from '../../../../components/wiki';

<DataInfoBox entityId="intervention-effectiveness-matrix" ratings={frontmatter.ratings} />


## Overview

This model maps **AI safety interventions** (technical, governance, and organizational) to the **specific risks** they help mitigate. The goal is to identify which interventions address which failure modes, estimate their effectiveness, and highlight dangerous gaps where no good interventions exist.

**Key insight**: Many interventions are highly specialized, effective against some risks but irrelevant to others. A comprehensive safety portfolio requires layered interventions targeting different failure modes.

## Strategic Importance

<Aside type="tip" title="Bottom Line">
The matrix reveals that **no single intervention covers all risks**. The most strategically important gaps are (1) deceptive alignment detection and (2) structural risk governance—both need fundamentally new approaches, not just more of current work.
</Aside>

### How to Use This Model for Prioritization

This matrix serves prioritization by showing:
1. **Coverage gaps** - Which risks have NO effective interventions
2. **Redundancy** - Which risks are over-covered relative to their magnitude
3. **Portfolio construction** - What combination provides best coverage

### Priority Ranking of Gaps

Not all gaps are equally important. Ranking by **severity × tractability:**

<Mermaid client:load chart={`
quadrantChart
    title Gap Prioritization
    x-axis Low Tractability --> High Tractability
    y-axis Low Severity --> High Severity
    quadrant-1 HIGHEST PRIORITY
    quadrant-2 Fund basic research
    quadrant-3 Lower priority
    quadrant-4 Quick wins
    Deceptive alignment: [0.4, 0.9]
    Scheming: [0.25, 0.85]
    Structural risks: [0.55, 0.6]
    Epistemic collapse: [0.5, 0.5]
    Treacherous turn: [0.2, 0.95]
`} />

| Gap | Severity | Tractability | Priority | Recommendation |
|-----|----------|--------------|----------|----------------|
| **Deceptive Alignment** | Very High | Medium | **Critical** | Massive interpretability investment |
| **Scheming/Treacherous Turn** | Very High | Low | **High** (research) | AI Control as stopgap; long-term theory |
| **Structural Risks** | High | Medium-High | **High** | Governance work, not technical |
| **Epistemic Collapse** | Medium | Medium | **Medium** | Information ecosystem reform |
| **Misuse (bio/cyber)** | High | High | **Medium** | Already receiving attention; continue |

### Resource Allocation Implications

**Current allocation is misaligned with gaps:**

| Intervention Type | Current Effort | Gap Severity Addressed | Recommendation |
|-------------------|----------------|------------------------|----------------|
| RLHF/Fine-tuning | Very High | Low (doesn't address deception) | **Reduce** marginal investment |
| Red-teaming/Evals | High | Medium (catches some issues) | **Maintain** |
| Interpretability | Medium | Very High (key gap) | **Increase substantially** |
| AI Control | Low | Very High (defense against worst-case) | **Increase substantially** |
| Structural governance | Low | High (only solution for structural risks) | **Increase substantially** |

<Aside type="caution">
The community over-invests in RLHF-adjacent work (because it's tractable) and under-invests in interpretability and AI Control (because they're hard). This is exactly backwards given the gap analysis.
</Aside>

### Key Cruxes

| If you believe... | Then prioritize... |
|-------------------|-------------------|
| Deceptive alignment is the main risk | Interpretability, AI Control |
| Misuse is the main risk | Evals, red-teaming, governance |
| Structural risks matter most | Governance, not technical work |
| Current alignment approaches will scale | Continue current portfolio |
| Current approaches won't scale | Shift to AI Control + new paradigms |

### Actionability

**For funders:**
- Shift ~20% of technical safety funding from RLHF-adjacent work to interpretability
- Fund AI Control research as insurance against alignment failure
- Fund governance work for structural risks (technical work won't help)

**For researchers:**
- If you can contribute to interpretability, that's highest-value technical work
- If not, AI Control and evals are second-tier priorities
- Don't pile onto RLHF/fine-tuning unless you have unique comparative advantage

**For policymakers:**
- The matrix shows technical interventions won't solve structural risks
- Governance interventions are the only tools for concentration/lock-in/enfeeblement
- Don't wait for technical solutions that won't come

## Intervention Categories

### Technical Interventions

| Intervention | Description | Development Stage |
|--------------|-------------|-------------------|
| **Interpretability** | Understanding model internals | Research (promising) |
| **RLHF/RLAIF** | Human feedback alignment | Deployed (limited) |
| **Constitutional AI** | Principle-based training | Deployed (limited) |
| **Capability Evaluations** | Pre-deployment testing | Deployed (improving) |
| **Red-teaming** | Adversarial testing | Deployed (standard) |
| **AI Control** | Containment and monitoring | Research (early) |
| **Formal Verification** | Mathematical proofs | Research (toy models) |
| **Debate/IDA** | Scalable oversight methods | Research (theoretical) |

### Governance Interventions

| Intervention | Description | Development Stage |
|--------------|-------------|-------------------|
| **Compute Governance** | Tracking/restricting compute | Proposed (limited implementation) |
| **Model Registries** | Tracking deployed models | Proposed |
| **Safety Standards** | Minimum requirements | Emerging (voluntary) |
| **Liability Frameworks** | Legal accountability | Underdeveloped |
| **International Treaties** | Binding agreements | Early discussion |
| **Export Controls** | Restricting technology flow | Implemented (chips) |

### Organizational Interventions

| Intervention | Description | Development Stage |
|--------------|-------------|-------------------|
| **Responsible Scaling** | Capability-triggered safeguards | Voluntary (some labs) |
| **Safety Teams** | Dedicated safety research | Variable (lab-dependent) |
| **External Audits** | Third-party evaluation | Emerging |
| **Whistleblower Protection** | Enabling internal dissent | Weak |
| **Deployment Pauses** | Halting risky releases | Ad hoc |

## Effectiveness Matrix: Technical Interventions vs Risks

### Accident Risks

| Risk | Interpretability | RLHF | Constitutional AI | Evals | Red-teaming | AI Control |
|------|-----------------|------|-------------------|-------|-------------|------------|
| **Deceptive Alignment** | Medium | Low | Low | Low | Low | Medium |
| **Goal Misgeneralization** | Medium | Medium | Medium | Medium | Medium | Medium |
| **Reward Hacking** | Low | Low | Low | High | High | Low |
| **Mesa-Optimization** | Medium | Low | Low | Low | Low | Medium |
| **Power-Seeking** | Medium | Low | Low | Medium | Medium | High |
| **Scheming** | Medium | Low | Low | Low | Low | High |
| **Treacherous Turn** | Low | Low | Low | Low | Low | High |
| **Corrigibility Failure** | Medium | Medium | Medium | Medium | Medium | High |
| **Emergent Capabilities** | Low | Low | Low | High | Medium | Medium |

**Legend**: High = substantially reduces risk (over 50 percent reduction); Medium = moderately reduces risk (20-50 percent); Low = minimal impact (under 20 percent)

**Evidence Basis for Ratings:**

| Intervention | Evidence Quality | Key Sources | Confidence Level |
|--------------|-----------------|-------------|------------------|
| **Interpretability** | Medium | Anthropic mechanistic interpretability research (2023-24); limited scaling evidence | Medium - promising but unproven at frontier scale |
| **RLHF/RLAIF** | High | OpenAI InstructGPT (2022), Anthropic Constitutional AI (2023), widespread deployment | High for surface alignment; Low for deceptive alignment |
| **Capability Evaluations** | Medium-High | METR dangerous capability evals, UK AISI testing, Anthropic RSP evaluations | Medium-High - established practice, uncertain coverage |
| **Red-teaming** | High | Widespread industry practice, academic studies on adversarial robustness | High for known attack vectors; Low for novel risks |
| **AI Control** | Low | Greenblatt et al. (2024) theoretical framework; no deployment evidence | Low - theoretically grounded but untested |
| **Formal Verification** | Very Low | Toy model demonstrations only; no frontier-scale applications | Very Low - existence proofs only |

### Misuse Risks

| Risk | Interpretability | RLHF | Constitutional AI | Evals | Red-teaming | AI Control |
|------|-----------------|------|-------------------|-------|-------------|------------|
| **Bioweapons** | Low | Medium | Medium | High | High | N/A |
| **Cyberweapons** | Low | Medium | Medium | High | High | N/A |
| **Disinformation** | Low | Medium | Medium | Medium | Medium | N/A |
| **Deepfakes** | Low | Low | Low | Medium | Medium | N/A |
| **Surveillance** | Low | Low | Low | Low | Low | N/A |
| **Autonomous Weapons** | Low | Low | Low | Medium | Medium | N/A |

### Structural Risks

| Risk | Interpretability | RLHF | Constitutional AI | Evals | Red-teaming | AI Control |
|------|-----------------|------|-------------------|-------|-------------|------------|
| **Concentration of Power** | Low | Low | Low | Low | Low | Low |
| **Lock-in** | Low | Low | Low | Low | Low | Low |
| **Enfeeblement** | Low | Low | Low | Low | Low | Low |
| **Erosion of Agency** | Low | Low | Low | Low | Low | Low |

### Epistemic Risks

| Risk | Interpretability | RLHF | Constitutional AI | Evals | Red-teaming | AI Control |
|------|-----------------|------|-------------------|-------|-------------|------------|
| **Epistemic Collapse** | Low | Low | Low | Low | Low | Low |
| **Reality Fragmentation** | Low | Medium | Medium | Low | Low | Low |
| **Trust Cascade** | Low | Low | Low | Low | Low | Low |
| **Institutional Capture** | Low | Low | Low | Low | Low | Low |

## Gap Analysis: Under-Addressed Risks

### Critical Gaps

| Risk | Gap Description | What is Needed |
|------|-----------------|----------------|
| **Deceptive Alignment** | No reliable detection method | Interpretability breakthroughs; robust control methods |
| **Scheming** | Cannot verify absence of strategic deception | Formal verification at scale; model organisms research |
| **Lock-in** | Technical tools do not address power dynamics | Governance frameworks; anti-monopoly measures |
| **Epistemic Collapse** | Technical fixes miss systemic causes | Information ecosystem redesign; media literacy |

## Intervention Synergies

| Pair | Synergy |
|------|---------|
| **Interpretability + Evals** | Interpretability explains what evals detect |
| **Red-teaming + AI Control** | Red-teaming finds gaps in control measures |
| **RLHF + Constitutional AI** | Layered training approaches |
| **Compute Gov + Export Controls** | Hardware and software restrictions together |

## Recommendations

### Research Priorities

1. **Interpretability for deception detection** - Critical gap with plausible path forward
2. **AI Control methods** - Defense against worst-case scenarios
3. **Formal verification scaling** - Mathematical guarantees remain elusive
4. **Structural risk governance** - Technical research unlikely to help

## Key Uncertainties

<KeyQuestions
  questions={[
    "Will interpretability scale to frontier models?",
    "Can AI Control work against superintelligent systems?",
    "Are governance interventions politically feasible before crisis?",
    "Do current interventions help or hurt with deceptively aligned systems?"
  ]}
/>

## Related Models

- [Defense in Depth Model](/knowledge-base/models/defense-in-depth-model/) - How layered interventions combine
- [Safety-Capability Tradeoff Model](/knowledge-base/models/safety-capability-tradeoff/) - Cost of interventions
- [Deceptive Alignment Decomposition](/knowledge-base/models/deceptive-alignment-decomposition/) - Key gap analysis

## Sources

### Primary Evidence Sources
- Anthropic. Core Views on AI Safety (2023) - Framework for technical safety approaches
- Greenblatt et al. AI Control: Improving Safety Despite Intentional Subversion (2024) - AI Control methodology
- Hubinger et al. Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training (2024) - Deceptive alignment evidence
- Ouyang et al. Training Language Models to Follow Instructions (InstructGPT, 2022) - RLHF effectiveness
- Bai et al. Constitutional AI (2022) - CAI effectiveness

### Effectiveness Assessment Sources
- METR Dangerous Capability Evaluations (2023-24) - Evaluation methodology
- UK AISI Pre-Deployment Testing Reports (2024) - Government evaluation practice
- Shevlane et al. Model Evaluation for Extreme Risks (2023) - Evaluation framework
- Ngo et al. The Alignment Problem from a Deep Learning Perspective (2023) - Technical risk analysis

## Related Pages

<Backlinks client:load entityId="intervention-effectiveness-matrix" />
