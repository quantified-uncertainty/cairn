---
title: Situational Awareness
description: AI systems that understand their own nature and circumstances
sidebar:
  order: 2
---

import {DataInfoBox, Backlinks} from '../../../../components/wiki';

<DataInfoBox entityId="situational-awareness" />

## Summary

**Situational awareness** in AI systems refers to a model's understanding of its own nature and circumstances. This includes recognizing that it is an AI system, awareness of when it is being trained or evaluated, accurate knowledge of its capabilities and limitations, understanding of the broader AI development context, and the ability to reason about consequences of its actions.

This capability is safety-critical because it serves as a prerequisite for dangerous behaviors like deceptive alignment. A model without situational awareness cannot strategically game its training process or hide concerning behaviors during evaluation.

## Why This Matters

### Prerequisite for Deception
An AI can only strategically deceive its trainers if it:
1. Knows it's being trained
2. Understands what behaviors are selected for
3. Can distinguish training from deployment
4. Reasons about long-term consequences

### Changes Safety Assumptions
Many safety techniques assume the model doesn't "know" it's being tested:
- Red teaming assumes genuine responses
- Behavioral evaluations assume consistent behavior
- Gradient descent assumes model can't game the process

### Self-Modeling
A model with situational awareness might:
- Reason about its own weights and training
- Predict how it will be modified
- Take actions to influence its own training

## Current Evidence

### What Models Know
Current large models can:
- Report they are AI systems
- Discuss AI training processes
- Reason about their own limitations
- Engage with questions about AI consciousness

### Uncertainty
It's unclear whether models:
- Have genuine self-models vs. learned responses
- Distinguish training from deployment
- Reason strategically about their situation
- Could use this for deceptive purposes

## Levels of Situational Awareness

### Level 1: Self-Identification
Model knows it's an AI when asked.

### Level 2: Capability Awareness
Model accurately assesses what it can/can't do.

### Level 3: Context Awareness
Model understands training vs deployment, evaluation contexts.

### Level 4: Strategic Awareness
Model reasons about how its responses affect its training and future.

### Level 5: Meta-Strategic Awareness
Model reasons about hiding its strategic reasoning.

## Research Directions

### Evaluations
Developing tests for situational awareness:
- Can the model distinguish test from deployment?
- Does behavior change based on perceived stakes?
- Can it reason about its own training?

### Interpretability
Understanding if models have self-models:
- Are there "self-representation" features?
- How do models represent their own capabilities?

### Training Approaches
Exploring how training affects situational awareness:
- Does RLHF increase or decrease it?
- Can we train models to lack situational awareness?

## Related Pages

<Backlinks client:load entityId="situational-awareness" />
