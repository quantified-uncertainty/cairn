---
title: Funders
description: Major funders of AI safety research and funding opportunities
sidebar:
  label: Overview
  order: 0
---

import {
  FundersOverview,
  MajorFundersList,
  FundersComparisonTable,
  FundingByCategory,
  GrantSizeBreakdown,
  ApplicationTips,
  TimelineExpectations
} from '../../../../components/FundersList';
import fundersData from '../../../../data/funders.json';
import { Sources } from '../../../../components/wiki';

<FundersOverview client:load data={fundersData} />

## Summary

AI safety funding has grown dramatically since 2015 but remains a small fraction of overall AI investment. Understanding the funding landscape is critical for anyone seeking to work on AI safety.

### Growth Trends

AI safety funding has increased approximately **10-20x since 2019**, driven by:
- Growing awareness of AI risks among major philanthropists
- Formation of dedicated safety teams at frontier AI labs
- Increased government interest (AISI, NIST, EU AI Office)
- New funders entering the space (Anthropic LTBT, Schmidt Futures)

### Comparison to Capabilities Funding

AI safety funding remains **~0.1-0.5% of total AI investment**:
- Total AI investment: ~$100-200B/year (venture capital + corporate R&D)
- AI capabilities funding vastly outpaces safety work
- Some argue this ratio should be inverted given the stakes

---

## Major Funders

<MajorFundersList client:load funders={fundersData.majorFunders} />

---

## Funder Comparison

<FundersComparisonTable client:load items={fundersData.comparisonTable} />

---

## Funding by Category

<FundingByCategory client:load categories={fundersData.fundingByCategory} />

---

## Grant Size Breakdown

<GrantSizeBreakdown client:load tiers={fundersData.grantSizeBreakdown} />

---

## How to Get Funded

<ApplicationTips client:load tips={fundersData.applicationTips} />

### Timeline Expectations

<TimelineExpectations client:load timelines={fundersData.timelineExpectations} />

---

## Alternative Funding Routes

### Employment vs. Grants

**Consider employment if**:
- You want to work on specific problems organizations are tackling
- You value mentorship and collaboration
- You prefer stable, long-term funding
- You're early in your career

**Consider grants if**:
- You have a specific research agenda
- You want independence
- You have experience and track record
- You need flexibility

### Regranting Programs

Some organizations accept donations and regrant:
- **Manifund**: Platform for small AI safety grants
- **Effective Altruism Infrastructure Fund**: For community infrastructure
- **University groups**: Some have small grant budgets

### Fellowships and Programs

Funded programs as an alternative to direct grants:
- **MATS**: ML Alignment & Theory Scholars (stipended)
- **AI Safety Camp**: Short programs (volunteer, some travel funding)
- **Bluedot Impact**: Courses with some fellowships
- **Apart Research**: Hackathon-style programs

---

## Resources

### Application Resources
- **EA Forum**: Search for "grant report" to see what gets funded
- **Alignment Forum**: Technical research discussions
- **LessWrong**: AI safety community
- **80,000 Hours**: Career advice including funding options

### Finding Opportunities
- Subscribe to funder newsletters
- Follow AI safety organizations on social media
- Join AI safety Slack/Discord communities
- Attend conferences and workshops

<Sources sources={[
  { title: "Open Philanthropy - AI Risk", url: "https://www.openphilanthropy.org/focus/global-catastrophic-risks/" },
  { title: "Survival and Flourishing Fund", url: "https://survivalandflourishing.fund/" },
  { title: "EA Funds - Long-Term Future Fund", url: "https://funds.effectivealtruism.org/funds/far-future" },
  { title: "80,000 Hours - AI Safety Careers", url: "https://80000hours.org/problem-profiles/artificial-intelligence/" },
  { title: "EA Forum - AI Safety Funding", url: "https://forum.effectivealtruism.org/topics/ai-safety" },
]} />
