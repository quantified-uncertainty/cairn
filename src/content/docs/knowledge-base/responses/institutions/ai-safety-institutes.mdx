---
title: AI Safety Institutes
description: Government institutions for AI safety evaluation and research
sidebar:
  order: 22
quality: 3
styleGuideVersion: kb-2.0
llmSummary: Comprehensive overview of government AI Safety Institutes (AISIs)
  that are building technical capacity within governments to evaluate and
  oversee AI systems. Covers the current landscape of institutes in UK, US, and
  other countries, their evaluation capabilities, structural limitations around
  independence and authority, and their role in the broader AI governance
  ecosystem.
lastEdited: "2025-12-26"
importance: 4
---
import {DataInfoBox, Backlinks} from '../../../../../components/wiki';

<DataInfoBox entityId="ai-safety-institutes" />

## Overview

**AI Safety Institutes (AISIs)** are government-affiliated institutions dedicated to evaluating AI systems, conducting safety research, and advising on AI policy. They represent a significant development in AI governance: **building technical capacity within government** to understand and oversee advanced AI systems.

The AISI model emerged in 2023-2024 and is spreading rapidly, with plans for an international network of coordinating institutes.

### Why They Exist

Traditional regulatory approaches face fundamental challenges with AI:

- **Information asymmetry**: Labs understand their systems far better than regulators
- **Rapid advancement**: Capabilities evolve faster than policy can respond
- **Technical complexity**: Evaluating AI safety requires deep ML expertise
- **Access requirements**: Meaningful evaluation requires model access

AISIs aim to address these gaps by building in-house technical expertise, securing access to frontier models, developing evaluation methodologies, and informing policy with technical understanding.

### Quick Assessment

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| Tractability | Medium-High | Already established, growing rapidly |
| If alignment hard | Medium | Provides evaluation but not solutions |
| If alignment easy | Medium | Still valuable for verification |
| Neglectedness | Low | Major government investment in UK/US |
| Grade | B+ | Promising infrastructure, authority unclear |

### Risks Addressed

| Risk | Mechanism | Effectiveness |
|------|-----------|---------------|
| [Deceptive Alignment](/knowledge-base/risks/accident/deceptive-alignment/) | Pre-deployment evaluation for deceptive behaviors | Medium |
| [Scheming](/knowledge-base/risks/accident/scheming/) | Red-teaming and adversarial testing | Medium |
| [Racing Dynamics](/knowledge-base/risk-factors/racing-dynamics/) | Independent evaluation could slow unsafe deployments | Low-Medium |
| [Bioweapons](/knowledge-base/risks/misuse/bioweapons/) | Dangerous capability evaluations | Medium |
| [Cyberweapons](/knowledge-base/risks/misuse/cyberweapons/) | Cyber offense capability testing | Medium |

---

## The Current Landscape

### UK AI Safety Institute

The most established AISI, announced at the November 2023 Bletchley Summit.

**Leadership**: Ian Hogarth (Chair), Yuntao Bai (Research Lead, formerly Anthropic)

**Staff**: ~50+ and growing rapidly

**Key activities**:
- Pre-deployment evaluations of frontier models
- Research on dangerous capabilities
- Development of the Inspect evaluation framework (open-source)
- International coordination leadership

**Model access**: Agreements with Anthropic, OpenAI, Google DeepMind, and Meta for pre-deployment access.

**Organizational status**: Housed within DSIT (Department for Science, Innovation and Technology), relatively independent but government-funded.

### US AI Safety Institute

Established February 2024 within NIST (National Institute of Standards and Technology).

**Leadership**: Elizabeth Kelly (Director), Paul Christiano (Head of AI Safety, formerly Anthropic/ARC)

**Staff**: ~30+, authorized for 100+

**Key activities**:
- Developing evaluation guidelines
- Coordinating with UK AISI
- Supporting Executive Order implementation

**Challenges**: Slower to establish than UK counterpart due to federal bureaucratic constraints (hiring, procurement) and political uncertainty about future.

### Other Institutes

**Japan AI Safety Institute** (February 2024): Within IPA, focused on evaluation methodologies and international coordination. Earlier stage than UK/US.

**Singapore AI Safety Institute** (planning stage): Focused on Asia-Pacific regional coordination.

**Proposed/Planned**: Canada, France, Germany, Australia, South Korea, and a potential EU-level body are at various stages of discussion.

### International Network

At the May 2024 Seoul AI Safety Summit, participating countries agreed to establish an **International Network of AI Safety Institutes** with information sharing protocols, coordinated evaluation approaches, personnel exchanges, and joint research initiatives.

Current coordination includes:
- **Bilateral**: UK-US MOU on AI safety cooperation, shared evaluation frameworks
- **Multilateral**: Summit-based coordination (Bletchley → Seoul → Paris), working groups on specific topics

**Harmonization challenges** include different national priorities, varying levels of model access, classification constraints, and jurisdictional limits.

---

## How AISIs Work

### Core Functions

1. **Safety Evaluations**: Pre-deployment testing of frontier models, capability assessments (dangerous capabilities, dual-use), red-teaming and adversarial testing

2. **Research**: Evaluation methodology development, interpretability and alignment research, risk assessment frameworks

3. **Policy Support**: Technical advice to policymakers, input to standards development, international coordination

4. **Ecosystem Development**: Supporting academic research, training government personnel, building evaluation infrastructure

### Evaluation Capabilities

AISIs are developing evaluation capabilities across two categories:

**Dangerous capabilities**: Biological/chemical weapons uplift, cyber offense capabilities, autonomous replication/resource acquisition, persuasion and manipulation

**Safety properties**: Honesty and truthfulness, refusal of harmful requests, robustness to jailbreaks, instruction following

### Methodological Challenges

Evaluation faces inherent difficulties:

1. **Validity**: Do tests predict real-world risks?
2. **Coverage**: Can't test for unknown capabilities
3. **Gaming**: Models may be optimized against known benchmarks
4. **Generalization**: Lab results may not reflect deployment behavior
5. **Speed**: Evaluations take time; development is fast

### Open Tools

The UK AISI's **Inspect** is an open-source evaluation framework with modular, extensible design supporting multiple model APIs. Making tools open-source helps build community capacity and enables faster iteration than government alone could achieve.

---

## Critical Assessment

### Structural Limitations

**Independence concerns**: Government AISIs face pressure from both industry (which provides model access and talent) and politicians (who control funding). The revolving door with industry may lead to adopting industry framings of safety.

**Authority gap**: Most AISIs are advisory only—they cannot compel model access and have no enforcement mechanisms. Labs cooperate voluntarily, which means AISI influence depends on maintaining good relationships rather than institutional power.

**Scale mismatch**: AISIs have dozens of staff; frontier labs have thousands. AISIs cannot match lab investment in evaluation and remain dependent on lab cooperation. Model access is often limited and conditional—labs control what evaluators see.

### The Central Questions

Whether AISIs succeed depends on how several uncertainties resolve:

**Will they maintain independence?** The UK AISI's relatively independent structure and hiring of respected safety researchers are positive signs, but long-term independence is unproven as political and commercial pressures mount.

**Will they gain real authority?** Meaningful oversight may require mandatory pre-deployment evaluation through legislation. Without enforcement power, AISIs may become safety theater—giving the appearance of oversight without substance.

**Can they keep pace?** If AISIs are always evaluating last quarter's model while labs deploy this quarter's, their assessments become irrelevant. The open-source approach may help by enabling faster community-driven evaluation.

**Will international coordination hold?** Geopolitical tensions, divergent national interests, or changes in government could fragment the network.

### Optimistic vs Pessimistic Views

| Optimistic | Pessimistic |
|------------|-------------|
| Building necessary government capacity | Too small to matter |
| Securing important model access | Access is limited and controlled by labs |
| Informing better policy | Policy moves independently of technical input |
| Creating coordination infrastructure | Coordination is superficial |
| Establishing technical credibility | Captured by industry framing |

---

## Future Directions

AISIs could evolve in several ways:

1. **Regulatory authority**: Gaining enforcement powers rather than remaining advisory
2. **Mandatory access**: Laws requiring pre-deployment evaluation
3. **International treaty**: AISIs as verification bodies for international AI agreements
4. **Expanded mandate**: Beyond evaluation to incident response and ongoing monitoring

The trajectory depends on political will, whether serious AI incidents motivate stronger regulation, and whether AISIs can demonstrate value before capabilities advance too far.

---

## Working at an AISI

AISIs may be a good fit for researchers who want to apply technical AI safety skills in a policy-relevant context, work on evaluation methodologies with real deployment stakes, build government capacity from inside, or contribute to international coordination infrastructure.

**Tradeoffs to consider**:
- Government salaries are typically lower than industry
- Bureaucratic constraints on research directions and hiring
- Potential for political interference
- Risk that your work remains advisory without impact
- Security clearance requirements may slow onboarding

Those who believe technical alignment research is more urgent, or who are pessimistic about government institutions' ability to maintain independence, may prefer working at labs, nonprofits, or academia instead.

---

## Related Interventions

AISIs are one component of a broader governance ecosystem:

- **[Evaluations](/knowledge-base/responses/technical/evals/)**: Technical evaluation methodologies that AISIs apply and develop
- **[Compute Governance](/knowledge-base/responses/governance/compute-governance/)**: Hardware-level governance as a complementary leverage point
- **[International Coordination](/knowledge-base/responses/governance/international/)**: Broader diplomatic frameworks that AISIs contribute to
- **[AI Control](/knowledge-base/responses/technical/ai-control/)**: Control methods whose effectiveness AISI evaluations help validate

## Related Pages

<Backlinks client:load entityId="ai-safety-institutes" />
