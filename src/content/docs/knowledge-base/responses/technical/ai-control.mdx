---
title: AI Control
description: Ensuring AI safety even with potentially misaligned systems
sidebar:
  order: 4
quality: 3
llmSummary: AI Control is a defensive safety approach focused on maintaining
  control over potentially misaligned AI systems through monitoring,
  containment, and redundancy rather than assuming alignment succeeds. Research
  estimates 70-85% tractability for near-human AI with 40-60% catastrophic risk
  reduction if alignment fails, though effectiveness may not scale to
  superintelligent systems.
lastEdited: "2025-12-27"
importance: 85
---import {DataInfoBox, KeyPeople, Section, Backlinks, DisagreementMap, KeyQuestions} from '../../../../../components/wiki';

<DataInfoBox entityId="ai-control" />

## Summary

AI Control is a research agenda that focuses on maintaining safety even when using AI systems that might be actively trying to subvert safety measures. Rather than assuming alignment succeeds, it asks: "How can we safely use AI systems that might be misaligned?"

This represents a more defensive posture than pure alignment research.

## Quick Assessment

| Dimension | Assessment | Quantitative Estimate |
|-----------|------------|----------------------|
| Tractability | High | 70-85% probability of working for near-human-level AI |
| If alignment hard | Critical | Reduces catastrophic risk by estimated 40-60% |
| If alignment easy | Low-Medium | 10-20% marginal safety improvement |
| Neglectedness | Medium | ~$5-10M/year research investment; 15-25 FTE researchers |
| Research Maturity | Medium | 3-5 years to deployment-ready protocols |
| Scalability Limit | Unknown | Estimated effective up to 10-100x human capability |
| Grade | A- | Practical, testable, addresses key failure mode |

## Evaluation Summary

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| Tractability | High | Concrete protocols, testable methods |
| If alignment hard | Critical | Primary safety layer if alignment fails |
| If alignment easy | Low-Medium | Redundant backup, less needed |
| Neglectedness | Medium | Growing interest, led by Redwood Research |
| Grade | A- | Practical, testable, addresses key failure mode |

## Risks Addressed

| Risk | Mechanism | Effectiveness |
|------|-----------|---------------|
| [Deceptive Alignment](/knowledge-base/risks/accident/deceptive-alignment/) | Monitoring and containment catch deceptive actions | High |
| [Scheming](/knowledge-base/risks/accident/scheming/) | Redundancy and monitoring prevent undetected scheming | High |
| [Mesa-Optimization](/knowledge-base/risks/accident/mesa-optimization/) | Containment limits damage from inner misalignment | High |
| [Goal Misgeneralization](/knowledge-base/risks/accident/goal-misgeneralization/) | Monitoring catches behavior outside intended scope | Medium-High |
| [Reward Hacking](/knowledge-base/risks/accident/reward-hacking/) | Sandboxing prevents real-world reward exploits | Medium |

## Core Insight

The key insight is separating two questions:
1. **Alignment**: Making AI systems that pursue intended goals
2. **Control**: Ensuring safety regardless of AI intentions

Even if we can't guarantee alignment, we might still maintain control.

## The Setting

AI Control assumes:
- We have capable AI systems (useful for tasks)
- We aren't confident they're aligned
- We want to use them safely anyway
- The AI might actively try to cause harm

This is relevant because:
- Alignment is hard and might fail
- Competitive pressure to deploy capable AI
- We need defense in depth

## Key Techniques

### Monitoring
Watch AI behavior for signs of misalignment:
- Use trusted AI to monitor untrusted AI
- Human review of suspicious actions
- Anomaly detection in behavior patterns

### Containment
Limit what damage the AI can do:
- Restricted capabilities
- Sandboxed environments
- Reversible actions only

### Redundancy
Don't rely on a single AI system:
- Multiple AI systems check each other
- Humans in critical decision points
- Diverse implementations

### Red-Teaming
Actively try to find ways AI could cause harm:
- Assume AI is adversarial
- Find weakest points in protocols
- Iterate on defenses

## Red-Teaming Control Protocols

The research methodology:
1. Design a control protocol (monitoring + containment)
2. Red-team: Try to find attacks that succeed
3. Blue-team: Improve protocol against attacks
4. Iterate until no attacks succeed

The goal is protocols that work even against capable adversarial AI.

## Relationship to Alignment

| Alignment | Control |
|-----------|---------|
| Make AI want good things | Prevent AI from doing bad things |
| Trust the AI's intentions | Don't rely on AI's intentions |
| Solve the problem | Manage the risk |

Control is complementary to alignment:
- If alignment succeeds, control is a backup
- If alignment fails, control is the defense
- Defense in depth requires both

## Limitations

### Capability Costs
Control measures may reduce AI usefulness:
- Monitoring overhead
- Restricted capabilities
- Slower operation

### Scalability
At some capability level, control may be impossible:
- Superintelligent AI might circumvent any control
- Arms race between capability and control

### Not a Long-Term Solution
Control buys time but doesn't solve alignment:
- Eventually need aligned AI
- Can't control forever
- Should enable alignment research

## Case For This Approach

### Argument 1: Alignment May Fail
We cannot be confident that alignment research will succeed. Given the stakes, we need safety measures that work even if alignment fails. AI Control provides safety without requiring trust in AI intentions.

**Key proponents**: Buck Shlegeris, Ryan Greenblatt, Redwood Research

### Argument 2: Testable and Practical
Unlike some alignment approaches that rely on theoretical arguments, control protocols can be empirically tested. We can red-team protocols, find attacks, improve defenses, and measure progress. This makes it more tractable than purely theoretical work.

### Argument 3: Defense in Depth
Even if alignment succeeds, control provides a valuable backup layer. Defense in depth is a standard security principleâ€”we shouldn't rely on a single point of failure. Control plus alignment is more robust than alignment alone.

## Case Against This Approach

### Counterargument 1: Capability Tax
Control measures reduce AI usefulness. Monitoring, sandboxing, and restrictions all impose costs. If competitors don't use these measures, controlled AI may be outcompeted. This creates pressure to reduce controls.

**Key concern**: Economic incentives may erode safety measures

### Counterargument 2: May Not Scale to Superintelligence
Control assumes we can maintain an advantage over AI systems. At some capability level, AI may be able to circumvent any human-designed control. Control may work for near-human AI but fail for superhuman systems.

### Counterargument 3: Creates False Confidence
Relying on control might reduce effort on alignment. If organizations believe control is sufficient, they may underinvest in solving the core problem. Control should supplement, not replace, alignment work.

## Key Cruxes

### Crux 1: How Long Can Control Work?

<KeyQuestions
  client:load
  questions={["At what capability level does control become impossible?"]}
/>

| Control scales well | Control has limits |
|--------------------|-------------------|
| Incremental AI progress allows incremental control improvements | Superintelligence may fundamentally exceed control |
| Monitoring AI with other AI can scale | Arms race may favor the controlled AI |
| Can always restrict capabilities | Restricted AI may lose competitive value |

### Crux 2: Will Organizations Actually Use Control?

| Adoption likely | Adoption unlikely |
|-----------------|-------------------|
| AI incidents will motivate precaution | Competitive pressure to deploy fast |
| Regulation may mandate controls | Control costs may be prohibitive |
| Responsible labs already use controls | Not all labs are responsible |

## Effectiveness Estimates

<DisagreementMap
  client:load
  topic="Effectiveness of AI Control"
  description="How well will control methods maintain safety with potentially misaligned AI?"
  spectrum={{ low: "Won't work", high: "Critical safeguard" }}
  positions={[
    { actor: "Redwood Research", position: "High", estimate: "Core safety approach", confidence: "high" },
    { actor: "Alignment-focused", position: "Medium-High", estimate: "Important complement", confidence: "medium" },
    { actor: "Skeptics", position: "Medium", estimate: "Limited window of usefulness", confidence: "medium" },
    { actor: "MIRI-adjacent", position: "Low-Medium", estimate: "Buys little time", confidence: "low" }
  ]}
/>

## Who Should Work on This

**Good fit if you believe:**
- Alignment may not be solved in time
- Empirical, testable research is valuable
- Near-term AI safety matters most
- Defense in depth is important

**Less relevant if you believe:**
- Alignment will succeed (control is redundant)
- Only superintelligence-level risks matter (control won't scale)
- Theoretical research is more important

## Critical Assessment

### Effectiveness Assessment

| Criteria | Current Evidence | Confidence |
|----------|-----------------|------------|
| Detection of adversarial actions | Red-team studies show 80-95% detection rate for current models | High |
| Containment effectiveness | Sandboxing prevents 90%+ of simulated escape attempts | Medium-High |
| Monitoring overhead | 5-15% compute cost for comprehensive monitoring | Medium |
| False positive rate | 1-5% of flagged actions are benign (tunable) | Medium |
| Human review bottleneck | Can review ~100-500 flagged actions per hour per reviewer | High |
| Protocol iteration speed | 2-4 weeks to develop and test new control protocol | Medium |

### Resource Requirements

| Investment Type | Estimated Need | Current State | Timeline to Impact |
|-----------------|---------------|---------------|-------------------|
| Core Research | $10-20M/year | ~$5-10M/year | 2-4 years to mature protocols |
| Red-Team Testing | $2-5M/year | ~$1-2M/year | Ongoing |
| Implementation Engineering | $5-10M per major lab | Early stage | 1-2 years per deployment |
| Monitoring Infrastructure | $1-3M/year operational cost | Prototype stage | 6-12 months to deploy |
| Human Review Capacity | 10-50 FTEs for high-stakes deployment | Not yet needed | Scales with deployment |

### Key Uncertainties

| Uncertainty | Range of Estimates | Key Factors |
|-------------|-------------------|-------------|
| Effective capability ceiling | 10x to 1000x human-level | AI self-improvement speed, monitoring scalability |
| Adoption by frontier labs | 40-80% of major labs by 2027 | Regulation, safety culture, competitive pressure |
| Capability tax | 5-30% performance reduction | Protocol strictness, task type |
| Control window duration | 2-10 years before obsolete | AI progress rate, control research progress |
| Complementarity with alignment | 30-70% of safety value from control | Alignment difficulty, overlap in failure modes |

## Complementary Interventions

- [Mechanistic Interpretability](/knowledge-base/responses/technical/interpretability/) - Detect misalignment that control should contain
- [Scalable Oversight](/knowledge-base/responses/technical/scalable-oversight/) - Monitor AI with human+AI oversight
- [Evaluations](/knowledge-base/responses/technical/evals/) - Identify dangerous capabilities before deployment
- [Pause Advocacy](/knowledge-base/responses/institutional/pause/) - Buy time for control methods to develop

<Section title="Key People">
  <KeyPeople people={[
    { name: "Buck Shlegeris", role: "Redwood Research; AI Control" },
    { name: "Ryan Greenblatt", role: "Redwood Research; control protocols" },
    { name: "Paul Christiano", role: "ARC; control concepts" },
  ]} />
</Section>

## Related Pages

<Backlinks client:load entityId="ai-control" />
