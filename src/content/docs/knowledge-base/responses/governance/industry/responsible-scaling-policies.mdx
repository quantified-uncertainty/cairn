---
title: Responsible Scaling Policies
description: Industry self-regulation frameworks for frontier AI development
sidebar:
  order: 8
quality: 3
llmSummary: Analyzes Responsible Scaling Policies as industry self-regulation
  frameworks where AI labs commit to evaluating dangerous capabilities and
  implementing safeguards at specific thresholds. Estimates 10-25% risk
  reduction but notes weak enforcement and self-policing limitations.
lastEdited: "2025-12-27"
importance: 4
---
import {DataInfoBox} from '../../../../../../components/wiki';

<DataInfoBox entityId="responsible-scaling-policies" />

## Quick Assessment

| Dimension | Assessment | Quantitative Estimate |
|-----------|------------|----------------------|
| **Adoption** | Moderate | 3-4 frontier labs with published RSPs (Anthropic, OpenAI, DeepMind) |
| **Tractability** | High | Framework can be implemented in 3-6 months per lab |
| **Enforcement** | Weak | Self-policed; 0% external enforcement currently |
| **Coverage** | Partial | ~60-70% of frontier model development covered |
| **Estimated Risk Reduction** | 10-25% | Medium confidence; depends on commitment durability |

## Summary

**Responsible Scaling Policies (RSPs)** are voluntary commitments by frontier AI labs to evaluate AI systems for dangerous capabilities and implement appropriate safeguards before deployment or further scaling.

The core idea: define **capability thresholds** that, if crossed, trigger **mandatory safety measures**. This creates a structured approach to managing risks as AI systems become more capable.

## The RSP Framework

### AI Safety Levels (ASLs)

Anthropic introduced the concept of **AI Safety Levels**, analogous to biosafety levels:

| Level | Capability | Required Safeguards |
|-------|-----------|---------------------|
| ASL-1 | No meaningful catastrophic risk | Standard practices |
| ASL-2 | Some dangerous knowledge, no uplift | Current security, deployment controls |
| ASL-3 | Meaningful uplift for CBRN/cyber | Enhanced security, evaluation protocols |
| ASL-4 | Could cause catastrophic harm autonomously | Not yet defined; likely extensive |

**Current status**: Most frontier models are assessed at ASL-2; labs are preparing ASL-3 protocols.

### Evaluation → Safeguard Pipeline

1. **Define thresholds**: What capabilities would require enhanced safeguards?
2. **Evaluate models**: Test for those capabilities before training/deployment
3. **Implement safeguards**: If thresholds crossed, deploy corresponding measures
4. **Don't proceed without safeguards**: Pause scaling until safeguards ready

## Lab-Specific Implementations

### Anthropic RSP

**Published**: September 2023 (updated 2024)

**Key elements**:
- AI Safety Levels framework
- Evaluation protocols for dangerous capabilities
- Commitment not to deploy ASL-3+ without corresponding safeguards
- Third-party audits of evaluations
- Public reporting on capability assessments

**Triggers for ASL-3**:
- Meaningful uplift for CBRN weapons creation
- Significant cyber-offense capabilities
- Ability to autonomously replicate and acquire resources

### OpenAI Preparedness Framework

**Published**: December 2023

**Key elements**:
- Four risk categories: Cybersecurity, CBRN, Persuasion, Model Autonomy
- Risk levels: Low, Medium, High, Critical
- **Deployment threshold**: Only deploy models at "Medium" or below
- **Development threshold**: Only continue training if post-mitigation risk is "High" or below
- Safety Advisory Group reviews all assessments

**Governance**:
- Preparedness team conducts evaluations
- Safety Advisory Group reviews and can escalate
- Board has final authority on "Critical" risk decisions

### Google DeepMind Frontier Safety Framework

**Published**: May 2024

**Key elements**:
- **Critical Capability Levels (CCLs)** for different risk domains
- Evaluations before training and deployment decisions
- Security and deployment mitigations corresponding to capability levels
- Commitment to not develop models exceeding mitigatable risk levels

**Risk domains**:
- Autonomy and self-proliferation
- Biosecurity
- Cybersecurity
- Machine learning R&D

## Critiques and Limitations

### Strengths of RSPs

- **Structured approach**: Clear framework for managing scaling risks
- **Proactive**: Address risks before they materialize
- **Adaptable**: Can be updated as understanding improves
- **Transparency**: Public commitments create accountability

### Weaknesses and Critiques

**Self-regulation concerns**:
- Labs evaluate their own models (conflict of interest)
- Commitments are voluntary (can be abandoned under competitive pressure)
- No external enforcement mechanism
- Commercial pressures may compromise rigor

**Technical limitations**:
- Capability evaluations may miss dangerous capabilities
- Thresholds may be set too high
- Safeguards may be insufficient for actual risks
- Evaluations can't predict emergent capabilities

**Governance gaps**:
- RSPs don't address structural risks (concentration, racing)
- No coordination between labs on thresholds
- Board oversight may be insufficient

## RSPs vs. Government Regulation

| Aspect | RSPs | Government Regulation |
|--------|------|----------------------|
| Speed | Can be updated quickly | Slow legislative process |
| Enforcement | Self-enforced | Legal penalties |
| Expertise | Labs understand models | Regulators may lack expertise |
| Conflicts | Self-interested | Public interest (in theory) |
| Coverage | Only participating labs | All entities in jurisdiction |

## Evolution and Future

RSPs are evolving rapidly:

**2023**: Initial frameworks published
**2024**: Refinements, more detailed thresholds, third-party evaluation discussions
**2025+**: Expected developments:
- More sophisticated evaluation techniques
- Cross-lab coordination on standards
- Government integration (RSPs as compliance with regulations)
- International harmonization efforts

## Assessment

RSPs represent a **significant step** toward structured AI safety management, but face fundamental limitations as **self-regulation**:

- **Best case**: RSPs establish norms that governments codify into law
- **Likely case**: RSPs provide some constraint but weaken under competitive pressure
- **Worst case**: RSPs serve as "safety-washing" while providing inadequate protection

Most AI safety experts view RSPs as **necessary but insufficient**—a useful tool that should complement, not replace, government oversight.

## Critical Assessment

### Effectiveness Assessment

| Criterion | Current Evidence | Estimated Effectiveness | Confidence |
|-----------|------------------|------------------------|------------|
| Capability evaluations conducted | Yes, at major labs | 50-70% of dangerous capabilities tested for | Medium |
| Thresholds appropriately set | Uncertain | 30-50% chance thresholds are too permissive | Low |
| Safeguards actually implemented | Some evidence | 60-80% compliance with own commitments | Low-Medium |
| Durability under competitive pressure | Untested | 40-60% survival probability in race scenario | Low |
| External verification | Limited | Less than 20% of evaluations independently verified | Medium |

### Resource Requirements

| Component | Investment Needed | Timeline | Expected Impact |
|-----------|------------------|----------|-----------------|
| Evaluation team (per lab) | $5-20M/year | 6-12 months to build | 10-20 evaluations per model |
| Third-party auditing | $1-5M/year per lab | 3-6 months | 30-50% increase in credibility |
| Cross-lab coordination | $2-5M/year total | 1-2 years | Standardized thresholds, reduced gaming |
| Government integration | $10-50M (regulatory setup) | 2-4 years | Legal enforcement mechanism |
| Red teaming (external) | $500K-2M per model | 1-3 months per model | Identify 10-30% additional vulnerabilities |

### Key Uncertainties

| Uncertainty | Range of Estimates | Impact on Assessment |
|-------------|-------------------|---------------------|
| Evaluation completeness | Can current evals catch 30-80% of dangerous capabilities? | High - determines if RSPs actually detect risks |
| Competitive defection | 20-60% chance a major lab abandons RSP under pressure | High - single defection undermines entire framework |
| Emergent capability gap | Capabilities may emerge unpredictably; 30-50% of risks may be missed | High - fundamental limitation of threshold-based approach |
| Safeguard sufficiency | Even with detected risks, safeguards may be 40-70% effective | Medium - deployment controls are imperfect |
| ASL threshold accuracy | ASL-3/4 boundaries may be off by 1-2 capability levels | Medium - could over- or under-regulate |

## Risks Addressed

| Risk | Mechanism | Effectiveness |
|------|-----------|---------------|
| [Bioweapons](/knowledge-base/risks/misuse/bioweapons/) | CBRN capability evaluations before deployment | Medium |
| [Cyberweapons](/knowledge-base/risks/misuse/cyberweapons/) | Cyber capability evaluations | Medium |
| [Deceptive Alignment](/knowledge-base/risks/accident/deceptive-alignment/) | Autonomy and deception evaluations | Low-Medium |

