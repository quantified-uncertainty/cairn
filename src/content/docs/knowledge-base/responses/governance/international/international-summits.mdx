---
title: "International AI Safety Summits"
description: "Global diplomatic initiatives bringing together 28+ countries and major AI companies to establish international coordination on AI safety, producing non-binding declarations and institutional capacity building through AI Safety Institutes. Bletchley (2023), Seoul (2024), and Paris (2025) summits achieved formal recognition of catastrophic AI risks, with 16 companies signing Frontier AI Safety Commitments, though US and UK refused to sign Paris declaration."
sidebar:
  order: 8
quality: 82
llmSummary: "International AI Safety Summits (Bletchley 2023, Seoul 2024, Paris 2025) achieved first formal recognition of catastrophic AI risks from 28+ countries including US and China, establishing 10+ AI Safety Institutes with $200-400M combined budgets and voluntary commitments from 16 major AI companies, though all agreements remain non-binding. Paris 2025 marked divergence as US and UK refused to sign inclusive AI declaration while China signed. UK AISI evaluations show cyber task completion rising from 9% (2023) to 50% (2025)."
lastEdited: "2025-12-28"
importance: 78.5
---
import {DataInfoBox, Backlinks, Mermaid} from '../../../../../../components/wiki';

<DataInfoBox entityId="international-summits" />

## Summit Series Overview

| Summit | Date | Location | Participants | Key Outcome | Declaration Signatories |
|--------|------|----------|--------------|-------------|------------------------|
| **Bletchley Park** | Nov 1-2, 2023 | UK | 100+ delegates | Bletchley Declaration on AI risks | 28 countries + EU |
| **Seoul** | May 21-22, 2024 | South Korea | Lower attendance | Frontier AI Safety Commitments | 27 countries + EU |
| **Paris** | Feb 10-11, 2025 | France | 1,000+ stakeholders | Statement on Inclusive AI | 58 countries (excl. US, UK) |
| **Delhi** (planned) | Feb 2026 | India | TBD | AI Impact focus | TBD |

## Comprehensive Overview

The **International AI Safety Summit series** represents humanity's first sustained diplomatic effort to coordinate global governance of advanced artificial intelligence systems that could pose catastrophic or existential risks. Beginning with the Bletchley Park Summit in November 2023, these convenings have brought together 28+ countries—including major AI powers like the United States, China, United Kingdom, and European Union—alongside leading AI companies, researchers, and civil society organizations to establish international dialogue on AI safety.

These summits matter profoundly for AI safety because they represent the primary mechanism through which governments are attempting to coordinate responses to potentially catastrophic AI risks. Unlike previous technology governance efforts that emerged after harms materialized, the summits represent an unprecedented attempt at proactive international cooperation on a rapidly advancing technology with global implications. They have produced several key outcomes: formal recognition by major powers that AI poses "potential for serious, even catastrophic harm"; establishment of 3-5 AI Safety Institutes with combined annual budgets of \$100-400 million; voluntary commitments from 16 major AI companies on safety practices; and ongoing diplomatic channels for coordination despite geopolitical tensions.

However, the summits face fundamental limitations that constrain their immediate impact on existential risk reduction. All commitments remain voluntary and non-binding, with no enforcement mechanisms or penalties for non-compliance. Geopolitical tensions, particularly between the US and China, limit the depth of cooperation possible. The summits must balance speed of technological development—where AI capabilities advance weekly—against the inherently slow pace of international diplomacy that operates on annual cycles. Current assessments suggest the summits contribute an estimated 15-30% toward eventual binding international frameworks, serving as necessary but insufficient steps toward comprehensive AI governance.

## The Bletchley Park Breakthrough (November 2023)

The inaugural AI Safety Summit at Bletchley Park from November 1-2, 2023, marked a watershed moment in international AI governance. Hosted by the UK government at the historic World War II codebreaking center, the summit achieved something unprecedented: getting 28 countries, including both the United States and China, to formally acknowledge that advanced AI poses catastrophic risks requiring international cooperation. The summit's symbolic location—where Alan Turing and colleagues broke the Enigma code—underscored the gravity with which governments approached this emerging technology challenge.

The summit's core achievement was the **Bletchley Declaration**, signed by all 28 participating countries plus the EU, which contained the first formal international recognition that frontier AI models pose "potential for serious, even catastrophic, harm, either deliberate or unintentional." Signatories included the US, UK, China, France, Germany, India, Japan, Australia, Singapore, UAE, Brazil, Canada, Saudi Arabia, and the Republic of Korea. This language represented a diplomatic breakthrough, as it required countries with vastly different political systems and values to agree on the reality of AI catastrophic risks. The declaration specifically identified risks including cybersecurity threats, biotechnology misuse, widespread misinformation, and crucially, "loss of control of AI systems"—the first time loss of control scenarios received formal international acknowledgment.

Beyond symbolic recognition, Bletchley produced concrete institutional commitments. The UK announced creation of its AI Safety Institute with an initial £100 million investment in the Frontier AI Taskforce—the largest funding for AI safety by any country at that time. The United States committed to establishing its own AI Safety Institute, formalized shortly after in President Biden's October 30, 2023 Executive Order. Most significantly for the AI industry, major companies including OpenAI, Google DeepMind, Anthropic, Microsoft, and Meta pledged to provide pre-deployment access to their most advanced models for safety testing by government institutes—marking the first formal agreement for government oversight of frontier AI development.

## Seoul's Operational Framework (May 2024)

The Seoul AI Safety Summit on May 21-22, 2024, represented the crucial transition from principles to operational frameworks. Co-hosted by South Korea and the UK, Seoul expanded participation while focusing specifically on frontier AI systems—defined as models requiring over 10^26 floating-point operations for training. This technical specificity reflected growing government sophistication in understanding AI capabilities and risks, moving beyond general statements to precise technological parameters.

Seoul's headline achievement was securing **Frontier AI Safety Commitments** from 16 leading AI companies. These commitments went beyond Bletchley's access provisions to establish ongoing accountability frameworks. Companies pledged to "invest proportionate resources in AI safety research," "work with governments, civil society, and other stakeholders," and crucially, "publish frameworks detailing how they will approach frontier AI safety." In the extreme, organizations commit not to develop or deploy a model if mitigations cannot keep risks below defined thresholds.

### Frontier AI Safety Commitments Signatories

| Company | Headquarters | Published Safety Framework |
|---------|--------------|---------------------------|
| Amazon | US | Yes |
| Anthropic | US | Yes (RSP) |
| Cohere | Canada | Yes |
| G42 | UAE | Yes |
| Google DeepMind | US/UK | Yes |
| IBM | US | No |
| Inflection AI | US | No |
| Meta | US | Yes |
| Microsoft | US | Yes |
| Mistral AI | France | No |
| Naver | South Korea | Yes |
| NVIDIA | US | Yes (joined later) |
| OpenAI | US | Yes |
| Samsung Electronics | South Korea | No |
| Technology Innovation Institute | UAE | No |
| xAI | US | Yes |
| Zhipu.ai | China | No |

The summit also launched the **International AI Safety Research Network**, connecting AI Safety Institutes globally to share evaluation methodologies, coordinate pre-deployment testing, and develop common technical standards. This network represents the operational backbone of international AI safety coordination, enabling real-time information sharing about emerging capabilities and risks. Ten countries and the EU pledged to establish AI Safety Institutes. Japan, South Korea, Singapore, France, and Canada launched institutes, while the EU announced its newly established AI Office would fulfill that role.

Seoul demonstrated the evolution of international AI governance from high-level diplomacy to technical cooperation. Working groups emerged focused on concrete challenges: developing standardized evaluation protocols for dangerous capabilities, establishing information-sharing frameworks that protect commercial interests while enabling safety coordination, and creating mechanisms for rapid response if concerning capabilities emerge. Notably, while attending the summit, China abstained from the Seoul Declaration commitments, signaling early divergence in participation.

## Paris AI Action Summit: Divergence Emerges (February 2025)

The AI Action Summit in Paris on February 10-11, 2025, co-chaired by French President Emmanuel Macron and Indian Prime Minister Narendra Modi, marked a significant shift in the summit process. Renamed from "AI Safety Summit" to "AI Action Summit," the gathering brought together over 1,000 participants from more than 100 countries. However, the summit revealed growing fractures in international AI governance alignment.

The central outcome was the **Statement on Inclusive and Sustainable Artificial Intelligence for People and the Planet**, signed by 58 countries including France, China, and India. The statement outlined principles including accessibility, ethical and trustworthy AI development, avoiding market concentration, positive labor market outcomes, environmental sustainability, and international cooperation. Critically, **the United States and United Kingdom refused to sign**—the first time either country declined such global AI statements since the summit series began.

US Vice President J.D. Vance addressed world leaders stating, "This administration will ensure that American AI technology continues to be the gold standard worldwide," while denouncing regulation that could hamper growth. Vance made clear the Trump Administration "cannot and will not" accept foreign governments "tightening the screws on US tech companies." The American delegation specifically objected to references to AI existential risk, environmental impact, and a role for the UN.

### Paris Summit Key Initiatives

| Initiative | Description | Funding/Scale |
|------------|-------------|---------------|
| **Current AI Foundation** | Support for AI "public goods" including datasets and open-source tools | \$100M initial investment |
| **Coalition for Sustainable AI** | Environmental impact partnership led by France, UNEP, and ITU | 11 countries, 5 intl. orgs, 37 companies |
| **International AI Safety Report** | Expert report on AI capabilities and risks | 96 experts from 30 countries |
| **French Investment Commitments** | Total AI investment secured | €109B over coming years |

The Current AI foundation is backed by nine governments (Finland, France, Germany, Chile, India, Kenya, Morocco, Nigeria, Slovenia, Switzerland) plus philanthropies and companies including Google and Salesforce. China announced establishment of a new body as its answer to AI safety institute calls, though functioning differently from US and UK models. India was announced as host of the next summit—the AI Impact Summit—in February 2026.

## Institutional Infrastructure and AI Safety Institutes

The summit series has catalyzed unprecedented institutional capacity building for AI safety governance. The **UK AI Safety Institute** (renamed AI Security Institute in February 2025), operational since late 2023, has conducted pre-deployment evaluations of frontier models including OpenAI's o1 and Anthropic's Claude 3.5 Sonnet. In May 2024, AISI released the open-source **Inspect AI Framework** for standardized safety evaluations. The institute's annual budget is approximately £50 million (~\$15 million), making it an outlier among AI Safety Institutes.

### UK AISI Frontier AI Trends Report (December 2025)

The UK AISI's inaugural evidence-based assessment revealed rapid capability advancement:

| Capability Area | 2023 Performance | 2025 Performance | Trend |
|-----------------|------------------|------------------|-------|
| Cyber (apprentice-level tasks) | ~9% success | ~50% success | Doubling every 8 months |
| Cyber (expert-level tasks) | 0% | First completion in 2025 | New threshold crossed |
| Software engineering (hour-long tasks) | &lt;5% completion | >40% completion | 8x improvement |
| Biology/chemistry knowledge | Below PhD-level | Outperforming PhD researchers | Exceeds human expert |
| Safeguard robustness | Baseline | 40x variation in jailbreak effort | Improving but variable |

Key finding: "The duration of some cyber tasks that AI systems can complete without human direction is roughly doubling every eight months." However, vulnerabilities have been found in every system tested.

The **US AI Safety Institute**, established within NIST, initially received \$10 million in March 2024—observers noted this was relatively small given the presence of major AI companies in the US. The Biden administration requested \$17.7 million increase for FY2025, but congressional appropriators cut further. In 2025, the US AI Safety Institute was renamed the **Center for AI Standards and Innovation (CAISI)**, with its mission transformed under the Trump administration.

Beyond these flagship institutes, the summit process has spurred institutional development across multiple countries. The **EU AI Office**, while primarily focused on implementing the AI Act, has expanded its mandate to include frontier AI evaluation. Japan, South Korea, Singapore, France, and Canada have launched AI Safety Institutes. Established AISIs report similar funding and staff size, with annual budgets generally around \$10 million with notable exceptions (UK significantly higher).

This institutional infrastructure represents the most tangible outcome of the summit process. Unlike voluntary company commitments or non-binding declarations, AI Safety Institutes create permanent government capacity to understand, evaluate, and potentially regulate advanced AI systems. Their technical work provides the foundation for evidence-based policy making and establishes precedents for how governments can maintain oversight of rapidly advancing AI capabilities.

## Summit Process Evolution

<Mermaid client:load chart={`
flowchart TD
    subgraph BLETCHLEY["Bletchley Park (Nov 2023)"]
        B1[28 Countries + EU Sign Declaration]
        B2[First Recognition of Catastrophic Risk]
        B3[UK/US Announce AI Safety Institutes]
    end

    subgraph SEOUL["Seoul (May 2024)"]
        S1[16 Companies Sign Safety Commitments]
        S2[International AISI Network Launched]
        S3[China Abstains from Declaration]
    end

    subgraph PARIS["Paris (Feb 2025)"]
        P1[58 Countries Sign Statement]
        P2[US and UK Refuse to Sign]
        P3[\$100M Current AI Foundation]
    end

    subgraph DELHI["Delhi (Feb 2026)"]
        D1[AI Impact Summit]
        D2[Focus TBD]
    end

    BLETCHLEY --> SEOUL
    SEOUL --> PARIS
    PARIS --> DELHI

    style B1 fill:#90EE90
    style S1 fill:#90EE90
    style P2 fill:#FFB6C1
    style S3 fill:#FFD700
`} />

The diagram illustrates the evolution from broad consensus at Bletchley to emerging divergence at Paris, with China abstaining at Seoul and the US/UK refusing to sign at Paris—representing a potential fracturing of the initial coalition.

## Industry Engagement and Corporate Commitments

The summit series has achieved unprecedented industry participation in international AI safety governance. The Seoul commitments from 16 major companies (later expanded to 20) represent roughly 80% of global frontier AI development capacity, including OpenAI (GPT family), Google DeepMind (Gemini), Anthropic (Claude), Microsoft, Meta, Amazon, and emerging players like xAI and Mistral. This broad industry engagement reflects both growing corporate recognition of AI risks and strategic calculations about the benefits of cooperative governance versus adversarial regulation.

Corporate commitments fall into several categories with varying implementation prospects. **Transparency commitments** appear most likely to be honored, as companies have begun publishing detailed safety frameworks and capability assessments. OpenAI's "Preparedness Framework," Anthropic's "Responsible Scaling Policy," and Google DeepMind's "Frontier Safety Framework" all emerged partially from summit discussions and represent substantive technical documents outlining safety practices. These publications enable external evaluation of company practices and create reputational pressure for follow-through.

**Pre-deployment testing agreements** represent the most operationally significant commitments, granting AI Safety Institutes access to evaluate models before public release. Early evidence suggests mixed compliance: the UK AI Safety Institute reports conducting evaluations of several frontier models, while some companies have delayed providing access to their most advanced systems. The challenge lies in balancing legitimate commercial confidentiality with meaningful safety evaluation, particularly as model evaluation requires understanding not just capabilities but training processes and intended deployment strategies.

**Investment commitments in safety research** remain difficult to verify without detailed financial disclosure. Industry spending on AI safety has increased substantially, with companies reporting 5-15% of AI research budgets dedicated to safety work, but verification mechanisms remain weak. The summit process has established expectations for safety investment but lacks enforcement mechanisms beyond reputational pressure and potential future regulatory requirements.

## Geopolitical Dynamics: US-China-EU Triangulation

The summit series has revealed complex and shifting geopolitical dynamics, with different powers adopting varied positions across summits:

| Country/Bloc | Bletchley 2023 | Seoul 2024 | Paris 2025 | Current Stance |
|--------------|----------------|------------|------------|----------------|
| **United States** | Signed declaration | Signed; supported commitments | **Refused to sign** | Opposing regulation; renamed AISI |
| **United Kingdom** | Host; signed | Co-host; signed | **Refused to sign** | Renamed AISI to "AI Security Institute" |
| **China** | Signed declaration | **Abstained** from Seoul Declaration | Signed Paris statement | Established own AI safety body |
| **EU** | Signed | Signed; AI Office joined network | Signed | Strong regulatory approach (AI Act) |
| **France** | Signed | Signed | Host; signed | €109B investment commitment |
| **India** | Signed | Signed | Co-chair; signed | Hosting 2026 summit |

China's participation trajectory is particularly notable. Chinese Vice Premier Zhang Guoqing attended Paris, and China signed the inclusive AI statement that the US and UK refused. China announced establishment of a new body as its answer to AI safety institute calls, though functioning differently from Western models. The success of DeepSeek demonstrated that smaller AI outfits can drive innovation, reinforcing China's position that market concentration concerns are legitimate.

The US position shifted dramatically with the Trump administration. Vice President Vance's Paris statement that the administration "cannot and will not" accept foreign governments "tightening the screws on US tech companies" signals a fundamental reorientation away from multilateral AI governance. The American delegation specifically objected to references to AI existential risk, environmental impact, and UN involvement.

Despite these tensions, some cooperation channels remain. The UK and US AI Safety Institutes conducted joint pre-deployment evaluations of OpenAI's o1 and Anthropic's Claude 3.5 Sonnet in late 2024. Technical working groups continue identifying common ground on evaluation methodologies. However, the Paris divergence suggests potential bifurcation into competing governance frameworks—one led by the US emphasizing innovation and market freedom, another involving the EU, China, and Global South emphasizing regulation and equity.

## Current State and Near-Term Trajectory (2025-2026)

Following the Paris summit in February 2025, the summit process has entered a new phase characterized by divergent national approaches. The renaming from "AI Safety Summit" to "AI Action Summit" and the US/UK refusal to sign the Paris declaration signals a potential fracturing of the initial coalition. India will host the next summit—the AI Impact Summit—in February 2026, likely further broadening focus beyond pure safety concerns.

The institutional infrastructure created by the summits is reaching operational maturity but facing political headwinds. The UK AISI's Frontier AI Trends Report (December 2025) provides the most comprehensive government assessment of AI capabilities to date. However, the US AI Safety Institute's transformation into the Center for AI Standards and Innovation (CAISI) under the Trump administration represents a significant shift in US government approach, with reduced emphasis on safety evaluation and increased focus on promoting US AI competitiveness.

Corporate commitments are showing mixed implementation. Twelve of 20 signatory companies have published frontier AI safety policies (as of December 2025): Anthropic, OpenAI, Google DeepMind, Magic, Naver, Meta, G42, Cohere, Microsoft, Amazon, xAI, and NVIDIA. Pre-deployment testing agreements face practical challenges, but the UK/US joint evaluations of Claude 3.5 Sonnet and o1 demonstrate that government-industry cooperation on safety evaluation remains possible despite political shifts.

The most significant development is the emerging governance bifurcation. The US is moving toward a deregulatory approach emphasizing AI competitiveness. The EU continues implementing the AI Act with strong regulatory requirements. China is developing its own governance framework emphasizing AI sovereignty. This fragmentation may lead to a "Brussels effect" dynamic where EU standards shape global practices, or alternatively to regulatory arbitrage where AI development concentrates in the least-regulated jurisdictions.

## Medium-Term Prospects and Binding Frameworks (2025-2030)

The critical question for the summit process is whether voluntary commitments can evolve into binding international agreements capable of meaningfully constraining AI development if catastrophic risks materialize. Historical precedents suggest this transition is possible but difficult, typically requiring 5-15 years of relationship building, technical development, and crisis motivation. The Nuclear Non-Proliferation Treaty required two decades of preliminary efforts; the Montreal Protocol addressing ozone depletion took only two years but addressed a simpler technical problem with clear scientific consensus.

Several factors could accelerate movement toward binding frameworks. A near-miss AI incident—perhaps an AI system causing significant economic damage or demonstrating unexpectedly dangerous capabilities—could provide crisis motivation for stronger international action. Technical advances in AI evaluation and verification could address current challenges in monitoring compliance with safety requirements. Generational changes in government leadership might reduce resistance to international constraints on AI development.

Conversely, several factors could prevent binding agreements. Intensifying geopolitical competition between the US and China could make meaningful cooperation impossible. Rapid AI advancement could outpace diplomatic processes, making governance frameworks obsolete before implementation. Commercial resistance to binding constraints could weaken government commitment to international agreements. Absence of clear AI-caused harm could undermine political momentum for stronger action.

The most likely scenario involves gradual strengthening of existing frameworks rather than comprehensive binding treaties. Technical cooperation through AI Safety Institutes is likely to deepen, creating shared methodologies and informal coordination mechanisms. Industry commitments may become more specific and verifiable, particularly as governments develop stronger oversight capabilities. Regional initiatives—particularly EU regulation and potential US-Canada-UK coordination—could create de facto international standards through market pressure.

## Safety Implications and Risk Assessment

From an AI safety perspective, the summit process generates both encouraging and concerning implications for existential risk reduction. On the positive side, the summits have achieved unprecedented international recognition of AI catastrophic risks, established government institutions with technical capacity to evaluate dangerous AI capabilities, and created diplomatic channels for coordination that could prove crucial if severe risks materialize. The process has also demonstrated that international cooperation on AI safety remains possible despite broader geopolitical tensions.

However, the limitations are severe. All current commitments remain voluntary and non-binding, providing no constraints on actors willing to develop dangerous AI systems regardless of international pressure. The summit process operates on annual timelines while AI capabilities advance continuously, creating fundamental mismatches between governance speed and technological development. Verification and enforcement mechanisms remain weak, potentially enabling cheating or free-riding on safety commitments.

The risk of false assurance represents a particular concern. Successful summits and industry commitments could create public perception that AI risks are being adequately managed, potentially reducing pressure for stronger action. Similarly, the emphasis on international cooperation could delay necessary unilateral actions by countries or companies concerned about AI risks. The summit process might serve corporate interests in avoiding binding regulation while providing minimal actual constraint on dangerous development.

Quantitative risk assessment suggests the summits contribute an estimated 15-30% probability increase toward achieving effective international AI governance by 2030, with wide uncertainty ranges. This contribution operates primarily through institutional capacity building, norm development, and relationship establishment rather than immediate risk reduction. The summits are necessary but insufficient for AI safety, requiring parallel efforts on technical safety research, domestic regulation, and industry safety practices.

## Key Uncertainties and Critical Variables

Several fundamental uncertainties will determine the ultimate success or failure of the international summit process. The **durability of US-China cooperation** represents perhaps the most important variable, as meaningful AI governance likely requires coordination between the world's two leading AI powers. Current engagement remains fragile and could collapse if geopolitical tensions escalate further or if AI technology becomes perceived as decisive for military or economic competition.

**The speed of AI development** relative to governance capability presents another critical uncertainty. If AI systems achieve dangerous capabilities much faster than anticipated—potentially through algorithmic breakthroughs rather than just scaling—the current summit timelines may prove inadequate. Conversely, if AI development plateaus or faces technical barriers, the summit process could have more time to develop effective governance frameworks.

**The occurrence of warning shots or near-miss events** could dramatically accelerate international cooperation or, conversely, trigger competitive dynamics that undermine collaboration. AI systems causing significant harm—whether through accidents, misuse, or unexpected capabilities—could provide the crisis motivation necessary for binding international agreements. However, such events could also trigger national security responses that prioritize competitive advantage over cooperative governance.

**Industry behavior and technological concentration** represent additional critical variables. The current concentration of advanced AI development among a small number of companies makes governance potentially easier but also creates single points of failure. If AI capabilities become more distributed across many actors or countries, international coordination could become much more difficult. Corporate responses to governance pressure—whether cooperative engagement or adversarial resistance—will significantly influence the summit process's effectiveness.

**Technical progress in AI evaluation and safety** could determine whether international governance remains feasible as AI systems become more advanced. Current evaluation methodologies struggle to assess the most dangerous potential capabilities, and verification of safety practices remains challenging. Breakthroughs in AI evaluation, interpretability, or alignment could enable much more effective international oversight, while technical barriers could make governance frameworks increasingly ineffective.

The interaction of these uncertainties creates wide probability distributions around potential outcomes, suggesting that while the summit process has established important foundations for international AI governance, its ultimate impact on existential risk reduction remains highly uncertain and dependent on future developments largely outside the direct control of summit participants.

## Sources and Further Reading

### Official Summit Documents
- [The Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023) - UK Government (November 2023)
- [Seoul Declaration for Safe, Innovative and Inclusive AI](https://www.gov.uk/government/publications/seoul-declaration-for-safe-innovative-and-inclusive-ai-ai-seoul-summit-2024) - UK Government (May 2024)
- [Frontier AI Safety Commitments](https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024) - UK Government (May 2024)

### AI Safety Institute Reports
- [Frontier AI Trends Report](https://www.aisi.gov.uk/frontier-ai-trends-report) - UK AI Security Institute (December 2025)
- [Pre-deployment evaluation of OpenAI's o1 model](https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-openais-o1-model) - UK AISI
- [Inspect AI Framework](https://inspect.aisi.org.uk/) - Open-source AI safety testing platform

### Analysis and Commentary
- [The AI Seoul Summit](https://www.csis.org/analysis/ai-seoul-summit) - CSIS analysis
- [France's AI Action Summit](https://www.csis.org/analysis/frances-ai-action-summit) - CSIS analysis
- [AI Safety Summits Overview](https://futureoflife.org/project/ai-safety-summits/) - Future of Life Institute
- [Common Elements of Frontier AI Safety Policies](https://metr.org/blog/2025-12-09-common-elements-of-frontier-ai-safety-policies/) - METR (December 2025)
- [The Paris Summit: Au Revoir, global AI Safety?](https://www.epc.eu/publication/The-Paris-Summit-Au-Revoir-global-AI-Safety-61ea68/) - European Policy Centre

### Industry Frameworks
- [AI Lab Watch: Company Commitments](https://ailabwatch.org/resources/commitments) - Tracking industry safety commitments
- [Google DeepMind Partnership with UK AISI](https://deepmind.google/blog/deepening-our-partnership-with-the-uk-ai-security-institute/) - DeepMind blog

<Backlinks client:load entityId="international-summits" />