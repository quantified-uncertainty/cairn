---
title: Key Researchers in AI Safety
description: Notable researchers and thought leaders shaping the AI safety and alignment field
sidebar:
  order: 0
---

import { EntityCards, EntityCard, Section } from '../../../../components/wiki';

This section profiles key researchers, thought leaders, and practitioners who have made significant contributions to AI safety, alignment, and governance. Their work spans technical research, strategy, policy, and institution-building.

## Understanding Different Perspectives

The AI safety field encompasses a wide range of views on critical questions. **Timeline estimates** for transformative AI range from 5 to 50+ years. Views on **alignment difficulty** span from "likely solvable with current approaches" to "extremely difficult, may require fundamental breakthroughs." **P(doom)** estimates—the probability of existential catastrophe from AI—range from under 1% to over 50%. And **strategic approaches** differ on whether we should race to build safe AI first, slow down development, focus on governance, or pursue fundamental research.

Understanding who believes what and why is crucial for navigating the field's key disagreements.

## Researchers by Primary Affiliation

<Section title="Frontier Labs (AGI-focused)">
  <EntityCards>
    <EntityCard
      id="dario-amodei"
      category="researcher"
      title="Dario Amodei"
      description="CEO of Anthropic, advocate for Constitutional AI and responsible scaling"
    />
    <EntityCard
      id="jan-leike"
      category="researcher"
      title="Jan Leike"
      description="Head of Alignment at Anthropic, previously led OpenAI's superalignment team"
    />
    <EntityCard
      id="chris-olah"
      category="researcher"
      title="Chris Olah"
      description="Co-founder of Anthropic, pioneer in neural network interpretability"
    />
    <EntityCard
      id="ilya-sutskever"
      category="researcher"
      title="Ilya Sutskever"
      description="Co-founder of Safe Superintelligence Inc., formerly Chief Scientist at OpenAI"
    />
  </EntityCards>
</Section>

<Section title="Safety Research Organizations">
  <EntityCards>
    <EntityCard
      id="paul-christiano"
      category="researcher"
      title="Paul Christiano"
      description="Founder of ARC, creator of iterated amplification and AI safety via debate"
    />
    <EntityCard
      id="eliezer-yudkowsky"
      category="researcher"
      title="Eliezer Yudkowsky"
      description="Co-founder of MIRI, advocate for strong AI safety precautions"
    />
    <EntityCard
      id="neel-nanda"
      category="researcher"
      title="Neel Nanda"
      description="DeepMind alignment researcher, mechanistic interpretability expert"
    />
    <EntityCard
      id="connor-leahy"
      category="researcher"
      title="Connor Leahy"
      description="CEO of Conjecture, focuses on interpretability and prosaic AGI safety"
    />
  </EntityCards>
</Section>

<Section title="Academic Researchers">
  <EntityCards>
    <EntityCard
      id="stuart-russell"
      category="researcher"
      title="Stuart Russell"
      description="UC Berkeley professor, CHAI founder, author of 'Human Compatible'"
    />
    <EntityCard
      id="yoshua-bengio"
      category="researcher"
      title="Yoshua Bengio"
      description="Turing Award winner, AI pioneer now focused on AI safety"
    />
    <EntityCard
      id="geoffrey-hinton"
      category="researcher"
      title="Geoffrey Hinton"
      description="Turing Award winner, 'Godfather of AI', vocal about AI risks"
    />
    <EntityCard
      id="dan-hendrycks"
      category="researcher"
      title="Dan Hendrycks"
      description="Director of CAIS, focuses on catastrophic AI risk reduction"
    />
  </EntityCards>
</Section>

<Section title="Strategy and Governance">
  <EntityCards>
    <EntityCard
      id="nick-bostrom"
      category="researcher"
      title="Nick Bostrom"
      description="Philosopher at FHI, author of 'Superintelligence'"
    />
    <EntityCard
      id="toby-ord"
      category="researcher"
      title="Toby Ord"
      description="Philosopher at FHI, author of 'The Precipice'"
    />
    <EntityCard
      id="holden-karnofsky"
      category="researcher"
      title="Holden Karnofsky"
      description="Co-CEO of Open Philanthropy, influential AI risk grantmaker"
    />
  </EntityCards>
</Section>

## Key Areas of Disagreement

Different researchers hold varying positions on crucial questions:

### Timeline Optimism vs. Pessimism

**Shorter timeline** views (AGI by 2030s) are held by Dario Amodei, Ilya Sutskever, and many at frontier labs. **Medium timeline** views (AGI by 2040s-2050s) are represented by Paul Christiano and Holden Karnofsky. **Longer or more uncertain timelines** are more common among academics and governance-focused researchers.

### Alignment Difficulty

**More optimistic** researchers like Dario Amodei, Jan Leike, and Paul Christiano believe alignment is solvable with iteration on current approaches. **Moderately pessimistic** views from Stuart Russell and Yoshua Bengio hold that major breakthroughs are needed. **Very pessimistic** positions, held by Eliezer Yudkowsky and some MIRI researchers, consider alignment extremely difficult and possibly not solvable in time.

### Strategic Approach

Researchers diverge on strategy. Anthropic and some OpenAI researchers favor **racing to build safe AGI first**. Geoffrey Hinton and some policy advocates push to **slow down development**. MIRI and some academics **focus on fundamentals**. FHI researchers and policy experts prioritize **governance and coordination**.

## Contributing to the Field

These researchers represent various paths to contributing to AI safety. **Technical research** includes interpretability, alignment methods, and adversarial robustness. **Conceptual work** involves identifying new threat models and framing key problems. **Empirical research** tests alignment techniques on current systems. **Institution building** creates labs, research organizations, and governance bodies. **Communication** involves writing for different audiences and building field awareness. **Funding** directs resources to high-priority work.

Understanding their different approaches helps newcomers identify where they might contribute most effectively.
