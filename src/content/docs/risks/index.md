---
title: Risks
description: Catalog of AI-related risks and failure modes
sidebar:
  order: 0
---

This section documents specific risks from advanced AI systems, from technical alignment failures to societal-scale harms.

## Categories

### Technical Alignment Risks
- [Deceptive Alignment](/risks/deceptive-alignment) - AI that appears aligned but isn't
- Mesa-Optimization - Learned optimizers with different objectives
- Reward Hacking - Gaming reward signals in unintended ways
- Goal Misgeneralization - Goals that don't transfer to new contexts

### Structural Risks
- Racing Dynamics - Competition driving unsafe practices
- Proliferation - Spread of dangerous capabilities
- Lock-in - Permanent entrenchment of bad outcomes

### Misuse Risks
- Bioweapons - AI-assisted pathogen design
- Cyberattacks - Autonomous hacking at scale
- Manipulation - Large-scale influence operations

## Risk Assessment Framework

Each risk is evaluated on:
- **Severity**: Low / Medium / High / Catastrophic
- **Likelihood**: Probability estimate with uncertainty
- **Timeframe**: When might this become relevant?
- **Tractability**: Can we do anything about it?
