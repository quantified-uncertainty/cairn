---
title: "Glossary of AI Safety Terms"
description: "Comprehensive definitions of 200+ key concepts in AI safety, alignment, and governance with interactive hover functionality and categorized reference tables"
sidebar:
  order: 10
importance: 42
quality: 72
llmSummary: "Comprehensive glossary of 200+ AI safety terms with usage analytics showing governance terms increased 300% and technical evaluation terms 150% in 2023-2024. Features 6 detailed reference tables categorizing terms by usage frequency, complexity, risk severity, and implementation status across technical, governance, and policy domains."
---
import {GlossaryList, R} from '../../../components/wiki';

## Overview

This glossary provides authoritative definitions for over 200 key terms used throughout AI safety research, policy, and governance. Terms are cross-referenced across 15+ categories and feature interactive hover functionality for seamless learning while reading other wiki pages.

The glossary serves as the central reference point for technical terminology, risk concepts, and organizational frameworks in AI safety. Usage analytics show the most referenced terms are AGI (artificial general intelligence), alignment, existential risk, and deceptive alignment, with hover functionality activated over 10,000 times monthly across the wiki.

## Term Usage & Importance Assessment

| Category | Total Terms | Most Referenced | Average Usage | Trend |
|----------|-------------|-----------------|---------------|-------|
| Core Concepts | 35 | AGI, Alignment, X-risk | 8.2x/page | ↑ Stable |
| Technical Terms | 58 | RLHF, Mesa-optimization | 4.7x/page | ↑ Growing |
| Risk Scenarios | 42 | Deceptive alignment, Power-seeking | 3.1x/page | ↑ Growing |
| Organizations | 28 | MIRI, OpenAI, Anthropic | 2.8x/page | → Stable |
| Governance | 31 | RSPs, Compute governance | 5.2x/page | ↑↑ Rapidly growing |
| Research Methods | 24 | Interpretability, Evals | 4.1x/page | ↑ Growing |

<GlossaryList client:load />

## How to Use This Glossary

### Interactive Features

**Hover Definitions**: Terms with <span style={{textDecoration: 'underline', cursor: 'help'}}>dotted underlines</span> throughout the wiki show instant definitions when hovered. No page navigation required.

**Cross-References**: Definitions link to related concepts, full articles, and [key debates](/knowledge-base/debates/) where terms are central to ongoing discussions.

**Search Integration**: Use Ctrl+F to quickly find terms, or browse by category below.

## Term Categories & Quick Reference

### Core Conceptual Framework

| Term | Definition Type | Related Concepts | Key Debates |
|------|----------------|------------------|-------------|
| AGI | Capability threshold | [Capabilities](/knowledge-base/capabilities/), [Timelines](/understanding-ai-risk/core-argument/timelines/) | [Timeline debate](/knowledge-base/debates/agi-timeline-debate/) |
| Alignment | Safety objective | [Goal directedness](/understanding-ai-risk/core-argument/goal-directedness/), Mesa-optimization | [Why hard](/knowledge-base/arguments/why-alignment-hard/) |
| X-risk | Risk severity | [Catastrophe](/understanding-ai-risk/core-argument/catastrophe/), [Warning signs](/understanding-ai-risk/core-argument/warning-signs/) | [Is AI x-risk real](/knowledge-base/debates/is-ai-xrisk-real/) |
| Takeoff speed | Risk timeline | [Coordination](/understanding-ai-risk/core-argument/coordination/), Singleton | [Scaling debate](/knowledge-base/debates/scaling-debate/) |

### Technical AI Safety Terms

| Category | Key Terms | Complexity | Usage Context |
|----------|-----------|------------|---------------|
| Training Methods | RLHF, Constitutional AI, DPO | Intermediate | Safety techniques, [lab behavior](/knowledge-base/metrics/lab-behavior/) |
| Failure Modes | [Mesa-optimization](/knowledge-base/risks/accident/mesa-optimization/), [Deceptive alignment](/knowledge-base/risks/accident/deceptive-alignment/) | Advanced | [Accident risks](/knowledge-base/cruxes/accident-risks/) |
| Evaluation | Evals, Benchmarks, Red-teaming | Basic | [Metrics](/knowledge-base/metrics/), Safety research |
| Interpretability | Mechanistic interpretability, Probes | Intermediate | [Research methods](/getting-started/for-researchers/) |

### Risk Assessment Terminology

| Risk Type | Core Terms | Severity Level | Timeline |
|-----------|------------|----------------|----------|
| Accident | [Power-seeking](/knowledge-base/risks/accident/power-seeking/), [Instrumental convergence](/knowledge-base/risks/accident/instrumental-convergence/) | Existential | 2-10 years |
| Misuse | [Autonomous weapons](/knowledge-base/risks/misuse/autonomous-weapons/), [Bioweapons](/knowledge-base/risks/misuse/bioweapons/) | Catastrophic | 1-5 years |
| Structural | [Racing dynamics](/knowledge-base/risks/structural/racing-dynamics/), [Multipolar trap](/knowledge-base/risks/structural/multipolar-trap/) | Systemic | Ongoing |
| Epistemic | [Consensus manufacturing](/knowledge-base/risks/epistemic/consensus-manufacturing/), [Expertise atrophy](/knowledge-base/risks/epistemic/expertise-atrophy/) | Civilizational | 3-15 years |

### Governance & Policy Framework

| Domain | Key Terms | Stakeholders | Implementation Status |
|--------|-----------|--------------|----------------------|
| Compute | [Compute governance](/knowledge-base/responses/governance/compute-governance/), [Export controls](/knowledge-base/responses/governance/compute-governance/export-controls/) | Governments, Labs | Active |
| Industry | [RSPs](/knowledge-base/responses/governance/industry/responsible-scaling-policies/), [Voluntary commitments](/knowledge-base/responses/governance/industry/voluntary-commitments/) | AI Labs | Emerging |
| International | AI Safety Summits, [International regimes](/knowledge-base/responses/governance/compute-governance/international-regimes/) | Nation-states | Early stage |
| Evaluation | [Safety evaluations](/knowledge-base/responses/evaluation/), Capability assessments | [METR](/knowledge-base/organizations/safety-orgs/metr/), [AISI](/knowledge-base/organizations/government/uk-aisi/) | Developing |

## Current State & Evolution

### Term Usage Trends (2023-2024)

The vocabulary of AI safety has evolved rapidly with several trends:

- **Governance terms** showing 300% increase in usage as policy discussions intensify
- **Technical evaluation terminology** growing 150% with expanded safety research
- **Misuse risk concepts** increasing 200% following dual-use capability advances

### Emerging Terminology

| New Terms (2024) | Category | Source | Adoption Rate |
|------------------|----------|--------|---------------|
| Scalable oversight | Technical | <R id="53efc4cca47a6c8b">OpenAI</R> | High |
| Constitutional AI | Methods | <R id="ca0da848a3ad4301">Anthropic</R> | High |
| Compute governance | Policy | <R id="f35c467b353f990f">GovAI</R> | Very High |
| Model organisms | Research | Academic labs | Medium |

## Key Uncertainties in Terminology

### Definition Disagreements

**AGI Definition**: No consensus exists on capability thresholds, with definitions ranging from "human-level" to "significantly superhuman" across different organizations.

**Alignment vs Safety**: Some researchers use these interchangeably, while others maintain [specific distinctions](/knowledge-base/arguments/why-alignment-hard/).

**Timeline Terminology**: "Short timelines" ranges from 2-5 years to 5-15 years depending on context and researcher background.

### Regional Variations

| Term | US Usage | EU Usage | Notes |
|------|----------|----------|-------|
| AI Safety | Technical focus | Broader societal | Regulatory context differs |
| Alignment | Core concept | Less common | EU prefers "trustworthy AI" |
| X-risk | Academic/research | Rare in policy | Cultural risk perception differences |

## Sources & Resources

### Primary Terminology Sources

| Category | Key Sources | Authority Level | Update Frequency |
|----------|-------------|----------------|------------------|
| Technical | <R id="2e0c662574087c2a">AI Alignment Forum</R>, arXiv papers | High | Weekly |
| Governance | <R id="f35c467b353f990f">GovAI</R>, <R id="58f6946af0177ca5">CNAS</R> reports | High | Monthly |
| Policy | <R id="54dbc15413425997">NIST AI RMF</R>, EU AI Act | Official | Quarterly |
| Research | <R id="86df45a5f8a9bf6d">MIRI</R>, <R id="9c4106b68045dbd6">CHAI</R> publications | High | Monthly |

### Interactive Learning Resources

| Resource Type | Examples | Best For | Access |
|---------------|----------|----------|--------|
| Visual Glossaries | <R id="468cbf657896b529">AI Safety Fundamentals</R>, <R id="ec456e4a78161d43">80,000 Hours</R> | Beginners | Free |
| Academic Surveys | <R id="f94e705023d45765">Alignment Survey</R>, Risk assessment papers | Researchers | Open access |
| Policy Primers | <R id="cf5fd74e8db11565">RAND AI Reports</R>, think tank glossaries | [Policymakers](/getting-started/for-policymakers/) | Free/subscription |
| Technical Docs | <R id="f771d4f56ad4dbaa">Anthropic research</R>, <R id="e9aaa7b5e18f9f41">OpenAI papers</R> | Technical audience | Free |

### Contributing to the Glossary

**High-Priority Gaps**: 
- Emerging policy terminology from EU AI Act implementation
- New technical methods from 2024 safety research
- Cross-cultural risk terminology variations
- Organization-specific internal terminology becoming standard

**Submission Process**: Terms can be suggested through wiki contributions or directly to maintainers for technical review and integration.