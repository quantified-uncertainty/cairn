---
title: "Compute (AI Capabilities)"
description: "Hardware resources for AI training and inference. Uniquely tractable as a governance lever due to measurability, concentration, and physicality."
sidebar:
  order: 1
lastEdited: "2026-01-06"
ratings:
  changeability: 30
  xriskImpact: 70
  trajectoryImpact: 80
  uncertainty: 35
---
import {Backlinks} from '../../../../../components/wiki';

Compute refers to the hardware resources required to train and run AI systemsâ€”GPUs, TPUs, and specialized accelerators. Training frontier models costs tens to hundreds of millions of dollars in compute alone.

Compute is uniquely tractable for governance because it is **measurable** (FLOPs, GPU-hours), **concentrated** (few chokepoints like ASML, TSMC, NVIDIA), and **physical** (can be tracked and controlled).

| Metric | Score | Notes |
|--------|-------|-------|
| Changeability | 30 | Requires international coordination |
| X-risk Impact | 70 | Directly affects capability timelines |
| Trajectory Impact | 80 | Primary driver of AI advancement speed |
| Uncertainty | 35 | Hardware trends relatively predictable |

## Related Content

**Governance Approaches:**
- [Compute Governance Overview](/knowledge-base/responses/governance/compute-governance/)
- [Export Controls](/knowledge-base/responses/governance/compute-governance/export-controls/)
- [Compute Monitoring](/knowledge-base/responses/governance/compute-governance/monitoring/)
- [Hardware-Enabled Governance](/knowledge-base/responses/governance/compute-governance/hardware-enabled-governance/)

**Key Debates:**
- Can compute controls effectively slow dangerous AI development?
- Will efficiency gains outpace hardware restrictions?

<Backlinks entityId="compute" />
